{"data": {"video_id": {"0": "rFwQDDbYTm4", "1": "Nq3auVtvd9Q", "2": "eyxmSmjmNS0", "3": "yexR53My2O4", "4": "GWt6Fu05voI", "5": "rHQPBqMULXo", "6": "Lg97gWXsiQ4", "7": "U0mxx7AoNz0", "8": "a4P8v8lGFPw", "9": "Xp3jR-ttMfo", "10": "w3knicSHx5s", "11": "1HEdXwEYrGM", "12": "AJwnbSP_rq8", "13": "vfBAUYpMCTU", "14": "OUCwujwE7bA", "15": "D6osiiEoV0w", "16": "zcGOPqFZ4Tk", "17": "qNfCVGbvnJc", "18": "VQoyypYTz2U", "19": "XHGh19Hbx48", "20": "FNDVy_BR8aA", "21": "C5sWbYwzKyg", "22": "kl3aBni87jg", "23": "6dvcYx9hcbE", "24": "MgJ3JsE3Tqo", "25": "smxwT82o40Y", "26": "Z3knUzwuIgo", "27": "AvHLJqtmQkE", "28": "3ks2gpqAKY8", "29": "z4lAlVRwbrc", "30": "DdkenV-ZdJU", "31": "C7mUYocWdG0", "32": "ccBMRryxGog", "33": "AIOE1l1W0Tw", "34": "16BsJI5I-Yw", "35": "X4S8F3bwuuw", "36": "_7xpGve9QEE", "37": "K-cXYoqHxBc", "38": "gwI6g1pBD84", "39": "qS-iYnp00uc", "40": "af6WPqvzjjk", "41": "YQ2QtKcK2dA", "42": "WncUlZYpdq4", "43": "dPsXxLyqpfs", "44": "_Z9ZP1eiKsI", "45": "-YiMVR3HEuY", "46": "56GW1IlWgMg", "47": "agXIYMCICcc", "48": "_N_nFzMtWkA", "49": "Qk4lJdp7ZAs", "50": "69IjNZaoeao", "51": "kOy49NqZeqI", "52": "BTLCdge7uSQ", "53": "KXEEqcwXn8w", "54": "We20YSAJZSE", "55": "RrvC8YW0pT0", "56": "EbFosdOi5SY", "57": "U3zmekzQ8WQ", "58": "awyuuJoHawo", "59": "8wkgDnNxiVs", "60": "gbG1X8Xq-T8", "61": "hg2Q_O5b9w4", "62": "-h1KB8ps11A", "63": "pZyxlf6l0N8", "64": "F5aaXrIMWyU", "65": "PDRtyrVskMU", "66": "to7vCdkLi4s", "67": "tjbEVY5XIk0", "68": "IiBFqnNu7A8", "69": "Nfry2b4RFI4", "70": "UjJU13GdL94", "71": "HYEzHX6-fIA", "72": "ml3Y1ljVSQ8", "73": "rFwQDDbYTm4", "74": "a4VvcmqnkhY", "75": "v2GRWzIhaqQ", "76": "9-o2aAoN0rY", "77": "vLTmnaMpQCs", "78": "lFGnsdV-sR4", "79": "O1b0cbgpRBw", "80": "BhUWvQmLzSk", "81": "o75ybZ-6Uu8", "82": "kU-tWy_wr78", "83": "-buULmf7dec", "84": "P38FZrbNHV4", "85": "NJCLUzkn-sA", "86": "U0mxx7AoNz0", "87": "XHGh19Hbx48", "88": "FNDVy_BR8aA", "89": "6dvcYx9hcbE", "90": "NeGJAUSQEJI", "91": "z4lAlVRwbrc", "92": "povBDxUn1VQ", "93": "16BsJI5I-Yw", "94": "Ru23eWAQ6_E", "95": "X4S8F3bwuuw", "96": "3N3Bl5AA5QU", "97": "ciNMc0Czmfc", "98": "u1_qMdb0kYU", "99": "-9evrZnBorM", "100": "_PyusGsbBPY", "101": "iDulhoQ2pro", "102": "H5vpBCLo74U", "103": "-MCYbmU9kfg", "104": "69IjNZaoeao", "105": "i4H0kjxrias", "106": "tC01FRB0M7w", "107": "p3sAF3gVMMA", "108": "qeEO2GECQk0", "109": "AU30czb4iQA", "110": "_8KNb5iqblE", "111": "wTIPGoHLw_8", "112": "cIUtRNhY6Rw", "113": "G3pOvrKkFuk", "114": "utuz7wBGjKM", "115": "IIebBjbBevs", "116": "SY5PvZrJhLE", "117": "q7QP_lfqnQM", "118": "nxEr4VNgYOE", "119": "rl4nUngiR2k", "120": "xTzFJIknh7E", "121": "-_2AF9Lhweo", "122": "ZfDZRX3WiJg", "123": "l12GXD0t_RE", "124": "1VdEw_mGjFk", "125": "q6Kyvy1zLwQ", "126": "hAooAOFRsYc", "127": "yexR53My2O4", "128": "WVPE62Gk3EM", "129": "nv6oFDp6rNQ", "130": "lj-LGrnh1oU", "131": "vLTmnaMpQCs", "132": "xJrKIPwVwGM", "133": "NAJOZTNkhlI", "134": "plK2WVdLTOY", "135": "j4xgkjWlfL4", "136": "T9XSU0pKX2E", "137": "iAR8LkkMMIM", "138": "zdb8MM94A5c", "139": "m-zrcmRd7E4", "140": "_c6A33Fg5Ns", "141": "-Kgxv64aG3o", "142": "0JlB9gufTw8", "143": "aX8phGhG8VQ", "144": "kP-dXK9JEhY", "145": "hgSGHusDx7M", "146": "AJwnbSP_rq8", "147": "OUCwujwE7bA", "148": "qNfCVGbvnJc", "149": "s9UAOmyah1A", "150": "_EDr3ryrT_Y", "151": "AvHLJqtmQkE", "152": "gYxJEd3EUKs", "153": "3ks2gpqAKY8", "154": "qlB0TPBQ7YY", "155": "C7mUYocWdG0", "156": "ccBMRryxGog", "157": "Ru23eWAQ6_E", "158": "X4S8F3bwuuw", "159": "mIZLGBD99iU", "160": "_NMQyOu2HTo", "161": "ZTs_mXwMCs8", "162": "0A8ljAkdFtg", "163": "64Izfm24FKA", "164": "E5OnoYF2oAk", "165": "2zW33LfffPc", "166": "4Cclp6yPDuw", "167": "ut5kp56wW_4", "168": "x8pW19wKfXQ", "169": "hMO6rbMAPew", "170": "qeEO2GECQk0", "171": "D-eg7k8YSfs", "172": "plK2WVdLTOY", "173": "k_hUdZJNzkU", "174": "TFiZYA_JfJs", "175": "l8JeokY5NsU", "176": "ZAW9EyNo2fw", "177": "rvr143crpuU", "178": "ctCv_NRpqvM", "179": "ZVVnvZdUMUk", "180": "eYgPJ_7BkEw", "181": "PZypP7PiKi0", "182": "Ok44otx90D4", "183": "D-eg7k8YSfs", "184": "a0f07M2uj_A", "185": "MpdbFLXOOIw", "186": "fvctpYph8Pc", "187": "jhCInVFE2sc", "188": "k1GOF2jmX7c", "189": "Cs_j-oNwGgg", "190": "bFn2xcGi1TQ", "191": "l5he9JNJqHA", "192": "u5BkO8XMS2I", "193": "a-VQfQqIMrE", "194": "3_qGrmD6iQY", "195": "THcuTJbeD34", "196": "O9kFX33nUcU", "197": "cuyM63ugsxI", "198": "Hdo81GtLC_4", "199": "q7PjrmGNx5A", "200": "1VdEw_mGjFk", "201": "z_3Qv4In2ac", "202": "3jT1qJ8ETzk", "203": "Jqvb7jp4Nm8", "204": "5IRlUVrEVL8", "205": "a6v92P0EbJc", "206": "nv6oFDp6rNQ", "207": "G2sr1g6rLdE", "208": "EbHUU-gLyRA", "209": "3baFTP0uYOc", "210": "DiNzQP7kK-s", "211": "gch94ttuy5s", "212": "LB4B5FYvtdI", "213": "ahRPdiCop3E", "214": "Z_kWZpgEZ7w", "215": "Elxn8rS88bI", "216": "8Oy7o3Yu-Xo", "217": "nQDZmf2Yb9k", "218": "dND-7llwrpw", "219": "EeMhj0sPrhE", "220": "vVRC-0VKPrg", "221": "W2UT8NjUqrk", "222": "1HEdXwEYrGM", "223": "i-J4T3uLC9M", "224": "MgJ3JsE3Tqo", "225": "O_dJ31T01i8", "226": "smxwT82o40Y", "227": "AIOE1l1W0Tw", "228": "jSdHmImyUjk", "229": "0PAiQ1jTN5k", "230": "3N3Bl5AA5QU", "231": "_okxGdHM5b8", "232": "jltgNGt8Lpg", "233": "u1_qMdb0kYU", "234": "-9evrZnBorM", "235": "nPB0ppcnzZA", "236": "iDulhoQ2pro", "237": "agXIYMCICcc", "238": "hMO6rbMAPew", "239": "H5vpBCLo74U", "240": "1L83tM8nwHU", "241": "nXGHJTtFYRU", "242": "MIEA8azwu1k", "243": "BTLCdge7uSQ", "244": "We20YSAJZSE", "245": "RrvC8YW0pT0", "246": "i4H0kjxrias", "247": "eCH0M4wzKJs", "248": "Cs_j-oNwGgg", "249": "T35ba_VXkMY", "250": "Uumd2zOOz60", "251": "7DGlElSVYGo", "252": "j4xgkjWlfL4", "253": "hHZSA9z_abE", "254": "cllFzkvrYmE", "255": "Rk3MBx20z24", "256": "rHQPBqMULXo", "257": "7OdhtAiPfWY", "258": "hIoCn_9QTVU", "259": "rR5_emVeyBk", "260": "vxdcX0JTEr0", "261": "Pm93D8CVlY8", "262": "2ethDz9KnLk", "263": "3N3Bl5AA5QU", "264": "0A8ljAkdFtg", "265": "efPrtcLdcdM", "266": "mIZLGBD99iU", "267": "64Izfm24FKA", "268": "Hi6cbeBY2oQ", "269": "ddG2fM9i4Kk", "270": "jltgNGt8Lpg", "271": "sbKaUc0tPaY", "272": "OioFONrSETc", "273": "_PyusGsbBPY", "274": "WYrvh50yu6s", "275": "iDulhoQ2pro", "276": "1L83tM8nwHU", "277": "H6Qiegq_36c", "278": "wZWn7Hm8osA", "279": "nXGHJTtFYRU", "280": "Xc9Rkbg6IZA", "281": "i4H0kjxrias", "282": "9Kec_7WFyp0", "283": "p3sAF3gVMMA", "284": "lmAj0SU_bW0", "285": "8wkgDnNxiVs", "286": "klPuEHCKG9M", "287": "ZVVnvZdUMUk", "288": "eYgPJ_7BkEw", "289": "Ok44otx90D4", "290": "_8KNb5iqblE", "291": "1aO-uHXbzmQ", "292": "k1GOF2jmX7c", "293": "l_3zj6HeWUE", "294": "p-zOeQCoG9c", "295": "T35ba_VXkMY", "296": "q7QP_lfqnQM", "297": "hQEnzdLkPj4", "298": "4GKCxJQSw-g", "299": "WTB2p4bqtXU", "300": "-_2AF9Lhweo", "301": "ZfDZRX3WiJg", "302": "8l-TDqpoUQs", "303": "DLq1DUcMh1Q", "304": "sEG8hD64c_Q", "305": "Q5g3p9Zwjrk", "306": "qSArFEIoSbo", "307": "LMb5tvW-UoQ", "308": "Hdo81GtLC_4", "309": "eI8xTdcZ6VY", "310": "V79rRI05Lj4", "311": "DYBmD88vpiA", "312": "1VdEw_mGjFk", "313": "qFRfnIRMNlk", "314": "hAooAOFRsYc", "315": "3jT1qJ8ETzk", "316": "Jqvb7jp4Nm8", "317": "x6T1zMSE4Ts", "318": "v-ZxzTSpmk4", "319": "Nq3auVtvd9Q", "320": "eyxmSmjmNS0", "321": "GWt6Fu05voI", "322": "a6v92P0EbJc", "323": "WVPE62Gk3EM", "324": "nv6oFDp6rNQ", "325": "hv3UO3G0Ofo", "326": "TrdevFK_am4", "327": "3qxJ2WD8p4w", "328": "xJrKIPwVwGM", "329": "IaS72aHrJKE", "330": "iAR8LkkMMIM", "331": "zdb8MM94A5c", "332": "rNkHjZtH0RQ", "333": "R5DiLFOMZrc", "334": "RSSVWpBak6s", "335": "cllFzkvrYmE", "336": "P_xeshTnPZg", "337": "7K4Z8RqjWIk", "338": "pH2jZun8MoY", "339": "2PYLNHqxd5A", "340": "qgUegkefocg", "341": "2h4tRsQzipQ", "342": "hgSGHusDx7M", "343": "InhMx1h0N40", "344": "Xp3jR-ttMfo", "345": "w3knicSHx5s", "346": "D6osiiEoV0w", "347": "qNfCVGbvnJc", "348": "X2k7n4FuI7c", "349": "igBp5vUrc7o", "350": "Z3knUzwuIgo", "351": "ccBMRryxGog", "352": "x8pW19wKfXQ", "353": "RrBapqCPnmE", "354": "G3pOvrKkFuk", "355": "fvctpYph8Pc", "356": "1aO-uHXbzmQ", "357": "cIUtRNhY6Rw", "358": "to7vCdkLi4s", "359": "tjbEVY5XIk0", "360": "k1GOF2jmX7c", "361": "Cs_j-oNwGgg", "362": "l_3zj6HeWUE", "363": "bFn2xcGi1TQ", "364": "p-zOeQCoG9c", "365": "l5he9JNJqHA", "366": "u5BkO8XMS2I", "367": "a-VQfQqIMrE", "368": "IiBFqnNu7A8", "369": "YrO1v7-KcXs", "370": "Nfry2b4RFI4", "371": "IIebBjbBevs", "372": "UjJU13GdL94", "373": "T35ba_VXkMY", "374": "SY5PvZrJhLE", "375": "q7QP_lfqnQM", "376": "HYEzHX6-fIA", "377": "3_qGrmD6iQY", "378": "hQEnzdLkPj4", "379": "CA8JPbJ75tY", "380": "4GKCxJQSw-g", "381": "rl4nUngiR2k", "382": "xTzFJIknh7E", "383": "WTB2p4bqtXU", "384": "-_2AF9Lhweo", "385": "ZfDZRX3WiJg", "386": "THcuTJbeD34", "387": "O9kFX33nUcU", "388": "cuyM63ugsxI", "389": "ml3Y1ljVSQ8", "390": "l12GXD0t_RE", "391": "8l-TDqpoUQs", "392": "DLq1DUcMh1Q", "393": "sEG8hD64c_Q", "394": "YPfUiOMYOEE", "395": "YBlNQK0Ao6g", "396": "2lkUNDZld-4", "397": "Q5g3p9Zwjrk", "398": "qSArFEIoSbo", "399": "Uumd2zOOz60", "400": "LMb5tvW-UoQ", "401": "Hdo81GtLC_4", "402": "eI8xTdcZ6VY", "403": "V79rRI05Lj4", "404": "q7PjrmGNx5A", "405": "nxEr4VNgYOE", "406": "DYBmD88vpiA", "407": "1VdEw_mGjFk", "408": "q6Kyvy1zLwQ", "409": "qFRfnIRMNlk", "410": "hAooAOFRsYc", "411": "3jT1qJ8ETzk", "412": "Jqvb7jp4Nm8", "413": "x6T1zMSE4Ts", "414": "v-ZxzTSpmk4", "415": "5IRlUVrEVL8", "416": "rFwQDDbYTm4", "417": "Nq3auVtvd9Q", "418": "eyxmSmjmNS0", "419": "yexR53My2O4", "420": "GWt6Fu05voI", "421": "a6v92P0EbJc", "422": "a4VvcmqnkhY", "423": "nv6oFDp6rNQ", "424": "v2GRWzIhaqQ", "425": "lj-LGrnh1oU", "426": "9-o2aAoN0rY", "427": "G2sr1g6rLdE", "428": "hv3UO3G0Ofo", "429": "EbHUU-gLyRA", "430": "vLTmnaMpQCs", "431": "MQ89be_685o", "432": "3baFTP0uYOc", "433": "TrdevFK_am4", "434": "DiNzQP7kK-s", "435": "3qxJ2WD8p4w", "436": "xJrKIPwVwGM", "437": "NAJOZTNkhlI", "438": "gch94ttuy5s", "439": "IaS72aHrJKE", "440": "LB4B5FYvtdI", "441": "B9PL__gVxLI", "442": "BhUWvQmLzSk", "443": "plK2WVdLTOY", "444": "j4xgkjWlfL4", "445": "T9XSU0pKX2E", "446": "iAR8LkkMMIM", "447": "yFAuXmcGk2Y", "448": "zdb8MM94A5c", "449": "ahRPdiCop3E", "450": "m-zrcmRd7E4", "451": "rNkHjZtH0RQ", "452": "R5DiLFOMZrc", "453": "o75ybZ-6Uu8", "454": "_c6A33Fg5Ns", "455": "RSSVWpBak6s", "456": "cllFzkvrYmE", "457": "Z_kWZpgEZ7w", "458": "Ag1bw8MfHGQ", "459": "Elxn8rS88bI", "460": "VCUDo9umKEQ", "461": "P_xeshTnPZg", "462": "qtu0aSTDE2I", "463": "CRlN-cYFxTk", "464": "uwfVxckuq50", "465": "h3ij3F3cPIk", "466": "pH2jZun8MoY", "467": "W-O7AZNzbzQ", "468": "2PYLNHqxd5A", "469": "kU-tWy_wr78", "470": "dmH1ZpcROMk", "471": "-buULmf7dec", "472": "8Oy7o3Yu-Xo", "473": "P38FZrbNHV4", "474": "g08NkNWmZTA", "475": "k_hUdZJNzkU", "476": "z15JLtAuwVI", "477": "nQDZmf2Yb9k", "478": "qgUegkefocg", "479": "-Kgxv64aG3o", "480": "0JlB9gufTw8", "481": "pBau7umFhjQ", "482": "aX8phGhG8VQ", "483": "19Q-vMd9bYg", "484": "wTzvKB6D_34", "485": "dND-7llwrpw", "486": "kP-dXK9JEhY", "487": "NJCLUzkn-sA", "488": "2h4tRsQzipQ", "489": "EeMhj0sPrhE", "490": "vVRC-0VKPrg", "491": "W2UT8NjUqrk", "492": "hgSGHusDx7M", "493": "InhMx1h0N40", "494": "Lg97gWXsiQ4", "495": "gwI6g1pBD84", "496": "U0mxx7AoNz0", "497": "a4P8v8lGFPw", "498": "Xp3jR-ttMfo", "499": "w3knicSHx5s", "500": "1HEdXwEYrGM", "501": "vfBAUYpMCTU", "502": "OUCwujwE7bA", "503": "D6osiiEoV0w", "504": "zcGOPqFZ4Tk", "505": "qNfCVGbvnJc", "506": "XHGh19Hbx48", "507": "s9UAOmyah1A", "508": "lvYVuOmUVs8", "509": "6dvcYx9hcbE", "510": "i-J4T3uLC9M", "511": "MgJ3JsE3Tqo", "512": "O_dJ31T01i8", "513": "X2k7n4FuI7c", "514": "_EDr3ryrT_Y", "515": "gYxJEd3EUKs", "516": "NeGJAUSQEJI", "517": "qlB0TPBQ7YY", "518": "povBDxUn1VQ", "519": "Ru23eWAQ6_E", "520": "qS-iYnp00uc", "521": "oz5yZc9ULAc", "522": "jSdHmImyUjk", "523": "3N3Bl5AA5QU", "524": "_okxGdHM5b8", "525": "_NMQyOu2HTo", "526": "ZTs_mXwMCs8", "527": "ciNMc0Czmfc", "528": "E5OnoYF2oAk", "529": "2zW33LfffPc", "530": "4Cclp6yPDuw", "531": "ut5kp56wW_4", "532": "x8pW19wKfXQ", "533": "WncUlZYpdq4", "534": "1waHlpKiNyY", "535": "SjQyLhQIXSM", "536": "C1N_PDHuJ6Q", "537": "6g0t3Phly2M", "538": "NyG-7nRpsW8", "539": "D8PJAL-MZv8", "540": "ARq74QuavAo", "541": "BOCLq2gpcGU", "542": "FDCfw-YqWTE", "543": "qhXZsFVxGKo", "544": "s2coXdufOzE", "545": "y1xoI7mBtOc", "546": "QrzApibhohY", "547": "4Ct3Yujl1dk", "548": "4qJaSmvhxi8", "549": "-_4Zi8fCZO4", "550": "lAq96T8FkTw", "551": "NxTFlzBjS-4", "552": "lWzo8CajF5s", "553": "k8fTYJPd3_I", "554": "_e-LFe_igno", "555": "JXQT_vxqwIs", "556": "QzulmoOg2JE", "557": "AXDByU3D1hA", "558": "cSoK_6Rkbfg", "559": "wKkcBPp3F1Y", "560": "tNIpEZLv_eg", "561": "em6dfRxYkYU", "562": "nUUqwaxLnWs", "563": "5qefnAek8OA", "564": "LLux1SW--oM", "565": "ueO_Ph0Pyqk", "566": "fODpu1-lNTw", "567": "S9ElPZupUsE", "568": "CS4cs9xVecg", "569": "n1l-9lIMW7E", "570": "BYGpKPY9pO0", "571": "xflCLdJh0n0", "572": "ysnIDax71yY", "573": "7AZjh2VXD6E", "574": "eqEc66RFY0I", "575": "hjrYrynGWGA", "576": "SHEPb1JHw5o", "577": "uJryes5Vk1o", "578": "GzphoJOVEcE", "579": "5H7M5Vd3-pk", "580": "hCP1vGoCdYU", "581": "nJyUyKN-XBQ", "582": "z_xiwjEdAC4", "583": "KKfZLXcF-aE", "584": "qsIrQi0fzbY", "585": "pYWASRauTzs", "586": "okpqeEUdEkY", "587": "2BkqApHKwn0", "588": "tKcLaGdvabM", "589": "V2QlTmh6P2Y", "590": "0S9c7nHoDws", "591": "k_S5fnKjO-4", "592": "fXOsFF95ifk", "593": "CcRkHl75Z-Y", "594": "rMOdrD61IoU", "595": "xy5MOQpx3aQ", "596": "kkWRbIb42Ms", "597": "Xvg00QnyaIY", "598": "NkOv_k7r6no", "599": "P7_jFxTtJEo", "600": "7bLEWDZng_M", "601": "yXcQ4B-YSjQ", "602": "6by6Xas_Kho", "603": "2gw5tE2ziqA", "604": "a8i2eJin0lY", "605": "yslMo3hSbqE", "606": "5dWp1mw_XNk", "607": "B7-iPbddhsw", "608": "qzPQ8cEsVK8", "609": "VTE2KlfoO3Q", "610": "2zgon7XfN4I", "611": "1waHlpKiNyY", "612": "SjQyLhQIXSM", "613": "C1N_PDHuJ6Q", "614": "6g0t3Phly2M", "615": "NyG-7nRpsW8", "616": "D8PJAL-MZv8", "617": "ARq74QuavAo", "618": "BOCLq2gpcGU", "619": "FDCfw-YqWTE", "620": "qhXZsFVxGKo", "621": "s2coXdufOzE", "622": "y1xoI7mBtOc", "623": "QrzApibhohY", "624": "4Ct3Yujl1dk", "625": "4qJaSmvhxi8", "626": "-_4Zi8fCZO4", "627": "lAq96T8FkTw", "628": "NxTFlzBjS-4", "629": "lWzo8CajF5s", "630": "k8fTYJPd3_I", "631": "_e-LFe_igno", "632": "JXQT_vxqwIs", "633": "QzulmoOg2JE", "634": "AXDByU3D1hA", "635": "cSoK_6Rkbfg", "636": "wKkcBPp3F1Y", "637": "tNIpEZLv_eg", "638": "em6dfRxYkYU", "639": "nUUqwaxLnWs", "640": "5qefnAek8OA", "641": "LLux1SW--oM", "642": "ueO_Ph0Pyqk", "643": "fODpu1-lNTw", "644": "S9ElPZupUsE", "645": "dFX8k1kXhOw", "646": "UEtvV1D6B3s", "647": "sofffBNhVSo", "648": "BH9mlmdXzzI", "649": "M3qpIzy4MQk", "650": "_Fe5kKmFieg", "651": "DFUqMbWs5d8", "652": "J3HHOwcrkK8", "653": "CZf3oo0fuh0", "654": "NUmbgp1h64E", "655": "dM0exrbVZ08", "656": "zg26t-BH7ao", "657": "JoAxZsdw_3w", "658": "jyjJ-RpQ5zQ", "659": "HfM8UIohGE0", "660": "sfk5h0yC67o", "661": "2BH49JG_sTs", "662": "sn_QSB7T1xo", "663": "yofjFQddwHE", "664": "UdXfsAr4Gjw", "665": "ImUoubi_t7s", "666": "l_-CUyEx_x4", "667": "OT91E6_Qm1A", "668": "dwFcodBz_2I", "669": "dqwx-F7Eits", "670": "oJFShOfCZiA", "671": "LpAiPYNnxW0", "672": "xxu4IqwKw0w", "673": "JS12eb1cTLE", "674": "eMh5YqKopjE", "675": "y7RfAwltHTw", "676": "-eyhCTvrEtE", "677": "6joSL0CUNtA", "678": "LWI3b5GtwVc", "679": "wlQvPJHxfOE", "680": "j2nGxw8sKYU", "681": "69dr4090Y-Q", "682": "NKpuX_yzdYs", "683": "0jspaMLxBig", "684": "qvT3NyaycoA", "685": "cI6WLiujJDY", "686": "7VHsWChH27g", "687": "nbfJ23FxvuI", "688": "y2v1-u6t5eQ", "689": "R2vIYVdMzPc", "690": "_YcAEBnbtPQ", "691": "zGFKSQlef_0", "692": "6dQgPFaZMGQ", "693": "oJ2FL5BsFuQ", "694": "dvnsbOtAmYI", "695": "_i3aqgKVNQI", "696": "Er2ucMxjdHE", "697": "DejHQYAGb7Q", "698": "SysgYptB198", "699": "quoGRI-1l0A", "700": "vm2SI8AJY0s", "701": "ArPaAX_PhIs", "702": "XuD4C8vJzEQ", "703": "am36dePheDc", "704": "smHa2442Ah4", "705": "tQYZaDn_kSg", "706": "KTB_OFoAQcc", "707": "jPOAS7uCODQ", "708": "3PyJA9AfwSk", "709": "8oOgPUO-TBY", "710": "bXJx7y51cl0", "711": "ay3zYUeuyhU", "712": "-bvTzZCEOdM", "713": "dZVkygnKh1M", "714": "ZILIbUvp5lk", "715": "RYth6EbBUqM", "716": "c1RBQzKsDCk", "717": "C86ZXvgpejM", "718": "KfV8CJh7hE0", "719": "cFFu__mcoIw", "720": "FQM13HkEfBk", "721": "JI8saFjK84o", "722": "c3zw6KI6dLc", "723": "GSwYGkTfOKk", "724": "rRB9iymNy1w", "725": "5e5pjeojznk", "726": "XdsmlBGOK-k", "727": "ANIzQ5G-XPE", "728": "VAo84c1hQX8", "729": "RTlwl2bv0Tg", "730": "9s_FpMpdYW8", "731": "6ykvU9WuIws", "732": "-FfMVnwXrZ0", "733": "96b_weTZb2w", "734": "6jfw8MuKwpI", "735": "d2XB5-tuCWU", "736": "0NSLgoEtdnw", "737": "R39tWYYKNcI", "738": "ChoV5h7tw5A", "739": "xY-DMAJpIP4", "740": "b1I5X3UfEYI", "741": "QgkLfjfGul8", "742": "Cn8AtS-9Nwc", "743": "H343JRrncfc", "744": "DffGdrfY9gI", "745": "PiF2Aln-L3w", "746": "KGI7K_ehHsU", "747": "NgWujOrCZFo", "748": "e69ZWbbsGng", "749": "YJsRD_hU4tc", "750": "1zhmudvZAs4", "751": "UyEtTyeahus", "752": "ErNp43wcudY", "753": "hq_XyP9y0xg", "754": "79UqdjnPEZ0", "755": "lHXd2hBnlJk", "756": "oBB47VrQucA", "757": "fiDmWKh_WeQ", "758": "O5mqR4EFBQk", "759": "8Covj8F-NNc", "760": "quEHyoA94rw", "761": "BdZ6bjcixhk", "762": "BlxnbyvHTyI", "763": "o4je1lSpyaw", "764": "k3UYUmp3Bi4", "765": "uot5sbPz1NQ", "766": "foCIxwn7VpI", "767": "O9ZrPXPLmWg", "768": "DTd7TyY7a-0", "769": "A2bnWAIpLIo", "770": "qOEeK1SNF3k", "771": "0aDhjrs8FMw", "772": "mzv1mkJRA10", "773": "s5qFpEPNXEY", "774": "f5sN3xAEAWQ", "775": "a-oCxdzFapE", "776": "eW546hpa744", "777": "Ny970B12IQk", "778": "qt9tXjtlQt4", "779": "gz-44N3MMOA", "780": "hbqxEJisBHo", "781": "mFD5hUZubTI", "782": "UEMMOdFbT94", "783": "43CZ0HjIC7U", "784": "opWrnW5v25w", "785": "CEBwVqRdKWc", "786": "9p7WWapTrpA"}, "channelTitle": {"0": "Yannic Kilcher", "1": "Yannic Kilcher", "2": "Yannic Kilcher", "3": "Yannic Kilcher", "4": "Yannic Kilcher", "5": "Yannic Kilcher", "6": "Yannic Kilcher", "7": "Yannic Kilcher", "8": "Yannic Kilcher", "9": "Yannic Kilcher", "10": "Yannic Kilcher", "11": "Yannic Kilcher", "12": "Yannic Kilcher", "13": "Yannic Kilcher", "14": "Yannic Kilcher", "15": "Yannic Kilcher", "16": "Yannic Kilcher", "17": "Yannic Kilcher", "18": "Yannic Kilcher", "19": "Yannic Kilcher", "20": "Yannic Kilcher", "21": "Yannic Kilcher", "22": "Yannic Kilcher", "23": "Yannic Kilcher", "24": "Yannic Kilcher", "25": "Yannic Kilcher", "26": "Yannic Kilcher", "27": "Yannic Kilcher", "28": "Yannic Kilcher", "29": "Yannic Kilcher", "30": "Yannic Kilcher", "31": "Yannic Kilcher", "32": "Yannic Kilcher", "33": "Yannic Kilcher", "34": "Yannic Kilcher", "35": "Yannic Kilcher", "36": "Yannic Kilcher", "37": "Yannic Kilcher", "38": "Yannic Kilcher", "39": "Yannic Kilcher", "40": "Yannic Kilcher", "41": "Yannic Kilcher", "42": "Yannic Kilcher", "43": "Yannic Kilcher", "44": "Yannic Kilcher", "45": "Yannic Kilcher", "46": "Yannic Kilcher", "47": "Yannic Kilcher", "48": "Yannic Kilcher", "49": "Yannic Kilcher", "50": "Yannic Kilcher", "51": "Yannic Kilcher", "52": "Yannic Kilcher", "53": "Yannic Kilcher", "54": "Yannic Kilcher", "55": "Yannic Kilcher", "56": "Yannic Kilcher", "57": "Yannic Kilcher", "58": "Yannic Kilcher", "59": "Yannic Kilcher", "60": "Yannic Kilcher", "61": "Yannic Kilcher", "62": "Yannic Kilcher", "63": "Yannic Kilcher", "64": "Yannic Kilcher", "65": "Yannic Kilcher", "66": "Yannic Kilcher", "67": "Yannic Kilcher", "68": "Yannic Kilcher", "69": "Yannic Kilcher", "70": "Yannic Kilcher", "71": "Yannic Kilcher", "72": "Yannic Kilcher", "73": "Yannic Kilcher", "74": "Yannic Kilcher", "75": "Yannic Kilcher", "76": "Yannic Kilcher", "77": "Yannic Kilcher", "78": "Flat Sabbath", "79": "Yannic Kilcher", "80": "Yannic Kilcher", "81": "Yannic Kilcher", "82": "Yannic Kilcher", "83": "Yannic Kilcher", "84": "Yannic Kilcher", "85": "Yannic Kilcher", "86": "Yannic Kilcher", "87": "Yannic Kilcher", "88": "Yannic Kilcher", "89": "Yannic Kilcher", "90": "Yannic Kilcher", "91": "Yannic Kilcher", "92": "Yannic Kilcher", "93": "Yannic Kilcher", "94": "Yannic Kilcher", "95": "Yannic Kilcher", "96": "Yannic Kilcher", "97": "Yannic Kilcher", "98": "Yannic Kilcher", "99": "Yannic Kilcher", "100": "Yannic Kilcher", "101": "Yannic Kilcher", "102": "Yannic Kilcher", "103": "Yannic Kilcher", "104": "Yannic Kilcher", "105": "Yannic Kilcher", "106": "Yannic Kilcher", "107": "Yannic Kilcher", "108": "Yannic Kilcher", "109": "Yannic Kilcher", "110": "Yannic Kilcher", "111": "Yannic Kilcher", "112": "Yannic Kilcher", "113": "Yannic Kilcher", "114": "Yannic Kilcher", "115": "Yannic Kilcher", "116": "Yannic Kilcher", "117": "Yannic Kilcher", "118": "Yannic Kilcher", "119": "Yannic Kilcher", "120": "Yannic Kilcher", "121": "Yannic Kilcher", "122": "Yannic Kilcher", "123": "Yannic Kilcher", "124": "Yannic Kilcher", "125": "Yannic Kilcher", "126": "Yannic Kilcher", "127": "Yannic Kilcher", "128": "Yannic Kilcher", "129": "Yannic Kilcher", "130": "Yannic Kilcher", "131": "Yannic Kilcher", "132": "Yannic Kilcher", "133": "Yannic Kilcher", "134": "Yannic Kilcher", "135": "Yannic Kilcher", "136": "Yannic Kilcher", "137": "Yannic Kilcher", "138": "Yannic Kilcher", "139": "Yannic Kilcher", "140": "Yannic Kilcher", "141": "Yannic Kilcher", "142": "Yannic Kilcher", "143": "Yannic Kilcher", "144": "Yannic Kilcher", "145": "Yannic Kilcher", "146": "Yannic Kilcher", "147": "Yannic Kilcher", "148": "Yannic Kilcher", "149": "Yannic Kilcher", "150": "Yannic Kilcher", "151": "Yannic Kilcher", "152": "Yannic Kilcher", "153": "Yannic Kilcher", "154": "Yannic Kilcher", "155": "Yannic Kilcher", "156": "Yannic Kilcher", "157": "Yannic Kilcher", "158": "Yannic Kilcher", "159": "Yannic Kilcher", "160": "Yannic Kilcher", "161": "Yannic Kilcher", "162": "Yannic Kilcher", "163": "Yannic Kilcher", "164": "Yannic Kilcher", "165": "Yannic Kilcher", "166": "Yannic Kilcher", "167": "Yannic Kilcher", "168": "Yannic Kilcher", "169": "Yannic Kilcher", "170": "Yannic Kilcher", "171": "Yannic Kilcher", "172": "Yannic Kilcher", "173": "Yannic Kilcher", "174": "Yannic Kilcher", "175": "Yannic Kilcher", "176": "Yannic Kilcher", "177": "Yannic Kilcher", "178": "Yannic Kilcher", "179": "Yannic Kilcher", "180": "Yannic Kilcher", "181": "Yannic Kilcher", "182": "Yannic Kilcher", "183": "Yannic Kilcher", "184": "Yannic Kilcher", "185": "Yannic Kilcher", "186": "Yannic Kilcher", "187": "Yannic Kilcher", "188": "Yannic Kilcher", "189": "Yannic Kilcher", "190": "Yannic Kilcher", "191": "Yannic Kilcher", "192": "Yannic Kilcher", "193": "Yannic Kilcher", "194": "Yannic Kilcher", "195": "Yannic Kilcher", "196": "Yannic Kilcher", "197": "Yannic Kilcher", "198": "Yannic Kilcher", "199": "Yannic Kilcher", "200": "Yannic Kilcher", "201": "Yannic Kilcher", "202": "Yannic Kilcher", "203": "Yannic Kilcher", "204": "Yannic Kilcher", "205": "Yannic Kilcher", "206": "Yannic Kilcher", "207": "Yannic Kilcher", "208": "Yannic Kilcher", "209": "Yannic Kilcher", "210": "Yannic Kilcher", "211": "Yannic Kilcher", "212": "Yannic Kilcher", "213": "Yannic Kilcher", "214": "Yannic Kilcher", "215": "Yannic Kilcher", "216": "Yannic Kilcher", "217": "Yannic Kilcher", "218": "Yannic Kilcher", "219": "Yannic Kilcher", "220": "Yannic Kilcher", "221": "Yannic Kilcher", "222": "Yannic Kilcher", "223": "Yannic Kilcher", "224": "Yannic Kilcher", "225": "Yannic Kilcher", "226": "Yannic Kilcher", "227": "Yannic Kilcher", "228": "Yannic Kilcher", "229": "Yannic Kilcher", "230": "Yannic Kilcher", "231": "Yannic Kilcher", "232": "Yannic Kilcher", "233": "Yannic Kilcher", "234": "Yannic Kilcher", "235": "Yannic Kilcher", "236": "Yannic Kilcher", "237": "Yannic Kilcher", "238": "Yannic Kilcher", "239": "Yannic Kilcher", "240": "Yannic Kilcher", "241": "Yannic Kilcher", "242": "Yannic Kilcher", "243": "Yannic Kilcher", "244": "Yannic Kilcher", "245": "Yannic Kilcher", "246": "Yannic Kilcher", "247": "Yannic Kilcher", "248": "Yannic Kilcher", "249": "Yannic Kilcher", "250": "Yannic Kilcher", "251": "Yannic Kilcher", "252": "Yannic Kilcher", "253": "Yannic Kilcher", "254": "Yannic Kilcher", "255": "Yannic Kilcher", "256": "Yannic Kilcher", "257": "Yannic Kilcher", "258": "Yannic Kilcher", "259": "Yannic Kilcher", "260": "Yannic Kilcher", "261": "Yannic Kilcher", "262": "Yannic Kilcher", "263": "Yannic Kilcher", "264": "Yannic Kilcher", "265": "Yannic Kilcher", "266": "Yannic Kilcher", "267": "Yannic Kilcher", "268": "Yannic Kilcher", "269": "Yannic Kilcher", "270": "Yannic Kilcher", "271": "Yannic Kilcher", "272": "Yannic Kilcher", "273": "Yannic Kilcher", "274": "Yannic Kilcher", "275": "Yannic Kilcher", "276": "Yannic Kilcher", "277": "Yannic Kilcher", "278": "Yannic Kilcher", "279": "Yannic Kilcher", "280": "Yannic Kilcher", "281": "Yannic Kilcher", "282": "Yannic Kilcher", "283": "Yannic Kilcher", "284": "Yannic Kilcher", "285": "Yannic Kilcher", "286": "Yannic Kilcher", "287": "Yannic Kilcher", "288": "Yannic Kilcher", "289": "Yannic Kilcher", "290": "Yannic Kilcher", "291": "Yannic Kilcher", "292": "Yannic Kilcher", "293": "Yannic Kilcher", "294": "Yannic Kilcher", "295": "Yannic Kilcher", "296": "Yannic Kilcher", "297": "Yannic Kilcher", "298": "Yannic Kilcher", "299": "Yannic Kilcher", "300": "Yannic Kilcher", "301": "Yannic Kilcher", "302": "Yannic Kilcher", "303": "Yannic Kilcher", "304": "Yannic Kilcher", "305": "Yannic Kilcher", "306": "Yannic Kilcher", "307": "Yannic Kilcher", "308": "Yannic Kilcher", "309": "Yannic Kilcher", "310": "Yannic Kilcher", "311": "Yannic Kilcher", "312": "Yannic Kilcher", "313": "Yannic Kilcher", "314": "Yannic Kilcher", "315": "Yannic Kilcher", "316": "Yannic Kilcher", "317": "Yannic Kilcher", "318": "Yannic Kilcher", "319": "Yannic Kilcher", "320": "Yannic Kilcher", "321": "Yannic Kilcher", "322": "Yannic Kilcher", "323": "Yannic Kilcher", "324": "Yannic Kilcher", "325": "Yannic Kilcher", "326": "Yannic Kilcher", "327": "Yannic Kilcher", "328": "Yannic Kilcher", "329": "Yannic Kilcher", "330": "Yannic Kilcher", "331": "Yannic Kilcher", "332": "Yannic Kilcher", "333": "Yannic Kilcher", "334": "Yannic Kilcher", "335": "Yannic Kilcher", "336": "Yannic Kilcher", "337": "Yannic Kilcher", "338": "Yannic Kilcher", "339": "Yannic Kilcher", "340": "Yannic Kilcher", "341": "Yannic Kilcher", "342": "Yannic Kilcher", "343": "Yannic Kilcher", "344": "Yannic Kilcher", "345": "Yannic Kilcher", "346": "Yannic Kilcher", "347": "Yannic Kilcher", "348": "Yannic Kilcher", "349": "Turkey Tom", "350": "Yannic Kilcher", "351": "Yannic Kilcher", "352": "Yannic Kilcher", "353": "Yannic Kilcher", "354": "Yannic Kilcher", "355": "Yannic Kilcher", "356": "Yannic Kilcher", "357": "Yannic Kilcher", "358": "Yannic Kilcher", "359": "Yannic Kilcher", "360": "Yannic Kilcher", "361": "Yannic Kilcher", "362": "Yannic Kilcher", "363": "Yannic Kilcher", "364": "Yannic Kilcher", "365": "Yannic Kilcher", "366": "Yannic Kilcher", "367": "Yannic Kilcher", "368": "Yannic Kilcher", "369": "Yannic Kilcher", "370": "Yannic Kilcher", "371": "Yannic Kilcher", "372": "Yannic Kilcher", "373": "Yannic Kilcher", "374": "Yannic Kilcher", "375": "Yannic Kilcher", "376": "Yannic Kilcher", "377": "Yannic Kilcher", "378": "Yannic Kilcher", "379": "Yannic Kilcher", "380": "Yannic Kilcher", "381": "Yannic Kilcher", "382": "Yannic Kilcher", "383": "Yannic Kilcher", "384": "Yannic Kilcher", "385": "Yannic Kilcher", "386": "Yannic Kilcher", "387": "Yannic Kilcher", "388": "Yannic Kilcher", "389": "Yannic Kilcher", "390": "Yannic Kilcher", "391": "Yannic Kilcher", "392": "Yannic Kilcher", "393": "Yannic Kilcher", "394": "Yannic Kilcher", "395": "Yannic Kilcher", "396": "Yannic Kilcher", "397": "Yannic Kilcher", "398": "Yannic Kilcher", "399": "Yannic Kilcher", "400": "Yannic Kilcher", "401": "Yannic Kilcher", "402": "Yannic Kilcher", "403": "Yannic Kilcher", "404": "Yannic Kilcher", "405": "Yannic Kilcher", "406": "Yannic Kilcher", "407": "Yannic Kilcher", "408": "Yannic Kilcher", "409": "Yannic Kilcher", "410": "Yannic Kilcher", "411": "Yannic Kilcher", "412": "Yannic Kilcher", "413": "Yannic Kilcher", "414": "Yannic Kilcher", "415": "Yannic Kilcher", "416": "Yannic Kilcher", "417": "Yannic Kilcher", "418": "Yannic Kilcher", "419": "Yannic Kilcher", "420": "Yannic Kilcher", "421": "Yannic Kilcher", "422": "Yannic Kilcher", "423": "Yannic Kilcher", "424": "Yannic Kilcher", "425": "Yannic Kilcher", "426": "Yannic Kilcher", "427": "Yannic Kilcher", "428": "Yannic Kilcher", "429": "Yannic Kilcher", "430": "Yannic Kilcher", "431": "Yannic Kilcher", "432": "Yannic Kilcher", "433": "Yannic Kilcher", "434": "Yannic Kilcher", "435": "Yannic Kilcher", "436": "Yannic Kilcher", "437": "Yannic Kilcher", "438": "Yannic Kilcher", "439": "Yannic Kilcher", "440": "Yannic Kilcher", "441": "Yannic Kilcher", "442": "Yannic Kilcher", "443": "Yannic Kilcher", "444": "Yannic Kilcher", "445": "Yannic Kilcher", "446": "Yannic Kilcher", "447": "Yannic Kilcher", "448": "Yannic Kilcher", "449": "Yannic Kilcher", "450": "Yannic Kilcher", "451": "Yannic Kilcher", "452": "Yannic Kilcher", "453": "Yannic Kilcher", "454": "Yannic Kilcher", "455": "Yannic Kilcher", "456": "Yannic Kilcher", "457": "Yannic Kilcher", "458": "Yannic Kilcher", "459": "Yannic Kilcher", "460": "CNCF [Cloud Native Computing Foundation]", "461": "Yannic Kilcher", "462": "Yannic Kilcher", "463": "Yannic Kilcher", "464": "Yannic Kilcher", "465": "Yannic Kilcher", "466": "Yannic Kilcher", "467": "Yannic Kilcher", "468": "Yannic Kilcher", "469": "Yannic Kilcher", "470": "Yannic Kilcher", "471": "Yannic Kilcher", "472": "Yannic Kilcher", "473": "Yannic Kilcher", "474": "Yannic Kilcher", "475": "Yannic Kilcher", "476": "Yannic Kilcher", "477": "Yannic Kilcher", "478": "Yannic Kilcher", "479": "Yannic Kilcher", "480": "Yannic Kilcher", "481": "Yannic Kilcher", "482": "Yannic Kilcher", "483": "Yannic Kilcher", "484": "Yannic Kilcher", "485": "Yannic Kilcher", "486": "Yannic Kilcher", "487": "Yannic Kilcher", "488": "Yannic Kilcher", "489": "Yannic Kilcher", "490": "Yannic Kilcher", "491": "Yannic Kilcher", "492": "Yannic Kilcher", "493": "Yannic Kilcher", "494": "Yannic Kilcher", "495": "Yannic Kilcher", "496": "Yannic Kilcher", "497": "Yannic Kilcher", "498": "Yannic Kilcher", "499": "Yannic Kilcher", "500": "Yannic Kilcher", "501": "Yannic Kilcher", "502": "Yannic Kilcher", "503": "Yannic Kilcher", "504": "Yannic Kilcher", "505": "Yannic Kilcher", "506": "Yannic Kilcher", "507": "Yannic Kilcher", "508": "Yannic Kilcher", "509": "Yannic Kilcher", "510": "Yannic Kilcher", "511": "Yannic Kilcher", "512": "Yannic Kilcher", "513": "Yannic Kilcher", "514": "Yannic Kilcher", "515": "Yannic Kilcher", "516": "Yannic Kilcher", "517": "Yannic Kilcher", "518": "Yannic Kilcher", "519": "Yannic Kilcher", "520": "Yannic Kilcher", "521": "Yannic Kilcher", "522": "Yannic Kilcher", "523": "Yannic Kilcher", "524": "Yannic Kilcher", "525": "Yannic Kilcher", "526": "Yannic Kilcher", "527": "Yannic Kilcher", "528": "Yannic Kilcher", "529": "Yannic Kilcher", "530": "Yannic Kilcher", "531": "Yannic Kilcher", "532": "Yannic Kilcher", "533": "Yannic Kilcher", "534": "DeepLearningAI", "535": "DeepLearningAI", "536": "DeepLearningAI", "537": "DeepLearningAI", "538": "DeepLearningAI", "539": "DeepLearningAI", "540": "DeepLearningAI", "541": "DeepLearningAI", "542": "DeepLearningAI", "543": "DeepLearningAI", "544": "DeepLearningAI", "545": "DeepLearningAI", "546": "DeepLearningAI", "547": "DeepLearningAI", "548": "DeepLearningAI", "549": "DeepLearningAI", "550": "DeepLearningAI", "551": "DeepLearningAI", "552": "DeepLearningAI", "553": "DeepLearningAI", "554": "DeepLearningAI", "555": "DeepLearningAI", "556": "DeepLearningAI", "557": "DeepLearningAI", "558": "DeepLearningAI", "559": "DeepLearningAI", "560": "DeepLearningAI", "561": "DeepLearningAI", "562": "DeepLearningAI", "563": "DeepLearningAI", "564": "DeepLearningAI", "565": "DeepLearningAI", "566": "DeepLearningAI", "567": "DeepLearningAI", "568": "DeepLearningAI", "569": "DeepLearningAI", "570": "DeepLearningAI", "571": "DeepLearningAI", "572": "DeepLearningAI", "573": "DeepLearningAI", "574": "DeepLearningAI", "575": "DeepLearningAI", "576": "DeepLearningAI", "577": "DeepLearningAI", "578": "DeepLearningAI", "579": "DeepLearningAI", "580": "DeepLearningAI", "581": "DeepLearningAI", "582": "DeepLearningAI", "583": "DeepLearningAI", "584": "DeepLearningAI", "585": "DeepLearningAI", "586": "DeepLearningAI", "587": "DeepLearningAI", "588": "DeepLearningAI", "589": "DeepLearningAI", "590": "DeepLearningAI", "591": "DeepLearningAI", "592": "DeepLearningAI", "593": "DeepLearningAI", "594": "DeepLearningAI", "595": "DeepLearningAI", "596": "DeepLearningAI", "597": "DeepLearningAI", "598": "DeepLearningAI", "599": "DeepLearningAI", "600": "DeepLearningAI", "601": "DeepLearningAI", "602": "DeepLearningAI", "603": "DeepLearningAI", "604": "DeepLearningAI", "605": "DeepLearningAI", "606": "DeepLearningAI", "607": "DeepLearningAI", "608": "DeepLearningAI", "609": "DeepLearningAI", "610": "DeepLearningAI", "611": "DeepLearningAI", "612": "DeepLearningAI", "613": "DeepLearningAI", "614": "DeepLearningAI", "615": "DeepLearningAI", "616": "DeepLearningAI", "617": "DeepLearningAI", "618": "DeepLearningAI", "619": "DeepLearningAI", "620": "DeepLearningAI", "621": "DeepLearningAI", "622": "DeepLearningAI", "623": "DeepLearningAI", "624": "DeepLearningAI", "625": "DeepLearningAI", "626": "DeepLearningAI", "627": "DeepLearningAI", "628": "DeepLearningAI", "629": "DeepLearningAI", "630": "DeepLearningAI", "631": "DeepLearningAI", "632": "DeepLearningAI", "633": "DeepLearningAI", "634": "DeepLearningAI", "635": "DeepLearningAI", "636": "DeepLearningAI", "637": "DeepLearningAI", "638": "DeepLearningAI", "639": "DeepLearningAI", "640": "DeepLearningAI", "641": "DeepLearningAI", "642": "DeepLearningAI", "643": "DeepLearningAI", "644": "DeepLearningAI", "645": "DeepLearningAI", "646": "DeepLearningAI", "647": "DeepLearningAI", "648": "DeepLearningAI", "649": "DeepLearningAI", "650": "DeepLearningAI", "651": "DeepLearningAI", "652": "DeepLearningAI", "653": "DeepLearningAI", "654": "DeepLearningAI", "655": "DeepLearningAI", "656": "DeepLearningAI", "657": "DeepLearningAI", "658": "DeepLearningAI", "659": "DeepLearningAI", "660": "DeepLearningAI", "661": "DeepLearningAI", "662": "DeepLearningAI", "663": "DeepLearningAI", "664": "DeepLearningAI", "665": "DeepLearningAI", "666": "DeepLearningAI", "667": "DeepLearningAI", "668": "DeepLearningAI", "669": "DeepLearningAI", "670": "DeepLearningAI", "671": "DeepLearningAI", "672": "DeepLearningAI", "673": "DeepLearningAI", "674": "DeepLearningAI", "675": "DeepLearningAI", "676": "Preserve Knowledge", "677": "DeepLearningAI", "678": "DeepLearningAI", "679": "DeepLearningAI", "680": "DeepLearningAI", "681": "DeepLearningAI", "682": "The Artificial Intelligence Channel", "683": "Lex Fridman", "684": "DeepLearningAI", "685": "DeepLearningAI", "686": "DeepLearningAI", "687": "DeepLearningAI", "688": "DeepLearningAI", "689": "DeepLearningAI", "690": "DeepLearningAI", "691": "DeepLearningAI", "692": "DeepLearningAI", "693": "DeepLearningAI", "694": "DeepLearningAI", "695": "DeepLearningAI", "696": "DeepLearningAI", "697": "DeepLearningAI", "698": "DeepLearningAI", "699": "DeepLearningAI", "700": "DeepLearningAI", "701": "DeepLearningAI", "702": "DeepLearningAI", "703": "DeepLearningAI", "704": "DeepLearningAI", "705": "DeepLearningAI", "706": "DeepLearningAI", "707": "DeepLearningAI", "708": "DeepLearningAI", "709": "DeepLearningAI", "710": "DeepLearningAI", "711": "DeepLearningAI", "712": "DeepLearningAI", "713": "DeepLearningAI", "714": "DeepLearningAI", "715": "DeepLearningAI", "716": "DeepLearningAI", "717": "DeepLearningAI", "718": "DeepLearningAI", "719": "DeepLearningAI", "720": "DeepLearningAI", "721": "DeepLearningAI", "722": "DeepLearningAI", "723": "DeepLearningAI", "724": "DeepLearningAI", "725": "DeepLearningAI", "726": "DeepLearningAI", "727": "DeepLearningAI", "728": "DeepLearningAI", "729": "DeepLearningAI", "730": "DeepLearningAI", "731": "DeepLearningAI", "732": "DeepLearningAI", "733": "DeepLearningAI", "734": "DeepLearningAI", "735": "DeepLearningAI", "736": "DeepLearningAI", "737": "DeepLearningAI", "738": "DeepLearningAI", "739": "DeepLearningAI", "740": "DeepLearningAI", "741": "DeepLearningAI", "742": "DeepLearningAI", "743": "DeepLearningAI", "744": "DeepLearningAI", "745": "DeepLearningAI", "746": "DeepLearningAI", "747": "DeepLearningAI", "748": "DeepLearningAI", "749": "DeepLearningAI", "750": "DeepLearningAI", "751": "DeepLearningAI", "752": "DeepLearningAI", "753": "DeepLearningAI", "754": "DeepLearningAI", "755": "DeepLearningAI", "756": "DeepLearningAI", "757": "DeepLearningAI", "758": "DeepLearningAI", "759": "DeepLearningAI", "760": "DeepLearningAI", "761": "DeepLearningAI", "762": "DeepLearningAI", "763": "DeepLearningAI", "764": "DeepLearningAI", "765": "DeepLearningAI", "766": "DeepLearningAI", "767": "DeepLearningAI", "768": "DeepLearningAI", "769": "DeepLearningAI", "770": "DeepLearningAI", "771": "DeepLearningAI", "772": "DeepLearningAI", "773": "DeepLearningAI", "774": "DeepLearningAI", "775": "DeepLearningAI", "776": "DeepLearningAI", "777": "DeepLearningAI", "778": "DeepLearningAI", "779": "DeepLearningAI", "780": "DeepLearningAI", "781": "DeepLearningAI", "782": "DeepLearningAI", "783": "DeepLearningAI", "784": "DeepLearningAI", "785": "DeepLearningAI", "786": "DeepLearningAI"}, "title": {"0": "[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)", "1": "[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)", "2": "[Classic] Generative Adversarial Networks (Paper Explained)", "3": "[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality", "4": "[Classic] Deep Residual Learning for Image Recognition (Paper Explained)", "5": "Machine Learning PhD Survival Guide 2021 | Advice on Topic Selection, Papers, Conferences & more!", "6": "Resolution-robust Large Mask Inpainting with Fourier Convolutions (w/ Author Interview)", "7": "Player of Games: All the games, one algorithm! (w/ author Martin Schmid)", "8": "This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)", "9": "Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)", "10": "Dynamic Inference with Neural Interpreters (w/ author interview)", "11": "Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)", "12": "GPT-NeoX-20B - Open-Source huge language model by EleutherAI (Interview w/ co-founder Connor Leahy)", "13": "Unsupervised Brain Models - How does Deep Learning inform Neuroscience? (w/ Patrick Mineault)", "14": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)", "15": "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)", "16": "AI against Censorship: Genetic Algorithms, The Geneva Project, ML in Security, and more!", "17": "CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)", "18": "All about AI Accelerators: GPU, TPU, Dataflow, Near-Memory, Optical, Neuromorphic & more (w/ Author)", "19": "Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)", "20": "Can Wikipedia Help Offline Reinforcement Learning? (Author Interview)", "21": "AlphaCode - with the authors!", "22": "First Author Interview: AI & formal math (Formal Mathematics Statement Curriculum Learning)", "23": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents", "24": "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis", "25": "Active Dendrites avoid catastrophic forgetting - Interview with the Authors", "26": "One Model For All The Tasks - BLIP (Author Interview)", "27": "Author Interview - Typical Decoding for Natural Language Generation", "28": "Author Interview - Memory-assisted prompt editing to improve GPT-3 after deployment", "29": "Author Interview - Improving Intrinsic Exploration with Language Abstractions", "30": "The Weird and Wonderful World of AI Art (w/ Author Jack Morris)", "31": "Author Interview - Transformer Memory as a Differentiable Search Index", "32": "Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)", "33": "LAION-5B: 5 billion image-text-pairs dataset (with the authors)", "34": "Author Interview - ACCEL: Evolving Curricula with Regret-Based Environment Design", "35": "Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "36": "The Future of AI is Self-Organizing and Self-Assembling (w/ Prof. Sebastian Risi)", "37": "More Is Different for AI - Scaling Up, Emergence, and Paperclip Maximizers (w/ Jacob Steinhardt)", "38": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "39": "Parti - Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Paper Explained)", "40": "[ML News] Text-to-Image models are taking over! (Imagen, DALL-E 2, Midjourney, CogView 2 & more)", "41": "The Man behind Stable Diffusion", "42": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust (Explained)", "43": "World Models", "44": "Curiosity-driven Exploration by Self-supervised Prediction", "45": "Reinforcement Learning with Unsupervised Auxiliary Tasks", "46": "Learning model-based planning from scratch", "47": "Imagination-Augmented Agents for Deep Reinforcement Learning", "48": "Reinforcement Learning, Fast and Slow", "49": "Learning World Graphs to Accelerate Hierarchical Reinforcement Learning", "50": "LeDeepChef \ud83d\udc68\u200d\ud83c\udf73 Deep Reinforcement Learning Agent for Families of Text-Based Games", "51": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures", "52": "AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning", "53": "A neurally plausible model learns successor representations in partially observable environments", "54": "MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "55": "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions", "56": "Go-Explore: a New Approach for Hard-Exploration Problems", "57": "Agent57: Outperforming the Atari Human Benchmark", "58": "Dream to Control: Learning Behaviors by Latent Imagination", "59": "POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Solutions", "60": "Enhanced POET: Open-Ended RL through Unbounded Invention of Learning Challenges and their Solutions", "61": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning", "62": "Datasets for Data-Driven Reinforcement Learning", "63": "Thinking While Moving: Deep Reinforcement Learning with Concurrent Control", "64": "The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies (Paper Explained)", "65": "Chip Placement with Deep Reinforcement Learning (Paper Explained)", "66": "Reinforcement Learning with Augmented Data (Paper Explained)", "67": "Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning (Paper Explained)", "68": "Planning to Explore via Self-Supervised World Models (Paper Explained)", "69": "Investigating Human Priors for Playing Video Games (Paper & Demo)", "70": "Regularizing Trajectory Optimization with Denoising Autoencoders (Paper Explained)", "71": "Dynamics-Aware Unsupervised Discovery of Skills (Paper Explained)", "72": "PCGRL: Procedural Content Generation via Reinforcement Learning (Paper Explained)", "73": "[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)", "74": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (Paper Explained)", "75": "Meta-Learning through Hebbian Plasticity in Random Networks (Paper Explained)", "76": "Fast reinforcement learning with generalized policy updates (Paper Explained)", "77": "Learning to summarize from human feedback (Paper Explained)", "78": "The XX   Intro HQ", "79": "Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess (Paper Explained)", "80": "ReBeL - Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (Explained)", "81": "Dreamer v2: Mastering Atari with Discrete World Models (Machine Learning Research Paper Explained)", "82": "Fast and Slow Learning of Recurrent Independent Mechanisms (Machine Learning Paper Explained)", "83": "Decision Transformer: Reinforcement Learning via Sequence Modeling (Research Paper Explained)", "84": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control (Paper Explained)", "85": "EfficientZero: Mastering Atari Games with Limited Data (Machine Learning Research Paper Explained)", "86": "Player of Games: All the games, one algorithm! (w/ author Martin Schmid)", "87": "Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)", "88": "Can Wikipedia Help Offline Reinforcement Learning? (Author Interview)", "89": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents", "90": "Improving Intrinsic Exploration with Language Abstractions (Machine Learning Paper Explained)", "91": "Author Interview - Improving Intrinsic Exploration with Language Abstractions", "92": "ACCEL: Evolving Curricula with Regret-Based Environment Design (Paper Review)", "93": "Author Interview - ACCEL: Evolving Curricula with Regret-Based Environment Design", "94": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)", "95": "Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "96": "This is a game changer! (AlphaTensor by DeepMind explained)", "97": "CICERO: An AI agent that negotiates, persuades, and cooperates with people", "98": "GPT-2: Language Models are Unsupervised Multitask Learners", "99": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "100": "Stochastic RNNs without Teacher-Forcing", "101": "Attention Is All You Need", "102": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "103": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "104": "LeDeepChef \ud83d\udc68\u200d\ud83c\udf73 Deep Reinforcement Learning Agent for Families of Text-Based Games", "105": "Reformer: The Efficient Transformer", "106": "Turing-NLG, DeepSpeed and the ZeRO optimizer", "107": "Deep Learning for Symbolic Mathematics", "108": "Evaluating NLP Models via Contrast Sets", "109": "Imputer: Sequence Modelling via Imputation and Dynamic Programming", "110": "Longformer: The Long-Document Transformer", "111": "I talk to the new Facebook Blender Chatbot", "112": "TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)", "113": "[Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)", "114": "[News] OpenAI Model Generates Python Code", "115": "When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)", "116": "GPT-3: Language Models are Few-Shot Learners (Paper Explained)", "117": "Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)", "118": "Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)", "119": "BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)", "120": "TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)", "121": "Linformer: Self-Attention with Linear Complexity (Paper Explained)", "122": "VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)", "123": "Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)", "124": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)", "125": "BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)", "126": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)", "127": "[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality", "128": "Big Bird: Transformers for Longer Sequences (Paper Explained)", "129": "Hopfield Networks is All You Need (Paper Explained)", "130": "REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)", "131": "Learning to summarize from human feedback (Paper Explained)", "132": "Rethinking Attention with Performers (Paper Explained)", "133": "Language Models are Open Knowledge Graphs (Paper Explained)", "134": "Extracting Training Data from Large Language Models (Paper Explained)", "135": "OpenAI DALL\u00b7E: Creating Images from Text (Blog Post Explained)", "136": "OpenAI CLIP: ConnectingText and Images (Paper Explained)", "137": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "138": "Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)", "139": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention (AI Paper Explained)", "140": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)", "141": "ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation", "142": "\u221e-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)", "143": "Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset", "144": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)", "145": "Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained", "146": "GPT-NeoX-20B - Open-Source huge language model by EleutherAI (Interview w/ co-founder Connor Leahy)", "147": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)", "148": "CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)", "149": "Competition-Level Code Generation with AlphaCode (Paper Review)", "150": "Typical Decoding for Natural Language Generation (Get more human-like outputs from language models!)", "151": "Author Interview - Typical Decoding for Natural Language Generation", "152": "Memory-assisted prompt editing to improve GPT-3 after deployment (Machine Learning Paper Explained)", "153": "Author Interview - Memory-assisted prompt editing to improve GPT-3 after deployment", "154": "Transformer Memory as a Differentiable Search Index (Machine Learning Research Paper Explained)", "155": "Author Interview - Transformer Memory as a Differentiable Search Index", "156": "Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)", "157": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)", "158": "Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "159": "Did Google's LaMDA chatbot just become sentient?", "160": "ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)", "161": "Galactica: A Large Language Model for Science (Drama & Paper Review)", "162": "ChatGPT: This AI has a JAILBREAK?! (Unbelievable AI Progress)", "163": "OpenAssistant - ChatGPT's Open Alternative (We need your help!)", "164": "LLaMA: Open and Efficient Foundation Language Models (Paper Explained)", "165": "GPT-4 is here! What we know so far (Full Analysis)", "166": "Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)", "167": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Full Paper Review)", "168": "RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)", "169": "Adversarial Examples Are Not Bugs, They Are Features", "170": "Evaluating NLP Models via Contrast Sets", "171": "Shortcut Learning in Deep Neural Networks", "172": "Extracting Training Data from Large Language Models (Paper Explained)", "173": "The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Research Paper Explained)", "174": "Population-Based Search and Open-Ended Algorithms", "175": "Conversation about Population-Based Methods (Re-upload)", "176": "Reconciling modern machine learning and the bias-variance trade-off", "177": "Accelerating Deep Learning by Focusing on the Biggest Losers", "178": "The Visual Task Adaptation Benchmark", "179": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "180": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "181": "Gradient Surgery for Multi-Task Learning", "182": "Feature Visualization & The OpenAI microscope", "183": "Shortcut Learning in Deep Neural Networks", "184": "Backpropagation and the brain", "185": "Supervised Contrastive Learning", "186": "Do ImageNet Classifiers Generalize to ImageNet? (Paper Explained)", "187": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Paper Explained)", "188": "Big Transfer (BiT): General Visual Representation Learning (Paper Explained)", "189": "Concept Learning with Energy-Based Models (Paper Explained)", "190": "Faster Neural Network Training with Data Echoing (Paper Explained)", "191": "A critical analysis of self-supervision, or what we can learn from a single image (Paper Explained)", "192": "iMAML: Meta-Learning with Implicit Gradients (Paper Explained)", "193": "mixup: Beyond Empirical Risk Minimization (Paper Explained)", "194": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 1: Foundations (Paper Explained)", "195": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 2: Human Priors (Paper Explained)", "196": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 4: The ARC Challenge (Paper Explained)", "197": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 3: The Math (Paper Explained)", "198": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)", "199": "Self-training with Noisy Student improves ImageNet classification (Paper Explained)", "200": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)", "201": "[Live Machine Learning Research] Plain Self-Ensembles (I actually DISCOVER SOMETHING) - Part 1", "202": "SupSup: Supermasks in Superposition (Paper Explained)", "203": "Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)", "204": "Deep Ensembles: A Loss Landscape Perspective (Paper Explained)", "205": "Neural Architecture Search without Training (Paper Explained)", "206": "Hopfield Networks is All You Need (Paper Explained)", "207": "Radioactive data: tracing through training (Paper Explained)", "208": "Self-classifying MNIST Digits (Paper Explained)", "209": "Training more effective learned optimizers, and using them to train themselves (Paper Explained)", "210": "Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Paper Explained)", "211": "Underspecification Presents Challenges for Credibility in Modern Machine Learning (Paper Explained)", "212": "Predictive Coding Approximates Backprop along Arbitrary Computation Graphs (Paper Explained)", "213": "Deep Networks Are Kernel Machines (Paper Explained)", "214": "Multimodal Neurons in Artificial Neural Networks (w/ OpenAI Microscope, Research Paper Explained)", "215": "Pretrained Transformers as Universal Computation Engines (Machine Learning Research Paper Explained)", "216": "Efficient and Modular Implicit Differentiation (Machine Learning Research Paper Explained)", "217": "PonderNet: Learning to Ponder (Machine Learning Research Paper Explained)", "218": "Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)", "219": "Gradients are Not All You Need (Machine Learning Research Paper Explained)", "220": "Learning Rate Grafting: Transferability of Optimizer Tuning (Machine Learning Research Paper Review)", "221": "Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Paper Explained)", "222": "Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)", "223": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis (Paper Explained)", "224": "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis", "225": "Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments (Review)", "226": "Active Dendrites avoid catastrophic forgetting - Interview with the Authors", "227": "LAION-5B: 5 billion image-text-pairs dataset (with the authors)", "228": "JEPA - A Path Towards Autonomous Machine Intelligence (Paper Explained)", "229": "How to make your CPU as fast as a GPU - Advances in Sparsity w/ Nir Shavit", "230": "This is a game changer! (AlphaTensor by DeepMind explained)", "231": "Neural Networks are Decision Trees (w/ Alexander Mattick)", "232": "Neural Ordinary Differential Equations", "233": "GPT-2: Language Models are Unsupervised Multitask Learners", "234": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "235": "What\u2019s in a name? The need to nip NIPS", "236": "Attention Is All You Need", "237": "Imagination-Augmented Agents for Deep Reinforcement Learning", "238": "Adversarial Examples Are Not Bugs, They Are Features", "239": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "240": "Manifold Mixup: Better Representations by Interpolating Hidden States", "241": "Dynamic Routing Between Capsules", "242": "DEEP LEARNING MEME REVIEW - Episode 1", "243": "AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning", "244": "MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "245": "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions", "246": "Reformer: The Efficient Transformer", "247": "WHO ARE YOU? 10k Subscribers Special (w/ Channel Analytics)", "248": "Concept Learning with Energy-Based Models (Paper Explained)", "249": "DETR: End-to-End Object Detection with Transformers (Paper Explained)", "250": "How I Read a Paper: Facebook's DETR (Video Tutorial)", "251": "MEMES IS ALL YOU NEED - Deep Learning Meme Review - Episode 2 (Part 1 of 2)", "252": "OpenAI DALL\u00b7E: Creating Images from Text (Blog Post Explained)", "253": "STOCHASTIC MEME DESCENT - Deep Learning Meme Review - Episode 2 (Part 2 of 2)", "254": "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)", "255": "Apple or iPod??? Easy Fix for Adversarial Textual Attacks on OpenAI's CLIP Model! #Shorts", "256": "Machine Learning PhD Survival Guide 2021 | Advice on Topic Selection, Papers, Conferences & more!", "257": "I BUILT A NEURAL NETWORK IN MINECRAFT | Analog Redstone Network w/ Backprop & Optimizer (NO MODS)", "258": "I COOKED A RECIPE MADE BY A.I. | Cooking with GPT-3 (Don't try this at home)", "259": "AI made this music video | What happens when OpenAI's CLIP meets BigGAN?", "260": "I took a Swiss train and it was awesome! Train Seat Review - SBB InterCity 1 - Geneva to St. Gallen", "261": "This A.I. creates infinite NFTs", "262": "The hidden dangers of loading open-source AI models (ARBITRARY CODE EXPLOIT!)", "263": "This is a game changer! (AlphaTensor by DeepMind explained)", "264": "ChatGPT: This AI has a JAILBREAK?! (Unbelievable AI Progress)", "265": "GPT-4chan: This is the worst AI ever", "266": "Did Google's LaMDA chatbot just become sentient?", "267": "OpenAssistant - ChatGPT's Open Alternative (We need your help!)", "268": "OpenAssistant First Models are here! (Open-Source ChatGPT)", "269": "OpenAssistant RELEASED! The world's best open-source Chat AI!", "270": "Neural Ordinary Differential Equations", "271": "The Odds are Odd: A Statistical Test for Detecting Adversarial Examples", "272": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "273": "Stochastic RNNs without Teacher-Forcing", "274": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations", "275": "Attention Is All You Need", "276": "Manifold Mixup: Better Representations by Interpolating Hidden States", "277": "Processing Megapixel Images with Deep Attention-Sampling Models", "278": "Gauge Equivariant Convolutional Networks and the Icosahedral CNN", "279": "Dynamic Routing Between Capsules", "280": "SinGAN: Learning a Generative Model from a Single Natural Image", "281": "Reformer: The Efficient Transformer", "282": "Growing Neural Cellular Automata", "283": "Deep Learning for Symbolic Mathematics", "284": "Axial Attention & MetNet: A Neural Weather Model for Precipitation Forecasting", "285": "POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Solutions", "286": "Evolving Normalization-Activation Layers", "287": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "288": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "289": "Feature Visualization & The OpenAI microscope", "290": "Longformer: The Long-Document Transformer", "291": "Jukebox: A Generative Model for Music (Paper Explained)", "292": "Big Transfer (BiT): General Visual Representation Learning (Paper Explained)", "293": "Group Normalization (Paper Explained)", "294": "Weight Standardization (Paper Explained)", "295": "DETR: End-to-End Object Detection with Transformers (Paper Explained)", "296": "Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)", "297": "Learning To Classify Images Without Labels (Paper Explained)", "298": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search (Paper Explained)", "299": "End-to-End Adversarial Text-to-Speech (Paper Explained)", "300": "Linformer: Self-Attention with Linear Complexity (Paper Explained)", "301": "VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)", "302": "SynFlow: Pruning neural networks without any data by iteratively conserving synaptic flow", "303": "A bio-inspired bistable recurrent cell allows for long-lasting memory (Paper Explained)", "304": "TUNIT: Rethinking the Truly Unsupervised Image-to-Image Translation (Paper Explained)", "305": "SIREN: Implicit Neural Representations with Periodic Activation Functions (Paper Explained)", "306": "RepNet: Counting Out Time - Class Agnostic Video Repetition Counting in the Wild (Paper Explained)", "307": "Discovering Symbolic Models from Deep Learning with Inductive Biases (Paper Explained)", "308": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)", "309": "Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection (Paper Explained)", "310": "Set Distribution Networks: a Generative Model for Sets of Images (Paper Explained)", "311": "Object-Centric Learning with Slot Attention (Paper Explained)", "312": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)", "313": "SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization (Paper Explained)", "314": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)", "315": "SupSup: Supermasks in Superposition (Paper Explained)", "316": "Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)", "317": "NVAE: A Deep Hierarchical Variational Autoencoder (Paper Explained)", "318": "Gradient Origin Networks (Paper Explained w/ Live Coding)", "319": "[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)", "320": "[Classic] Generative Adversarial Networks (Paper Explained)", "321": "[Classic] Deep Residual Learning for Image Recognition (Paper Explained)", "322": "Neural Architecture Search without Training (Paper Explained)", "323": "Big Bird: Transformers for Longer Sequences (Paper Explained)", "324": "Hopfield Networks is All You Need (Paper Explained)", "325": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation (Paper Explained)", "326": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)", "327": "LambdaNetworks: Modeling long-range Interactions without Attention (Paper Explained)", "328": "Rethinking Attention with Performers (Paper Explained)", "329": "Fourier Neural Operator for Parametric Partial Differential Equations (Paper Explained)", "330": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "331": "Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)", "332": "NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)", "333": "TransGAN: Two Transformers Can Make One Strong GAN (Machine Learning Research Paper Explained)", "334": "Linear Transformers Are Secretly Fast Weight Memory Systems (Machine Learning Paper Explained)", "335": "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)", "336": "Perceiver: General Perception with Iterative Attention (Google DeepMind Research Paper Explained)", "337": "MLP-Mixer: An all-MLP Architecture for Vision (Machine Learning Research Paper Explained)", "338": "Involution: Inverting the Inherence of Convolution for Visual Recognition (Research Paper Explained)", "339": "Expire-Span: Not All Memories are Created Equal: Learning to Forget by Expiring (Paper Explained)", "340": "Fastformer: Additive Attention Can Be All You Need (Machine Learning Research Paper Explained)", "341": "Autoregressive Diffusion Models (Machine Learning Research Paper Explained)", "342": "Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained", "343": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion (ML Research Paper Explained)", "344": "Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)", "345": "Dynamic Inference with Neural Interpreters (w/ author interview)", "346": "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)", "347": "CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)", "348": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding&Generation", "349": "The Pewdiepie Story: Maintaining A YouTube Empire", "350": "One Model For All The Tasks - BLIP (Author Interview)", "351": "Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)", "352": "RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)", "353": "[ML Coding Tips] Separate Computation & Plotting using locals", "354": "[Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)", "355": "Do ImageNet Classifiers Generalize to ImageNet? (Paper Explained)", "356": "Jukebox: A Generative Model for Music (Paper Explained)", "357": "TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)", "358": "Reinforcement Learning with Augmented Data (Paper Explained)", "359": "Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning (Paper Explained)", "360": "Big Transfer (BiT): General Visual Representation Learning (Paper Explained)", "361": "Concept Learning with Energy-Based Models (Paper Explained)", "362": "Group Normalization (Paper Explained)", "363": "Faster Neural Network Training with Data Echoing (Paper Explained)", "364": "Weight Standardization (Paper Explained)", "365": "A critical analysis of self-supervision, or what we can learn from a single image (Paper Explained)", "366": "iMAML: Meta-Learning with Implicit Gradients (Paper Explained)", "367": "mixup: Beyond Empirical Risk Minimization (Paper Explained)", "368": "Planning to Explore via Self-Supervised World Models (Paper Explained)", "369": "Deep image reconstruction from human brain activity (Paper Explained)", "370": "Investigating Human Priors for Playing Video Games (Paper & Demo)", "371": "When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)", "372": "Regularizing Trajectory Optimization with Denoising Autoencoders (Paper Explained)", "373": "DETR: End-to-End Object Detection with Transformers (Paper Explained)", "374": "GPT-3: Language Models are Few-Shot Learners (Paper Explained)", "375": "Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)", "376": "Dynamics-Aware Unsupervised Discovery of Skills (Paper Explained)", "377": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 1: Foundations (Paper Explained)", "378": "Learning To Classify Images Without Labels (Paper Explained)", "379": "CornerNet: Detecting Objects as Paired Keypoints (Paper Explained)", "380": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search (Paper Explained)", "381": "BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)", "382": "TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)", "383": "End-to-End Adversarial Text-to-Speech (Paper Explained)", "384": "Linformer: Self-Attention with Linear Complexity (Paper Explained)", "385": "VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)", "386": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 2: Human Priors (Paper Explained)", "387": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 4: The ARC Challenge (Paper Explained)", "388": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 3: The Math (Paper Explained)", "389": "PCGRL: Procedural Content Generation via Reinforcement Learning (Paper Explained)", "390": "Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)", "391": "SynFlow: Pruning neural networks without any data by iteratively conserving synaptic flow", "392": "A bio-inspired bistable recurrent cell allows for long-lasting memory (Paper Explained)", "393": "TUNIT: Rethinking the Truly Unsupervised Image-to-Image Translation (Paper Explained)", "394": "BYOL: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (Paper Explained)", "395": "Image GPT: Generative Pretraining from Pixels (Paper Explained)", "396": "Big Self-Supervised Models are Strong Semi-Supervised Learners (Paper Explained)", "397": "SIREN: Implicit Neural Representations with Periodic Activation Functions (Paper Explained)", "398": "RepNet: Counting Out Time - Class Agnostic Video Repetition Counting in the Wild (Paper Explained)", "399": "How I Read a Paper: Facebook's DETR (Video Tutorial)", "400": "Discovering Symbolic Models from Deep Learning with Inductive Biases (Paper Explained)", "401": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)", "402": "Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection (Paper Explained)", "403": "Set Distribution Networks: a Generative Model for Sets of Images (Paper Explained)", "404": "Self-training with Noisy Student improves ImageNet classification (Paper Explained)", "405": "Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)", "406": "Object-Centric Learning with Slot Attention (Paper Explained)", "407": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)", "408": "BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)", "409": "SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization (Paper Explained)", "410": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)", "411": "SupSup: Supermasks in Superposition (Paper Explained)", "412": "Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)", "413": "NVAE: A Deep Hierarchical Variational Autoencoder (Paper Explained)", "414": "Gradient Origin Networks (Paper Explained w/ Live Coding)", "415": "Deep Ensembles: A Loss Landscape Perspective (Paper Explained)", "416": "[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)", "417": "[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)", "418": "[Classic] Generative Adversarial Networks (Paper Explained)", "419": "[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality", "420": "[Classic] Deep Residual Learning for Image Recognition (Paper Explained)", "421": "Neural Architecture Search without Training (Paper Explained)", "422": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (Paper Explained)", "423": "Hopfield Networks is All You Need (Paper Explained)", "424": "Meta-Learning through Hebbian Plasticity in Random Networks (Paper Explained)", "425": "REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)", "426": "Fast reinforcement learning with generalized policy updates (Paper Explained)", "427": "Radioactive data: tracing through training (Paper Explained)", "428": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation (Paper Explained)", "429": "Self-classifying MNIST Digits (Paper Explained)", "430": "Learning to summarize from human feedback (Paper Explained)", "431": "The Hardware Lottery (Paper Explained)", "432": "Training more effective learned optimizers, and using them to train themselves (Paper Explained)", "433": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)", "434": "Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Paper Explained)", "435": "LambdaNetworks: Modeling long-range Interactions without Attention (Paper Explained)", "436": "Rethinking Attention with Performers (Paper Explained)", "437": "Language Models are Open Knowledge Graphs (Paper Explained)", "438": "Underspecification Presents Challenges for Credibility in Modern Machine Learning (Paper Explained)", "439": "Fourier Neural Operator for Parametric Partial Differential Equations (Paper Explained)", "440": "Predictive Coding Approximates Backprop along Arbitrary Computation Graphs (Paper Explained)", "441": "DeepMind's AlphaFold 2 Explained! AI Breakthrough in Protein Folding! What we know (& what we don't)", "442": "ReBeL - Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (Explained)", "443": "Extracting Training Data from Large Language Models (Paper Explained)", "444": "OpenAI DALL\u00b7E: Creating Images from Text (Blog Post Explained)", "445": "OpenAI CLIP: ConnectingText and Images (Paper Explained)", "446": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "447": "SingularityNET - A Decentralized, Open Market and Network for AIs (Whitepaper Explained)", "448": "Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)", "449": "Deep Networks Are Kernel Machines (Paper Explained)", "450": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention (AI Paper Explained)", "451": "NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)", "452": "TransGAN: Two Transformers Can Make One Strong GAN (Machine Learning Research Paper Explained)", "453": "Dreamer v2: Mastering Atari with Discrete World Models (Machine Learning Research Paper Explained)", "454": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)", "455": "Linear Transformers Are Secretly Fast Weight Memory Systems (Machine Learning Paper Explained)", "456": "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)", "457": "Multimodal Neurons in Artificial Neural Networks (w/ OpenAI Microscope, Research Paper Explained)", "458": "Yann LeCun - Self-Supervised Learning: The Dark Matter of Intelligence (FAIR Blog Post Explained)", "459": "Pretrained Transformers as Universal Computation Engines (Machine Learning Research Paper Explained)", "460": "Webinar: MLOps automation with Git Based CI/CD for ML", "461": "Perceiver: General Perception with Iterative Attention (Google DeepMind Research Paper Explained)", "462": "DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning", "463": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ML Research Paper Explained)", "464": "Why AI is Harder Than We Think (Machine Learning Research Paper Explained)", "465": "DINO: Emerging Properties in Self-Supervised Vision Transformers (Facebook AI Research Explained)", "466": "Involution: Inverting the Inherence of Convolution for Visual Recognition (Research Paper Explained)", "467": "DDPM - Diffusion Models Beat GANs on Image Synthesis (Machine Learning Research Paper Explained)", "468": "Expire-Span: Not All Memories are Created Equal: Learning to Forget by Expiring (Paper Explained)", "469": "Fast and Slow Learning of Recurrent Independent Mechanisms (Machine Learning Paper Explained)", "470": "Reward Is Enough (Machine Learning Research Paper Explained)", "471": "Decision Transformer: Reinforcement Learning via Sequence Modeling (Research Paper Explained)", "472": "Efficient and Modular Implicit Differentiation (Machine Learning Research Paper Explained)", "473": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control (Paper Explained)", "474": "XCiT: Cross-Covariance Image Transformers (Facebook AI Machine Learning Research Paper Explained)", "475": "The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Research Paper Explained)", "476": "How Apple scans your phone (and how to evade it) - NeuralHash CSAM Detection Algorithm Explained", "477": "PonderNet: Learning to Ponder (Machine Learning Research Paper Explained)", "478": "Fastformer: Additive Attention Can Be All You Need (Machine Learning Research Paper Explained)", "479": "ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation", "480": "\u221e-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)", "481": "Topographic VAEs learn Equivariant Capsules (Machine Learning Research Paper Explained)", "482": "Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset", "483": "Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment (Paper Explained)", "484": "How far can we scale up? Deep Learning's Diminishing Returns (Article Review)", "485": "Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)", "486": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)", "487": "EfficientZero: Mastering Atari Games with Limited Data (Machine Learning Research Paper Explained)", "488": "Autoregressive Diffusion Models (Machine Learning Research Paper Explained)", "489": "Gradients are Not All You Need (Machine Learning Research Paper Explained)", "490": "Learning Rate Grafting: Transferability of Optimizer Tuning (Machine Learning Research Paper Review)", "491": "Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Paper Explained)", "492": "Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained", "493": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion (ML Research Paper Explained)", "494": "Resolution-robust Large Mask Inpainting with Fourier Convolutions (w/ Author Interview)", "495": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "496": "Player of Games: All the games, one algorithm! (w/ author Martin Schmid)", "497": "This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)", "498": "Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)", "499": "Dynamic Inference with Neural Interpreters (w/ author interview)", "500": "Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)", "501": "Unsupervised Brain Models - How does Deep Learning inform Neuroscience? (w/ Patrick Mineault)", "502": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)", "503": "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)", "504": "AI against Censorship: Genetic Algorithms, The Geneva Project, ML in Security, and more!", "505": "CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)", "506": "Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)", "507": "Competition-Level Code Generation with AlphaCode (Paper Review)", "508": "OpenAI tackles Math - Formal Mathematics Statement Curriculum Learning (Paper Explained)", "509": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents", "510": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis (Paper Explained)", "511": "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis", "512": "Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments (Review)", "513": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding&Generation", "514": "Typical Decoding for Natural Language Generation (Get more human-like outputs from language models!)", "515": "Memory-assisted prompt editing to improve GPT-3 after deployment (Machine Learning Paper Explained)", "516": "Improving Intrinsic Exploration with Language Abstractions (Machine Learning Paper Explained)", "517": "Transformer Memory as a Differentiable Search Index (Machine Learning Research Paper Explained)", "518": "ACCEL: Evolving Curricula with Regret-Based Environment Design (Paper Review)", "519": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)", "520": "Parti - Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Paper Explained)", "521": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Paper Explained)", "522": "JEPA - A Path Towards Autonomous Machine Intelligence (Paper Explained)", "523": "This is a game changer! (AlphaTensor by DeepMind explained)", "524": "Neural Networks are Decision Trees (w/ Alexander Mattick)", "525": "ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)", "526": "Galactica: A Large Language Model for Science (Drama & Paper Review)", "527": "CICERO: An AI agent that negotiates, persuades, and cooperates with people", "528": "LLaMA: Open and Efficient Foundation Language Models (Paper Explained)", "529": "GPT-4 is here! What we know so far (Full Analysis)", "530": "Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)", "531": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Full Paper Review)", "532": "RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)", "533": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust (Explained)", "534": "Train/Dev/Test Sets (C2W1L01)", "535": "Bias/Variance (C2W1L02)", "536": "Basic Recipe for Machine Learning (C2W1L03)", "537": "Regularization (C2W1L04)", "538": "Why Regularization Reduces Overfitting (C2W1L05)", "539": "Dropout Regularization (C2W1L06)", "540": "Understanding Dropout (C2W1L07)", "541": "Other Regularization Methods (C2W1L08)", "542": "Normalizing Inputs (C2W1L09)", "543": "Vanishing/Exploding Gradients (C2W1L10)", "544": "Weight Initialization in a Deep Network (C2W1L11)", "545": "Numerical Approximations of Gradients (C2W1L12)", "546": "Gradient Checking (C2W1L13)", "547": "Gradient Checking Implementation Notes (C2W1L14)", "548": "Mini Batch Gradient Descent (C2W2L01)", "549": "Understanding Mini-Batch Gradient Dexcent (C2W2L02)", "550": "Exponentially Weighted Averages (C2W2L03)", "551": "Understanding Exponentially Weighted Averages (C2W2L04)", "552": "Bias Correction of Exponentially Weighted Averages (C2W2L05)", "553": "Gradient Descent With Momentum (C2W2L06)", "554": "RMSProp (C2W2L07)", "555": "Adam Optimization Algorithm (C2W2L08)", "556": "Learning Rate Decay (C2W2L09)", "557": "Tuning Process (C2W3L01)", "558": "Using an Appropriate Scale (C2W3L02)", "559": "Hyperparameter Tuning in Practice (C2W3L03)", "560": "Normalizing Activations in a Network (C2W3L04)", "561": "Fitting Batch Norm Into Neural Networks (C2W3L05)", "562": "Why Does Batch Norm Work? (C2W3L06)", "563": "Batch Norm At Test Time (C2W3L07)", "564": "Softmax Regression (C2W3L08)", "565": "Training Softmax Classifier (C2W3L09)", "566": "The Problem of Local Optima (C2W3L10)", "567": "TensorFlow (C2W3L11)", "568": "Welcome (Deep Learning Specialization C1W1L01)", "569": "What is a Neural Network? (C1W1L02)", "570": "Supervised Learning with a Neural Network (C1W1L03)", "571": "Why is deep learning taking off? (C1W1L04)", "572": "About This Course (C1W1L05)", "573": "Course Resources (C1W1L06)", "574": "Binary Classification (C1W2L01)", "575": "Logistic Regression (C1W2L02)", "576": "Logistic Regression Cost Function (C1W2L03)", "577": "Gradient Descent (C1W2L04)", "578": "Derivatives (C1W2L05)", "579": "More Derivative Examples (C1W2L06)", "580": "Computation Graph (C1W2L07)", "581": "Derivatives With Computation Graphs (C1W2L08)", "582": "Logistic Regression Gradient Descent (C1W2L09)", "583": "Gradient Descent on m Examples (C1W2L10)", "584": "Vectorization (C1W2L11)", "585": "More Vectorization Examples (C1W2L12)", "586": "Vectorizing Logistic Regression (C1W2L13)", "587": "Vectorizing Logistic Regression's Gradient Computation (C1W2L14)", "588": "Broadcasting in Python (C1W2L15)", "589": "A Note on Python/Numpy Vectors (C1W2L16)", "590": "Quick Tour of Jupyter/iPython Notebooks (C1W2L17)", "591": "Explanation of Logistic Regression's Cost Function (C1W2L18)", "592": "Neural Network Overview (C1W3L01)", "593": "Neural Network Representations (C1W3L02)", "594": "Computing Neural Network Output (C1W3L03)", "595": "Vectorizing Across Multiple Examples (C1W3L04)", "596": "Explanation For Vectorized Implementation (C1W3L05)", "597": "Activation Functions (C1W3L06)", "598": "Why Non-linear Activation Functions (C1W3L07)", "599": "Derivatives Of Activation Functions (C1W3L08)", "600": "Gradient Descent For Neural Networks (C1W3L09)", "601": "Backpropagation Intuition (C1W3L10)", "602": "Random Initialization (C1W3L11)", "603": "Deep L-Layer Neural Network (C1W4L01)", "604": "Forward Propagation in a Deep Network (C1W4L02)", "605": "Getting Matrix Dimensions Right (C1W4L03)", "606": "Why Deep Representations? (C1W4L04)", "607": "Building Blocks of a Deep Neural Network (C1W4L05)", "608": "Forward and Backward Propagation (C1W4L06)", "609": "Parameters vs Hyperparameters (C1W4L07)", "610": "What does this have to do with the brain? (C1W4L08)", "611": "Train/Dev/Test Sets (C2W1L01)", "612": "Bias/Variance (C2W1L02)", "613": "Basic Recipe for Machine Learning (C2W1L03)", "614": "Regularization (C2W1L04)", "615": "Why Regularization Reduces Overfitting (C2W1L05)", "616": "Dropout Regularization (C2W1L06)", "617": "Understanding Dropout (C2W1L07)", "618": "Other Regularization Methods (C2W1L08)", "619": "Normalizing Inputs (C2W1L09)", "620": "Vanishing/Exploding Gradients (C2W1L10)", "621": "Weight Initialization in a Deep Network (C2W1L11)", "622": "Numerical Approximations of Gradients (C2W1L12)", "623": "Gradient Checking (C2W1L13)", "624": "Gradient Checking Implementation Notes (C2W1L14)", "625": "Mini Batch Gradient Descent (C2W2L01)", "626": "Understanding Mini-Batch Gradient Dexcent (C2W2L02)", "627": "Exponentially Weighted Averages (C2W2L03)", "628": "Understanding Exponentially Weighted Averages (C2W2L04)", "629": "Bias Correction of Exponentially Weighted Averages (C2W2L05)", "630": "Gradient Descent With Momentum (C2W2L06)", "631": "RMSProp (C2W2L07)", "632": "Adam Optimization Algorithm (C2W2L08)", "633": "Learning Rate Decay (C2W2L09)", "634": "Tuning Process (C2W3L01)", "635": "Using an Appropriate Scale (C2W3L02)", "636": "Hyperparameter Tuning in Practice (C2W3L03)", "637": "Normalizing Activations in a Network (C2W3L04)", "638": "Fitting Batch Norm Into Neural Networks (C2W3L05)", "639": "Why Does Batch Norm Work? (C2W3L06)", "640": "Batch Norm At Test Time (C2W3L07)", "641": "Softmax Regression (C2W3L08)", "642": "Training Softmax Classifier (C2W3L09)", "643": "The Problem of Local Optima (C2W3L10)", "644": "TensorFlow (C2W3L11)", "645": "Improving Model Performance (C3W1L01)", "646": "Orthogonalization (C3W1L02 )", "647": "Single Number Evaluation Metric (C3W1L03)", "648": "Satisficing and Optimizing Metrics (C3W1L04)", "649": "Train/Dev/Test Set Distributions (C3W1L05)", "650": "Sizeof Dev and Test Sets (C3W1L06)", "651": "When to Change Dev/Test Sets (C3W1L07)", "652": "C3W1L08 WhyHumanLevelPerformance", "653": "Avoidable Bias (C3W1L09)", "654": "Understanding Human-Level Performance? (C3W1L10)", "655": "Surpassing Human-Level Performance (C3W1L11)", "656": "Improving Model Performance (C3W1L12)", "657": "Carrying Out Error Analysis (C3W2L01)", "658": "Cleaning Up Incorrectly Labelled Data (C3W2L02)", "659": "Build First System Quickly, Then Iterate (C3W2L03)", "660": "Training and Testing on Different Distributions (C3W2L04)", "661": "Bias and Variance With Mismatched Data (C3W2L05)", "662": "Addressing Data Mismatch (C3W2L06)", "663": "Transfer Learning (C3W2L07)", "664": "Multitask Learning (C3W2L08)", "665": "What is end-to-end deep learning? (C3W2L09)", "666": "Whether to Use End-To-End Deep Learning (C3W2L10)", "667": "deeplearning.ai's Heroes of Deep Learning: Ruslan Salakhutdinov", "668": "deeplearning.ai's Heroes of Deep Learning: Yuanqing Lin", "669": "deeplearning.ai's Heroes of Deep Learning: Ian Goodfellow", "670": "deeplearning.ai's Heroes of Deep Learning: Yoshua Bengio", "671": "deeplearning.ai's Heroes of Deep Learning: Pieter Abbeel", "672": "deeplearning.ai's Heroes of Deep Learning: Andrej Karpathy", "673": "deeplearning.ai's Heroes of Deep Learning: Yann LeCun", "674": "deeplearning.ai's Heroes of Deep Learning: Dawn Song", "675": "Heroes of Deep Learning: Pieter Abbeel and Andrej Karpathy on Getting Started", "676": "Heroes of Deep Learning: Andrew Ng interviews Geoffrey Hinton", "677": "Heroes of Deep Learning: Yann LeCun and Ruslan Salakhutdinov on Getting Started", "678": "Pie & AI Bogot\u00e1", "679": "Pie & AI Medell\u00edn: A Discussion with Andrew Ng and Helmuth Trefftz", "680": "Andrew Ng at Amazon re:MARS 2019", "681": "Pie & AI: TensorFlow Specialization Launch @ Google HQ", "682": "Andrew Ng - The State of Artificial Intelligence", "683": "Andrew Ng: Deep Learning, Education, and Real-World AI | Lex Fridman Podcast #73", "684": "Why Computer Vision is a Hard Problem (TensorFlow in Practice)", "685": "What AI Can and Cannot Do (AI For Everyone)", "686": "Training in the Browser (TensorFlow: Data and Deployment)", "687": "Coming soon: a new deeplearning.ai Specialization", "688": "How to pick AI projects (AI For Everyone)", "689": "Four advanced deployment scenarios (TensorFlow: Data and Deployment)", "690": "Augmenting Data (TensorFlow in Practice)", "691": "New AI For Medicine Specialization coming soon!", "692": "Practical Challenges Training with Medical Data (AI For Medicine)", "693": "AI For Prognosis (AI For Medicine)", "694": "How deep learning can detect cancerous tissue (AI For Medicine)", "695": "C5W3L01 Basic Models", "696": "C5W3L02 Picking the most likely sentence", "697": "C5W3L06 Bleu Score (Optional)", "698": "C5W3L07 Attention Model Intuition", "699": "C5W3L08 Attention Model", "700": "C5W3L09 SpeechRecog", "701": "C4W1L01 Computer Vision", "702": "C4W1L02 Edge Detection Examples", "703": "C4W1L03 More Edge Detection", "704": "C4W1L04 Padding", "705": "C4W1L05 Strided Convolutions", "706": "C4W1L06 Convolutions Over Volumes", "707": "C4W1L07 One Layer of a Convolutional Net", "708": "C4W1L08 Simple Convolutional Network Example", "709": "C4W1L09 Pooling Layers", "710": "C4W1L10 CNN Example", "711": "C4W1L11 Why Convolutions", "712": "C4W2L01 Why look at case studies?", "713": "C4W2L02 Classic Network", "714": "C4W2L03 Resnets", "715": "C4W2L04 Why ResNets Work", "716": "C4W2L05 Network In Network", "717": "C4W2L06 Inception Network Motivation", "718": "C4W2L07 Inception Network", "719": "C4W2L08 Using Open Source Implementation", "720": "C4W2L09 Transfer Learning", "721": "C4W2L10 Data Augmentation", "722": "C4W2L11 State of Computer Vision", "723": "C4W3L01 Object Localization", "724": "C4W3L02 Landmark Detection", "725": "C4W3L03 Object Detection", "726": "C4W3L04 Convolutional Implementation Sliding Windows", "727": "C4W3L06 Intersection Over Union", "728": "C4W3L07 Nonmax Suppression", "729": "C4W3L08 Anchor Boxes", "730": "C4W3L09 YOLO Algorithm", "731": "C4W3L10 Region Proposals", "732": "C4W4L01 What is face recognition", "733": "C4W4L02 One Shot Learning", "734": "C4W4L03 Siamese Network", "735": "C4W4L04 Triplet loss", "736": "C4W4L05 Face Verification", "737": "C4W4L06 What is neural style transfer?", "738": "C4W4L07 What are deep CNs learning?", "739": "C4W4L08 Cost Function", "740": "C4W4L09 Content Cost Function", "741": "C4W4L10 Style Cost Function", "742": "C4W4L11 1D and 3D Generalizations", "743": "Heroes of NLP: Chris Manning", "744": "Heroes of NLP: Kathleen McKeown", "745": "Heroes of NLP: Oren Etzioni", "746": "Heroes of NLP: Quoc Le", "747": "#1 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 1]", "748": "#2 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 2]", "749": "#3 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 3]", "750": "#4 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 4]", "751": "#5 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 5]", "752": "#6 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 6]", "753": "#7 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 7]", "754": "#8 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 8]", "755": "#9 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 1]", "756": "#10 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 2]", "757": "#11 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 3]", "758": "#12 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 4]", "759": "#13 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 5]", "760": "#14 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 6]", "761": "#15 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 7]", "762": "#16 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 8]", "763": "#17 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 9]", "764": "#18 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 10]", "765": "#19 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 11]", "766": "#20 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 12]", "767": "#21 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 13]", "768": "#22 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 14]", "769": "#23 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 15]", "770": "#24 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 16]", "771": "#25 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 1]", "772": "#26 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3 Lesson 2]", "773": "#27 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 3]", "774": "#28 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 4]", "775": "#29 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 5]", "776": "#30 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 6]", "777": "#31 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 7]", "778": "#32 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 8]", "779": "#33 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 9]", "780": "#34 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 10]", "781": "#35 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 11]", "782": "#36 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 12]", "783": "#37 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 13]", "784": "#38 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 14]", "785": "#39 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 15]", "786": "#40 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 16]"}, "description": {"0": "#ai #dqn #deepmind\n\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Arcade Learning Environment\n4:25 - Deep Reinforcement Learning\n9:20 - Deep Q-Learning\n26:30 - Experience Replay\n32:25 - Network Architecture\n33:50 - Experiments\n37:45 - Conclusion\n\nPaper: https://arxiv.org/abs/1312.5602\n\nAbstract:\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "1": "#ai #research #alexnet\n\nAlexNet was the start of the deep learning revolution. Up until 2012, the best computer vision systems relied on hand-crafted features and highly specialized algorithms to perform object classification. This paper was the first to successfully train a deep convolutional neural network on not one, but two GPUs and managed to outperform the competition on ImageNet by an order of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - The necessity of larger models\n6:20 - Why CNNs?\n11:05 - ImageNet\n12:05 - Model Architecture Overview\n14:35 - ReLU Nonlinearities\n18:45 - Multi-GPU training\n21:30 - Classification Results\n24:30 - Local Response Normalization\n28:05 - Overlapping Pooling\n32:25 - Data Augmentation\n38:30 - Dropout\n40:30 - More Results\n43:50 - Conclusion\n\nPaper: http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\n\nAbstract:\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n\nAuthors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "2": "#ai #deeplearning #gan\n\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:50 - Motivation\n8:40 - Minimax Loss Function\n13:20 - Intuition Behind the Loss\n19:30 - GAN Algorithm\n22:05 - Theoretical Analysis\n27:00 - Experiments\n33:10 - Advantages & Disadvantages\n35:00 - Conclusion\n\nPaper: https://arxiv.org/abs/1406.2661\n\nAbstract:\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "3": "#ai #research #word2vec\n\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:50 - Distributed Word Representations\n5:40 - Skip-Gram Model\n12:00 - Hierarchical Softmax\n14:55 - Negative Sampling\n22:30 - Mysterious 3/4 Power\n25:50 - Frequent Words Subsampling\n28:15 - Empirical Results\n29:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1310.4546\nCode: https://code.google.com/archive/p/word2vec/\n\nAbstract:\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\nAuthors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "4": "#ai #research #resnet\n\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper's invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:45 - The Problem with Depth\n3:15 - VGG-Style Networks\n6:00 - Overfitting is Not the Problem\n7:25 - Motivation for Residual Connections\n10:25 - Residual Blocks\n12:10 - From VGG to ResNet\n18:50 - Experimental Results\n23:30 - Bottleneck Blocks\n24:40 - Deeper ResNets\n28:15 - More Results\n29:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1512.03385\n\nAbstract:\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "5": "#machinelearning #phd #howto\n\nThis video is advice for new PhD students in the field of Machine Learning in 2021 and after. The field has shifted dramatically in the last few years and navigating grad school can be very hard, especially when you're as clueless as I was when I started. The video is a personal recount of my mistakes and what I've learned from them. If you already have several published papers and know what to do, this video is not for you. However, if you are not even sure where to start, how to select a topic, or what goes in a paper, you might benefit from this video, because that's exactly how I felt.\n\nMain Takeaways:\n- Select niche topics rather than hype topics\n- Write papers that can't be rejected\n- Don't be discouraged by bad reviews\n- Take reviewing & teaching seriously\n- Keep up your focus\n- Conferences are for networking\n- Internships are great opportunities\n- Team up with complementary skills\n- Don't work too hard\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Thesis Topic Selection\n4:25 - How To Publish Papers\n5:35 - Dealing With Reviewers\n6:30 - How To Be A Reviewer\n7:40 - Take Teaching Seriously\n8:30 - Maintain Focus\n10:20 - Navigating Conferences\n12:40 - Internships\n13:40 - Collaborations\n14:55 - Don't Forget To Enjoy\n\nTranscript: https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae\n\nCredits to Lanz for editing\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "6": "#lama #inpainting #deeplearning\n\nAt the end of the video is an interview with the paper authors!\nLaMa is a system that is amazing at removing foreground objects from images, especially when those objects cover a large part of the image itself. LaMa is specifically trained to reconstruct large masked areas and includes global information throughout its forward propagation by using Fourier Convolutions in its layers. This makes it incredibly effective at reconstructing periodic structures with long-range consistency, compared to regular convolutions.\n\nOUTLINE:\n0:00 - Intro\n0:45 - Sponsor: ClearML\n3:30 - Inpainting Examples\n5:05 - Live Demo\n6:40 - Locality as a weakness of convolutions\n10:30 - Using Fourier Transforms for global information\n12:55 - Model architecture overview\n14:35 - Fourier convolution layer\n21:15 - Loss function\n24:25 - Mask generation algorithm\n25:40 - Experimental results\n28:25 - Interview with the authors\n\nPaper: https://arxiv.org/abs/2109.07161\nCode: https://github.com/saic-mdal/lama\nOnline Demo: https://cleanup.pictures/\n\nSponsor: ClearML\nhttps://clear.ml\n\nAbstract:\nModern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter&time costs than the competitive baselines. The code is available at \\url{this https URL}.\n\nAuthors: Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "7": "#playerofgames #deepmind #alphazero\n\nSpecial Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\nGames have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\n\nOUTLINE:\n0:00 - Introduction\n2:50 - What games can Player of Games be trained on?\n4:00 - Tree search algorithms (AlphaZero)\n8:00 - What is different in imperfect information games?\n15:40 - Counterfactual Value- and Policy-Networks\n18:50 - The Player of Games search procedure\n28:30 - How to train the network?\n34:40 - Experimental Results\n47:20 - Discussion & Outlook\n\nPaper: https://arxiv.org/abs/2112.03178\n\nAbstract:\nGames have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.\n\nAuthors: Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "8": "#minerl #minecraft #deeplearning\n\nThe MineRL BASALT challenge has no reward functions or technical descriptions of what's to be achieved. Instead, the goal of each task is given as a short natural language string, and the agent is evaluated by a team of human judges who rate both how well the goal has been fulfilled, as well as how human-like the agent behaved. In this video, I interview KAIROS, the winning team of the 2021 challenge, and discuss how they used a combination of machine learning, efficient data collection, hand engineering, and a bit of knowledge about Minecraft to beat all other teams.\n\nOUTLINE:\n0:00 - Introduction\n4:10 - Paper Overview\n11:15 - Start of Interview\n17:05 - First Approach\n20:30 - State Machine\n26:45 - Efficient Label Collection\n30:00 - Navigation Policy\n38:15 - Odometry Estimation\n46:00 - Pain Points & Learnings\n50:40 - Live Run Commentary\n58:50 - What other tasks can be solved?\n1:01:55 - What made the difference?\n1:07:30 - Recommendations & Conclusion\n1:11:10 - Full Runs: Waterfall\n1:12:40 - Full Runs: Build House\n1:17:45 - Full Runs: Animal Pen\n1:20:50 - Full Runs: Find Cave\n\nPaper: https://arxiv.org/abs/2112.03482\nCode: https://github.com/viniciusguigo/kairos_minerl_basalt\nChallenge Website: https://minerl.io/basalt/\n\nPaper Title: Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft\n\nAbstract:\nReal-world tasks of interest are generally poorly defined by human-readable descriptions and have no pre-defined reward signals unless it is defined by a human designer. Conversely, data-driven algorithms are often designed to solve a specific, narrowly defined, task with performance metrics that drives the agent's learning. In this work, we present the solution that won first place and was awarded the most human-like agent in the 2021 NeurIPS Competition MineRL BASALT Challenge: Learning from Human Feedback in Minecraft, which challenged participants to use human data to solve four tasks defined only by a natural language description and no reward function. Our approach uses the available human demonstration data to train an imitation learning policy for navigation and additional human feedback to train an image classifier. These modules, together with an estimated odometry map, are then combined into a state-machine designed based on human knowledge of the tasks that breaks them down in a natural hierarchy and controls which macro behavior the learning agent should follow at any instant. We compare this hybrid intelligence approach to both end-to-end machine learning and pure engineered solutions, which are then judged by human evaluators. Codebase is available at this https URL.\n\nAuthors: Vinicius G. Goecks, Nicholas Waytowich, David Watkins, Bharat Prakash\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "9": "#deeplearning #noether #symmetries\n\nThis video includes an interview with first author Ferran Alet!\nEncoding inductive biases has been a long established methods to provide deep networks with the ability to learn from less data. Especially useful are encodings of symmetry properties of the data, such as the convolution's translation invariance. But such symmetries are often hard to program explicitly, and can only be encoded exactly when done in a direct fashion. Noether Networks use Noether's theorem connecting symmetries to conserved quantities and are able to dynamically and approximately enforce symmetry properties upon deep neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n18:10 - Interview Start\n21:20 - Symmetry priors vs conserved quantities\n23:25 - Example: Pendulum\n27:45 - Noether Network Model Overview\n35:35 - Optimizing the Noether Loss\n41:00 - Is the computation graph stable?\n46:30 - Increasing the inference time computation\n48:45 - Why dynamically modify the model?\n55:30 - Experimental Results & Discussion\n\nPaper: https://arxiv.org/abs/2112.03321\nWebsite: https://dylandoblar.github.io/noether-networks/\nCode: https://github.com/dylandoblar/noether-networks\n\nAbstract:\nProgress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.\n\nAuthors: Ferran Alet, Dylan Doblar, Allan Zhou, Joshua Tenenbaum, Kenji Kawaguchi, Chelsea Finn\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "10": "#deeplearning #neuralinterpreter #ai\n\nThis video includes an interview with the paper's authors!\nWhat if we treated deep networks like modular programs? Neural Interpreters divide computation into small modules and route data to them via a dynamic type inference system. The resulting model combines recurrent elements, weight sharing, attention, and more to tackle both abstract reasoning, as well as computer vision tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Model Overview\n7:00 - Interpreter weights and function code\n9:40 - Routing data to functions via neural type inference\n14:55 - ModLin layers\n18:25 - Experiments\n21:35 - Interview Start\n24:50 - General Model Structure\n30:10 - Function code and signature\n40:30 - Explaining Modulated Layers\n49:50 - A closer look at weight sharing\n58:30 - Experimental Results\n\nPaper: https://arxiv.org/abs/2110.06399\n\nGuests:\nNasim Rahaman: https://twitter.com/nasim_rahaman\nFrancesco Locatello: https://twitter.com/FrancescoLocat8\nWaleed Gondal: https://twitter.com/Wallii_gondal\n\nAbstract:\nModern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \\emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization\n\nAuthors: Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, Bernhard Sch\u00f6lkopf\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "11": "#deeplearning #symbolic #research\n\nThis video includes an interview with first author St\u00e9phane d'Ascoli (https://sdascoli.github.io/).\nDeep neural networks are typically excellent at numeric regression, but using them for symbolic computation has largely been ignored so far. This paper uses transformers to do symbolic regression on integer and floating point number sequences, which means that given the start of a sequence of numbers, the model has to not only predict the correct continuation, but also predict the data generating formula behind the sequence. Through clever encoding of the input space and a well constructed training data generation process, this paper's model can learn and represent many of the sequences in the OEIS, the online encyclopedia of integer sequences and it also features an interactive demo if you want to try it by yourself. \n\nOUTLINE:\n0:00 - Introduction\n2:20 - Summary of the Paper\n16:10 - Start of Interview\n17:15 - Why this research direction?\n20:45 - Overview of the method\n30:10 - Embedding space of input tokens\n33:00 - Data generation process\n42:40 - Why are transformers useful here?\n46:40 - Beyond number sequences, where is this useful?\n48:45 - Success cases and failure cases\n58:10 - Experimental Results\n1:06:30 - How did you overcome difficulties?\n1:09:25 - Interactive demo\n\nPaper: https://arxiv.org/abs/2201.04600\nInteractive demo: https://symbolicregression.metademolab.com/\n\nAbstract:\nSymbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. bessel0(x)\u2248sin(x)+cos(x)\u03c0x\u221a and 1.644934\u2248\u03c02/6. An interactive demonstration of our models is provided at this https URL.\n\nAuthors: St\u00e9phane d'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, Fran\u00e7ois Charton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "12": "#eleuther #gptneo #gptj\n\nEleutherAI announces GPT-NeoX-20B, a 20 billion parameter open-source language model, inspired by GPT-3. Connor joins me to discuss the process of training, how the group got their hands on the necessary hardware, what the new model can do, and how anyone can try it out!\n\nOUTLINE:\n0:00 - Intro\n1:00 - Start of interview\n2:00 - How did you get all the hardware?\n3:50 - What's the scale of this model?\n6:00 - A look into the experimental results\n11:15 - Why are there GPT-Neo, GPT-J, and GPT-NeoX?\n14:15 - How difficult is training these big models?\n17:00 - Try out the model on GooseAI\n19:00 - Final thoughts\n\nRead the announcement: https://blog.eleuther.ai/announcing-20b/\nTry out the model: https://goose.ai/\nCheck out EleutherAI: https://www.eleuther.ai/\nRead the code: https://github.com/EleutherAI/gpt-neox\nHardware sponsor: https://www.coreweave.com/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "13": "#deeplearning #brain #neuroscience\n\nOriginally, Deep Learning sprang into existence inspired by how the brain processes information, but the two fields have diverged ever since. However, given that deep models can solve many perception tasks with remarkable accuracy, is it possible that we might be able to learn something about how the brain works by inspecting our models? I speak to Patrick Mineault about his blog post \"2021 in review: unsupervised brain models\" and we explore why neuroscientists are taking interest in unsupervised and self-supervised deep neural networks in order to explain how the brain works. We discuss a series of influential papers that have appeared last year, and we go into the more general questions of connecting neuroscience and machine learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:35 - Start of Interview\n10:30 - Visual processing in the brain\n12:50 - How does deep learning inform neuroscience?\n21:15 - Unsupervised training explains the ventral stream\n30:50 - Predicting own motion parameters explains the dorsal stream\n42:20 - Why are there two different visual streams?\n49:45 - Concept cells and representation learning\n56:20 - Challenging the manifold theory\n1:08:30 - What are current questions in the field?\n1:13:40 - Should the brain inform deep learning?\n1:18:50 - Neuromatch Academy and other endeavours\n\nBlog Post: https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/\nPatrick's Blog: https://xcorr.net/\nTwitter: https://twitter.com/patrickmineault\nNeuromatch Academy: https://academy.neuromatch.io/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "14": "#gpt3 #embodied #planning\n\nIn this video: Paper explanation, followed by first author interview with Wenlong Huang.\nLarge language models contain extraordinary amounts of world knowledge that can be queried in various ways. But their output format is largely uncontrollable. This paper investigates the VirtualHome environment, which expects a particular set of actions, objects, and verbs to be used. Turns out, with proper techniques and only using pre-trained models (no fine-tuning), one can translate unstructured language model outputs into the structured grammar of the environment. This is potentially very useful anywhere where the models' world knowledge needs to be provided in a particular structured format.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:45 - The VirtualHome environment\n6:25 - The problem of plan evaluation\n8:40 - Contributions of this paper\n16:40 - Start of interview\n24:00 - How to use language models with environments?\n34:00 - What does model size matter?\n40:00 - How to fix the large models' outputs?\n55:00 - Possible improvements to the translation procedure\n59:00 - Why does Codex perform so well?\n1:02:15 - Diving into experimental results\n1:14:15 - Future outlook\n\nPaper: https://arxiv.org/abs/2201.07207\nWebsite: https://wenlong.page/language-planner/\nCode: https://github.com/huangwl18/language-planner\nWenlong's Twitter: https://twitter.com/wenlong_huang\n\nAbstract:\nCan world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \"make breakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into low-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at this https URL\n\nAuthors: Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "15": "#hypertransformer #metalearning #deeplearning\n\nThis video contains a paper explanation and an interview with author Andrey Zhmoginov!\nFew-shot learning is an interesting sub-field in meta-learning, with wide applications, such as creating personalized models based on just a handful of data points. Traditionally, approaches have followed the BERT approach where a large model is pre-trained and then fine-tuned. However, this couples the size of the final model to the size of the model that has been pre-trained. Similar problems exist with \"true\" meta-learners, such as MaML. HyperTransformer fundamentally decouples the meta-learner from the size of the final model by directly predicting the weights of the final model. The HyperTransformer takes the few-shot dataset as a whole into its context and predicts either one or multiple layers of a (small) ConvNet, meaning its output are the weights of the convolution filters. Interestingly, and with the correct engineering care, this actually appears to deliver promising results and can be extended in many ways.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:05 - Weight-generation vs Fine-tuning for few-shot learning\n10:10 - HyperTransformer model architecture overview\n22:30 - Why the self-attention mechanism is useful here\n34:45 - Start of Interview\n39:45 - Can neural networks even produce weights of other networks?\n47:00 - How complex does the computational graph get?\n49:45 - Why are transformers particularly good here?\n58:30 - What can the attention maps tell us about the algorithm?\n1:07:00 - How could we produce larger weights?\n1:09:30 - Diving into experimental results\n1:14:30 - What questions remain open?\n\nPaper: https://arxiv.org/abs/2201.04182\n\nERRATA: I introduce Max Vladymyrov as Mark Vladymyrov\n\nAbstract:\nIn this work we propose a HyperTransformer, a transformer-based model for few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable. Finally, we extend our approach to a semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance.\n\nAuthors: Andrey Zhmoginov, Mark Sandler, Max Vladymyrov\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "16": "#security #censorship #ai\n\nMost of us conceive the internet as a free and open space where we are able to send traffic between any two nodes, but for large parts of the world this is not the case. Entire nations have large machinery in place to survey all internet traffic and automated procedures to block any undesirable connections. Evading such censorship has been largely a cat-and-mouse game between security researchers and government actors. A new system, called Geneva, uses a Genetic Algorithm in combination with Evolutionary Search in order to dynamically evade such censorship and adjust itself in real-time to any potential response by its adversaries. In this video, I talk to Security researcher Kevin Bock, who is one of Geneva's main contributors and member of the Breakerspace project. We talk about the evolution of internet censorship, how to evade it, how to mess with the censors' infrastructure, as well as the broader emerging connections between AI and Security.\n\nOUTLINE:\n0:00 - Intro\n3:30 - What is automated censorship in networks?\n7:20 - The evolution of censorship vs evasion\n12:40 - Why do we need a dynamic, evolving system?\n16:30 - The building blocks of Geneva\n23:15 - Introducing evolution\n28:30 - What's the censors' response?\n31:45 - How was Geneva's media reception?\n33:15 - Where do we go from here?\n37:30 - Can we deliberately attack the censors?\n47:00 - On responsible disclosure\n49:40 - Breakerspace: Security research for undergrads\n50:40 - How often do you get into trouble?\n52:10 - How can I get started in security?\n\nLearn more at:\n- Geneva (& more) project page: https://censorship.ai\n- Open Observatory of Network Interference: https://ooni.org\n- Censored Planet: https://censoredplanet.org\n- Breakerspace: https://breakerspace.cs.umd.edu\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "17": "#cm3 #languagemodel #transformer\n\nThis video contains a paper explanation and an incredibly informative interview with first author Armen Aghajanyan.\nAutoregressive Transformers have come to dominate many fields in Machine Learning, from text generation to image creation and many more. However, there are two problems. First, the collected data is usually scraped from the web and uni- or bi-modal and throws away a lot of structure of the original websites, and second, language modelling losses are uni-directional. CM3 addresses both problems: It directly operates on HTML and includes text, hyperlinks, and even images (via VQGAN tokenization) and can therefore be used in plenty of ways: Text generation, captioning, image creation, entity linking, and much more. It also introduces a new training strategy called Causally Masked Language Modelling, which brings a level of bi-directionality into autoregressive language modelling. In the interview after the paper explanation, Armen and I go deep into the how and why of these giant models, we go over the stunning results and we make sense of what they mean for the future of universal models.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:30 - Directly learning the structure of HTML\n12:30 - Causally Masked Language Modelling\n18:50 - A short look at how to use this model\n23:20 - Start of interview\n25:30 - Feeding language models with HTML\n29:45 - How to get bi-directionality into decoder-only Transformers?\n37:00 - Images are just tokens\n41:15 - How does one train such giant models?\n45:40 - CM3 results are amazing\n58:20 - Large-scale dataset collection and content filtering\n1:04:40 - More experimental results\n1:12:15 - Why don't we use raw HTML?\n1:18:20 - Does this paper contain too many things?\n\nPaper: https://arxiv.org/abs/2201.07520\n\nAbstract:\nWe introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.\n\nAuthors: Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "18": "#ai #gpu #tpu\n\nThis video is an interview with Adi Fuchs, author of a series called \"AI Accelerators\", and an expert in modern AI acceleration technology.\nAccelerators like GPUs and TPUs are an integral part of today's AI landscape. Deep Neural Network training can be sped up by orders of magnitudes by making good use of these specialized pieces of hardware. However, GPUs and TPUs are only the beginning of a vast landscape of emerging technologies and companies that build accelerators for the next generation of AI models. In this interview, we go over many aspects of building hardware for AI, including why GPUs have been so successful, what the most promising approaches look like, how they work, and what the main challenges are.\n\nOUTLINE:\n0:00 - Intro\n5:10 - What does it mean to make hardware for AI?\n8:20 - Why were GPUs so successful?\n16:25 - What is \"dark silicon\"?\n20:00 - Beyond GPUs: How can we get even faster AI compute?\n28:00 - A look at today's accelerator landscape\n30:00 - Systolic Arrays and VLIW\n35:30 - Reconfigurable dataflow hardware\n40:50 - The failure of Wave Computing\n42:30 - What is near-memory compute?\n46:50 - Optical and Neuromorphic Computing\n49:50 - Hardware as enabler and limiter\n55:20 - Everything old is new again\n1:00:00 - Where to go to dive deeper?\n\nRead the full blog series here:\nPart I: https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4\nPart II: https://medium.com/@adi.fu7/ai-accelerators-part-ii-transistors-and-pizza-or-why-do-we-need-accelerators-75738642fdaa\nPart III: https://medium.com/@adi.fu7/ai-accelerators-part-iii-architectural-foundations-3f1f73d61f1f\nPart  IV: https://medium.com/@adi.fu7/ai-accelerators-part-iv-the-very-rich-landscape-17481be80917\nPart V: https://medium.com/@adi.fu7/ai-accelerators-part-v-final-thoughts-94eae9dbfafb\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "19": "#wikipedia #reinforcementlearning #languagemodels\n\nTransformers have come to overtake many domain-targeted custom models in a wide variety of fields, such as Natural Language Processing, Computer Vision, Generative Modelling, and recently also Reinforcement Learning. This paper looks at the Decision Transformer and shows that, surprisingly, pre-training the model on a language-modelling task significantly boosts its performance on Offline Reinforcement Learning. The resulting model achieves higher scores, can get away with less parameters, and exhibits superior scaling properties. This raises many questions about the fundamental connection between the domains of language and RL.\n\nOUTLINE:\n0:00 - Intro\n1:35 - Paper Overview\n7:35 - Offline Reinforcement Learning as Sequence Modelling\n12:00 - Input Embedding Alignment & other additions\n16:50 - Main experimental results\n20:45 - Analysis of the attention patterns across models\n32:25 - More experimental results (scaling properties, ablations, etc.)\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "20": "#wikipedia #reinforcementlearning #languagemodels\n\nOriginal paper review here: https://youtu.be/XHGh19Hbx48\n\nMachel Reid and Yutaro Yamada join me to discuss their recent paper on langauge model pre-training for decision transformers in offline reinforcement learning.\n\nOUTLINE:\n0:00 - Intro\n1:00 - Brief paper, setup & idea recap\n7:30 - Main experimental results & high standard deviations\n10:00 - Why is there no clear winner?\n13:00 - Why are bigger models not a lot better?\n14:30 - What\u2019s behind the name ChibiT?\n15:30 - Why is iGPT underperforming?\n19:15 - How are tokens distributed in Reinforcement Learning?\n22:00 - What other domains could have good properties to transfer?\n24:20 - A deeper dive into the models' attention patterns\n33:30 - Codebase, model sizes, and compute requirements\n37:30 - Scaling behavior of pre-trained models\n40:05 - What did not work out in this project?\n42:00 - How can people get started and where to go next?\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "21": "#ai #alphacode #deepmind\n\nAn interview with the creators of AlphaCode!\nPaper review video here: https://youtu.be/s9UAOmyah1A\n\nOUTLINE:\n0:00 - Intro\n1:10 - Media Reception\n5:10 - How did the project go from start to finish?\n9:15 - Does the model understand its own code?\n14:45 - Are there plans to reduce the number of samples?\n16:15 - Could one do smarter filtering of samples?\n18:55 - How crucial are the public test cases?\n21:55 - Could we imagine an adversarial method?\n24:45 - How are coding problems even made?\n27:40 - Does AlphaCode evaluate a solution's asymptotic complexity?\n33:15 - Are our sampling procedures inappropriate for diversity?\n36:30 - Are all generated solutions as instructive as the example?\n41:30 - How are synthetic examples created during training?\n42:30 - What were high and low points during this research?\n45:25 - What was the most valid criticism after publication?\n47:40 - What are applications in the real world?\n51:00 - Where do we go from here?\n\nPaper: https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\nCode: https://github.com/deepmind/code_contests\n\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. Evaluated on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in programming competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\n\nAuthors: Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu and Oriol Vinyals\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "22": "#openai #math #imo\n\nThis is an interview with Stanislas Polu, research engineer at OpenAI and first author of the paper \"Formal Mathematics Statement Curriculum Learning\".\nWatch the paper review here: https://youtu.be/lvYVuOmUVs8\n\nOUTLINE:\n0:00 - Intro\n2:00 - How do you explain the big public reaction?\n4:00 - What's the history behind the paper?\n6:15 - How does algorithmic formal math work?\n13:10 - How does expert iteration replace self-play?\n22:30 - How is the language model trained and used?\n30:50 - Why is every model fine-tuned on the initial state?\n33:05 - What if we want to prove something we don't know already?\n40:35 - How can machines and humans work together?\n43:40 - Aren't most produced statements useless?\n46:20 - A deeper look at the experimental results\n50:10 - What were the high and low points during the research?\n54:25 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2202.01344\nminiF2F benchmark: https://github.com/openai/miniF2F\nFollow Stan here: https://twitter.com/spolu\n\nAbstract:\nWe explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.\n\nAuthors: Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "23": "#deepmind #rl #society\n\nThis is an in-depth paper review, followed by an interview with the papers' authors!\nSociety is ruled by norms, and most of these norms are very useful, such as washing your hands before cooking. However, there also exist plenty of social norms which are essentially arbitrary, such as what hairstyles are acceptable, or what words are rude. These are called \"silly rules\". This paper uses multi-agent reinforcement learning to investigate why such silly rules exist. Their results indicate a plausible mechanism, by which the existence of silly rules drastically speeds up the agents' acquisition of the skill of enforcing rules, which generalizes well, and therefore a society that has silly rules will be better at enforcing rules in general, leading to faster adaptation in the face of genuinely useful norms.\n\nOUTLINE:\n0:00 - Intro\n3:00 - Paper Overview\n5:20 - Why are some social norms arbitrary?\n11:50 - Reinforcement learning environment setup\n20:00 - What happens if we introduce a \"silly\" rule?\n25:00 - Experimental Results: how silly rules help society\n30:10 - Isolated probing experiments\n34:30 - Discussion of the results\n37:30 - Start of Interview\n39:30 - Where does the research idea come from?\n44:00 - What is the purpose behind this research?\n49:20 - Short recap of the mechanics of the environment\n53:00 - How much does such a closed system tell us about the real world?\n56:00 - What do the results tell us about silly rules?\n1:01:00 - What are these agents really learning?\n1:08:00 - How many silly rules are optimal?\n1:11:30 - Why do you have separate weights for each agent?\n1:13:45 - What features could be added next?\n1:16:00 - How sensitive is the system to hyperparameters?\n1:17:20 - How to avoid confirmation bias?\n1:23:15 - How does this play into progress towards AGI?\n1:29:30 - Can we make real-world recommendations based on this?\n1:32:50 - Where do we go from here?\n\nPaper: https://www.pnas.org/doi/10.1073/pnas.2106028118\nBlog: https://deepmind.com/research/publications/2021/Spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents\n\nAbstract:\nThe fact that humans enforce and comply with norms is an important reason why humans enjoy higher levels of cooperation and welfare than other animals. Some norms are relatively easy to explain; they may prohibit obviously harmful or uncooperative actions. But many norms are not easy to explain. For example, most cultures prohibit eating certain kinds of foods and almost all societies have rules about what constitutes appropriate clothing, language, and gestures. Using a computational model focused on learning shows that apparently pointless rules can have an indirect effect on welfare. They can help agents learn how to enforce and comply with norms in general, improving the group\u2019s ability to enforce norms that have a direct effect on welfare.\n\nAuthors: Raphael K\u00f6ster, Dylan Hadfield-Menell, Richard Everett, Laura Weidinger, Gillian K. Hadfield, Joel Z. Leibo\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "24": "#deeplearning #objectdetection #outliers\n\nAn interview with the authors of \"Virtual Outlier Synthesis\".\nWatch the paper review video here: https://youtu.be/i-J4T3uLC9M\n\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\n\nOUTLINE:\n0:00 - Intro\n2:20 - What was the motivation behind this paper?\n5:30 - Why object detection?\n11:05 - What's the connection to energy-based models?\n12:15 - Is a Gaussian mixture model appropriate for high-dimensional data?\n16:15 - What are the most important components of the method?\n18:30 - What are the downstream effects of the regularizer?\n22:00 - Are there severe trade-offs to outlier detection?\n23:55 - Main experimental takeaways?\n26:10 - Why do outlier detection in the last layer?\n30:20 - What does it take to finish a research projects successfully?\n\nPaper: https://arxiv.org/abs/2202.01197\nCode: https://github.com/deeplearning-wisc/vos\n\nAbstract:\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\n\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "25": "#multitasklearning #biology #neuralnetworks\n\nThis is an interview with the paper's authors: Abhiram Iyer, Karan Grewal, and Akash Velu!\nPaper Review Video: https://youtu.be/O_dJ31T01i8\n\nCheck out Zak's course on Graph Neural Networks (discount with this link): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\n\nOUTLINE:\n0:00 - Intro\n0:55 - Sponsor: GNN Course\n2:30 - How did the idea come to be?\n7:05 - What roles do the different parts of the method play?\n8:50 - What was missing in the paper review?\n10:35 - Are biological concepts viable if we still have backprop?\n11:50 - How many dendrites are necessary?\n14:10 - Why is there a plateau in the sparsity plot?\n20:50 - How does task difficulty play into the algorithm?\n24:10 - Why are there different setups in the experiments?\n30:00 - Is there a place for unsupervised pre-training?\n32:50 - How can we apply the online prototyping to more difficult tasks?\n37:00 - What did not work out during the project?\n41:30 - How do you debug a project like this?\n47:10 - How is this related to other architectures?\n51:10 - What other things from neuroscience are to be included?\n55:50 - Don't miss the awesome ending :)\n\nPaper: https://arxiv.org/abs/2201.00042\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\n\nLink to the GNN course (with discount): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nAbstract:\nA key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\n\nAuthors: Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "26": "#blip #interview #salesforce\n\nPaper Review Video: https://youtu.be/X2k7n4FuI7c\nSponsor: Assembly AI\nhttps://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic2\n\nThis is an interview with Junnan Li and Dongxu Li, authors of BLIP and members of Salesforce research.\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Assembly AI\n1:30 - Start of Interview\n2:30 - What's the pitch?\n4:40 - How did data bootstrapping come into the project?\n7:10 - How big of a problem is data quality?\n11:10 - Are the captioning & filtering models biased towards COCO data?\n14:40 - Could the data bootstrapping be done multiple times?\n16:20 - What was the evolution of the BLIP architecture?\n21:15 - Are there additional benefits to adding language modelling?\n23:50 - Can we imagine a modular future for pre-training?\n29:45 - Diving into the experimental results\n42:40 - What did and did not work out during the research?\n45:00 - How is research life at Salesforce?\n46:45 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2201.12086\nCode: https://github.com/salesforce/BLIP\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\n\nAbstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\n\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "27": "#deeplearning #nlp #sampling\n\nThis is an interview with first author Clara Meister.\nPaper review video here\u00e9 https://youtu.be/_EDr3ryrT_Y\n\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\n\nSponsor: Introduction to Graph Neural Networks Course\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nOUTLINE:\n0:00 - Intro\n0:35 - Sponsor: Introduction to GNNs Course (link in description)\n1:30 - Why does sampling matter?\n5:40 - What is a \"typical\" message?\n8:35 - How do humans communicate?\n10:25 - Why don't we just sample from the model's distribution?\n15:30 - What happens if we condition on the information to transmit?\n17:35 - Does typical sampling really represent human outputs?\n20:55 - What do the plots mean?\n31:00 - Diving into the experimental results\n39:15 - Are our training objectives wrong?\n41:30 - Comparing typical sampling to top-k and nucleus sampling\n44:50 - Explaining arbitrary engineering choices\n47:20 - How can people get started with this?\n\nPaper: https://arxiv.org/abs/2202.00666\nCode: https://github.com/cimeister/typical-sampling/blob/3e676cfd88fa2e6a24f2bdc6f9f07fddb87827c2/src/transformers/generation_logits_process.py#L242-L272\n\nAbstract:\nDespite achieving incredibly low perplexities on myriad natural language corpora, today's language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (\u00e0 la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.\n\nAuthors: Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "28": "#nlp #gpt3 #prompt\n\nThis is an interview with the authors of this work, Aman Madaan and Niket Tandon.\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\n\nOUTLINE:\n0:00 - Intro\n0:45 - Paper Overview\n2:00 - What was your original motivation?\n4:20 - There is an updated version of the paper!\n9:00 - Have you studied this on real-world users?\n12:10 - How does model size play into providing feedback?\n14:10 - Can this be used for personalization?\n16:30 - Discussing experimental results\n17:45 - Can this be paired with recommender systems?\n20:00 - What are obvious next steps to make the system more powerful?\n23:15 - Clarifying the baseline methods\n26:30 - Exploring cross-lingual customization\n31:00 - Where did the idea for the clarification prompt come from?\n33:05 - What did not work out during this project?\n34:45 - What did you learn about interacting with large models?\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.06009\nCode & Data: https://github.com/madaan/memprompt\n\nAbstract:\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \"What word is similar to good?\" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. All the code and data is available at this https URL.\n\nAuthors: Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "29": "#reinforcementlearning #ai #explained\n\nThis is an interview with Jesse Mu, first author of the paper.\nOriginal Paper Review: https://youtu.be/NeGJAUSQEJI\n\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\n\nOUTLINE:\n0:00 - Intro\n0:55 - Paper Overview\n4:30 - Aren't you just adding extra data?\n9:35 - Why are you splitting up the AMIGo teacher?\n13:10 - How do you train the grounding network?\n16:05 - What about causally structured environments?\n17:30 - Highlights of the experimental results\n20:40 - Why is there so much variance?\n22:55 - How much does it matter that we are testing in a video game?\n27:00 - How does novelty interface with the goal specification?\n30:20 - The fundamental problems of exploration\n32:15 - Are these algorithms subject to catastrophic forgetting?\n34:45 - What current models could bring language to other environments?\n40:30 - What does it take in terms of hardware?\n43:00 - What problems did you encounter during the project?\n46:40 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2202.08938\n\nAbstract:\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\n\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rockt\u00e4schel, Edward Grefenstette\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "30": "#aiart #deeplearning #clip\n\nSince the release of CLIP, the world of AI art has seen an unprecedented level of acceleration in what's possible to do. Whereas image generation had previously been mostly in the domain of scientists, now a community of professional artists, researchers, and amateurs are sending around colab notebooks and sharing their creations via social media. How did this happen? What is going on? And where do we go from here? Jack Morris and I attempt to answer some of these questions, following his blog post \"The Weird and Wonderful World of AI Art\" (linked below).\n\nOUTLINE:\n0:00 - Intro\n2:30 - How does one get into AI art?\n5:00 - Deep Dream & Style Transfer: the early days of art in deep learning\n10:50 - The advent of GANs, ArtBreeder and TikTok\n19:50 - Lacking control: Pre-CLIP art\n22:40 - CLIP & DALL-E\n30:20 - The shift to shared colabs\n34:20 - Guided diffusion models\n37:20 - Prompt engineering for art models\n43:30 - GLIDE\n47:00 - Video production & Disco Diffusion\n48:40 - Economics, money, and NFTs\n54:15 - What does the future hold for AI art?\n\nBlog post: https://jxmo.notion.site/The-Weird-and-Wonderful-World-of-AI-Art-b9615a2e7278435b98380ff81ae1cf09\nJack's Blog: https://jxmo.io/\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "31": "#neuralsearch #interview #google\n\nThis is an interview with the authors Yi Tay and Don Metzler.\nPaper Review Video: https://youtu.be/qlB0TPBQ7YY\n\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model's weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\n\nOUTLINE:\n0:00 - Intro\n0:50 - Start of Interview\n1:30 - How did this idea start?\n4:30 - How does memorization play into this?\n5:50 - Why did you not compare to cross-encoders?\n7:50 - Instead of the ID, could one reproduce the document itself?\n10:50 - Passages vs documents\n12:00 - Where can this model be applied?\n14:25 - Can we make this work on large collections?\n19:20 - What's up with the NQ100K dataset?\n23:55 - What is going on inside these models?\n28:30 - What's the smallest scale to obtain meaningful results?\n30:15 - Investigating the document identifiers\n34:45 - What's the end goal?\n38:40 - What are the hardest problems currently?\n40:40 - Final comments & how to get started\n\nPaper: https://arxiv.org/abs/2202.06991\n\nAbstract:\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "32": "#nlp #sparsity #transformers\n\nThis video is an interview with Barret Zoph and William Fedus of Google Brain about Sparse Expert Models.\nSparse Expert models have been hugely successful at distributing parts of models, mostly Transformers, across large array of machines and use a routing function to effectively route signals between them. This means that even though these models have a huge number of parameters, the computational load for a given signal does not increase because the model is only sparsely activated. Sparse expert models, such as Switch Transformers and GLAM can scale up to trillions of parameters and bring a number of desirable properties. We discuss everything from the fundamentals, history, strengths and weaknesses, up to the current state of the art of these models.\n\nOUTLINE:\n0:00 - Intro\n0:30 - What are sparse expert models?\n4:25 - Start of Interview\n5:55 - What do you mean by sparse experts?\n8:10 - How does routing work in these models?\n12:10 - What is the history of sparse experts?\n14:45 - What does an individual expert learn?\n19:25 - When are these models appropriate?\n22:30 - How comparable are sparse to dense models?\n26:30 - How does the pathways system connect to this?\n28:45 - What improvements did GLAM make?\n31:30 - The \"designing sparse experts\" paper\n37:45 - Can experts be frozen during training?\n41:20 - Can the routing function be improved?\n47:15 - Can experts be distributed beyond data centers?\n50:20 - Are there sparse experts for other domains than NLP?\n52:15 - Are sparse and dense models in competition?\n53:35 - Where do we go from here?\n56:30 - How can people get started with this?\n\nPapers:\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (https://arxiv.org/abs/2101.03961)\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts (https://arxiv.org/abs/2112.06905)\nDesigning Effective Sparse Expert Models (https://arxiv.org/abs/2202.08906)\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "33": "#laion #clip #dalle\n\nLAION-5B is an open, free dataset consisting of over 5 billion image-text-pairs. Today's video is an interview with three of its creators. We dive into the mechanics and challenges of operating at such large scale, how to keep cost low, what new possibilities are enabled with open datasets like this, and how to best handle safety and legal concerns.\n\nOUTLINE:\n0:00 - Intro\n1:30 - Start of Interview\n2:30 - What is LAION?\n11:10 - What are the effects of CLIP filtering?\n16:40 - How big is this dataset?\n19:05 - Does the text always come from the alt-property?\n22:45 - What does it take to work at scale?\n25:50 -When will we replicate DALL-E?\n31:30 - The surprisingly efficient pipeline\n35:20 - How do you cover the S3 costs?\n40:30 - Addressing safety & legal concerns\n55:15 - Where can people get started?\n\nReferences:\nLAION website: https://laion.ai/\nLAION Discord: https://discord.com/invite/mVcgxMPD7e\nLAION-5B: https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/\nimg2dataset tool: https://github.com/rom1504/img2dataset\nLAION-400M: https://paperswithcode.com/dataset/laion-400m\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "34": "#ai #accel #evolution\n\nThis is an interview with the authors Jack Parker-Holder and Minqi Jiang.\nOriginal Paper Review Video: https://www.youtube.com/watch?v=povBDxUn1VQ\n\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\n\nOUTLINE:\n0:00 - Intro\n1:00 - Start of interview\n4:45 - How did you get into this field?\n8:10 - What is minimax regret?\n11:45 - What levels does the regret objective select?\n14:20 - Positive value loss (correcting my mistakes)\n21:05 - Why is the teacher not learned?\n24:45 - How much domain-specific knowledge is needed?\n29:30 - What problems is this applicable to?\n33:15 - Single agent vs population of agents\n37:25 - Measuring and balancing level difficulty\n40:35 - How does generalization emerge?\n42:50 - Diving deeper into the experimental results\n47:00 - What are the unsolved challenges in the field?\n50:00 - Where do we go from here?\n\nWebsite: https://accelagent.github.io\nPaper: https://arxiv.org/abs/2203.01302\nICLR Workshop: https://sites.google.com/view/aloe2022\nBook on topic: https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/\n\nAbstract:\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\n\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rockt\u00e4schel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "35": "#saycan #robots #ai\n\nThis is an interview with the authors Brian Ichter, Karol Hausman, and Fei Xia.\nOriginal Paper Review Video: https://youtu.be/Ru23eWAQ6_E\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy's ability to progress towards the goal, given by the language model, and the policy's probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nOUTLINE:\n0:00 - Introduction & Setup\n3:40 - Acquiring atomic low-level skills\n7:45 - How does the language model come in?\n11:45 - Why are you scoring instead of generating?\n15:20 - How do you deal with ambiguity in language?\n20:00 - The whole system is modular\n22:15 - Going over the full algorithm\n23:20 - What if an action fails?\n24:30 - Debunking a marketing video :)\n27:25 - Experimental Results\n32:50 - The insane scale of data collection\n40:15 - How do you go about large-scale projects?\n43:20 - Where did things go wrong?\n45:15 - Where do we go from here?\n52:00 - What is the largest unsolved problem in this?\n53:35 - Thoughts on the Tesla Bot\n55:00 - Final thoughts\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "36": "#ai #selforganization #emergence\n\nRead Sebastian's article here: https://sebastianrisi.com/self_assembling_ai/\n\nOUTLINE:\n0:00 - Introduction\n2:25 - Start of Interview\n4:00 - The intelligence of swarms\n9:15 - The game of life & neural cellular automata\n14:10 - What's missing from neural CAs?\n17:20 - How does local computation compare to centralized computation?\n25:40 - Applications beyond games and graphics\n33:00 - Can we do away with goals?\n35:30 - Where do these methods shine?\n43:30 - The paradox of scales & brains\n49:45 - Connections to graphical systems & GNNs\n51:30 - Could this solve ARC?\n57:45 - Where can people get started?\n\nReferences:\nhttps://sebastianrisi.com/\nhttps://modl.ai/\nhttps://sebastianrisi.com/self_assembling_ai/\nhttps://twitter.com/risi1979/status/1519053654921293827?cxt=HHwWhsC9hYfQ4ZQqAAAA\nhttps://distill.pub/2020/growing-ca/\nhttps://arxiv.org/abs/2201.12360?source=techstories.org\nhttps://distill.pub/2020/selforg/mnist/\nhttps://arxiv.org/pdf/2204.11674.pdf\nhttps://github.com/fchollet/ARC\nhttps://github.com/volotat/ARC-Game\nhttp://animalaiolympics.com/AAI/\nhttps://www.deepmind.com/publications/alchemy-a-structured-task-distribution-for-meta-reinforcement-learning-f\nhttps://melaniemitchell.me/BooksContent/CAGTReviews.html \n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "37": "#ai #interview #research \n\nJacob Steinhardt believes that future AI systems will be qualitatively different than the ones we know currently. We talk about how emergence happens when scaling up, what implications that has on AI Safety, and why thought experiments like the Paperclip Maximizer might be more useful than most people think.\n\nOUTLINE:\n0:00 Introduction\n1:10 Start of Interview\n2:10 Blog posts series\n3:56 More Is Different for AI (Blog Post)\n7:40 Do you think this emergence is mainly a property from the interaction of things?\n9:17 How does phase transition or scaling-up play into AI and Machine Learning?\n12:10 GPT-3 as an example of qualitative difference in scaling up\n14:08 GPT-3 as an emergent phenomenon in context learning\n15:58 Brief introduction of different viewpoints on the future of AI and its alignment\n18:51 How does the phenomenon of emergence play into this game between the Engineering and the Philosophy viewpoint? \n22:41 Paperclip Maximizer on AI safety and alignment\n31:37 Thought Experiments\n37:34 Imitative Deception\n39:30 TruthfulQA: Measuring How Models Mimic Human Falsehoods (Paper)\n42:24 ML Systems Will Have Weird Failure Models (Blog Post)\n51:10 Is there any work to get a system to be deceptive?\n54:37 Empirical Findings Generalize Surprisingly Far (Blog Post)\n1:00:18 What would you recommend to guarantee better AI alignment or safety?\n1:05:13 Remarks\n\nReferences:\nhttps://bounded-regret.ghost.io/more-is-different-for-ai/\nhttps://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit#heading=h.n1wk9bxo847o\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "38": "#glide #openai #diffusion\n\nDiffusion models learn to iteratively reverse a noising process that is applied repeatedly during training. The result can be used for conditional generation as well as various other tasks such as inpainting. OpenAI's GLIDE builds on recent advances in diffusion models and combines text-conditional diffusion with classifier-free guidance and upsampling to achieve unprecedented quality in text-to-image samples.\n\nTry it yourself: https://huggingface.co/spaces/valhalla/glide-text2im\n\nOUTLINE:\n0:00 - Intro & Overview\n6:10 - What is a Diffusion Model?\n18:20 - Conditional Generation and Guided Diffusion\n31:30 - Architecture Recap\n34:05 - Training & Result metrics\n36:55 - Failure cases & my own results\n39:45 - Safety considerations\n\nPaper: https://arxiv.org/abs/2112.10741\nCode & Model: https://github.com/openai/glide-text2im\n\nMore diffusion papers:\nhttps://arxiv.org/pdf/2006.11239.pdf\nhttps://arxiv.org/pdf/2102.09672.pdf\n\nAbstract:\nDiffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at this https URL.\n\nAuthors: Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "39": "#parti #ai #aiart \n\nParti is a new autoregressive text-to-image model that shows just how much scale can achieve. This model's outputs are crips, accurate, realistic, and can combine arbitrary styles, concepts, and fulfil even challenging requests.\n\nOUTLINE:\n0:00 - Introduction\n2:40 - Example Outputs\n6:00 - Model Architecture\n17:15 - Datasets (incl. PartiPrompts)\n21:45 - Experimental Results\n27:00 - Picking a cherry tree\n29:30 - Failure cases\n33:20 - Final comments\n\nWebsite: https://parti.research.google/\nPaper: https://arxiv.org/abs/2206.10789\nGithub: https://github.com/google-research/parti\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "40": "#mlnews #dalle #imagen \n\nAll things text-to-image models like DALL-E and Imagen!\n\nOUTLINE:\n0:00 - Intro\n0:30 - Imagen: Google's Text-to-Image Diffusion Model\n7:15 - Unified I/O by AllenAI\n9:40 - CogView2 is Open-Source\n11:05 - Google bans DeepFakes from Colab\n13:05 - DALL-E generates real Cosmopolitan cover\n15:45 - DALL-E tips & tricks\n17:00 - Midjourney moves to Open Beta\n17:50 - DALLE-mini is not Crayon\n19:00 - Deep Learning Resources\n\nAMENDMENTS:\nThe Unified-IO paper is here: https://arxiv.org/abs/2206.08916\n\nReferences:\nImagen: Google's Text-to-Image Diffusion Model\nhttps://imagen.research.google/?utm_source=pocket_mylist\nhttps://arxiv.org/pdf/2205.11487.pdf\n\nUnified I/O by AllenAI\nhttps://unified-io.allenai.org/\nhttps://blog.allenai.org/introducing-ai2s-unified-io-9c0ec7fe1e43\n\nCogView2 is Open-Source\nhttps://github.com/THUDM/CogView2\nfile:///Users/yk/Downloads/big.1.pdf\nhttps://huggingface.co/spaces/THUDM/CogView2\nhttps://arxiv.org/pdf/2204.14217.pdf\n\nGoogle bans DeepFakes from Colab\nhttps://www-vice-com.cdn.ampproject.org/c/s/www.vice.com/amp/en/article/v7v4gx/google-bans-deepfakes-from-its-machine-learning-platform?utm_source=pocket_mylist\n\nDALL-E generates real Cosmopolitan cover\nhttps://www.cosmopolitan.com/lifestyle/a40314356/dall-e-2-artificial-intelligence-cover/\nhttps://www.instagram.com/p/CfEwohiJdXW/?hl=en\n\nDALL-E tips & tricks\nhttps://twitter.com/GuyP/status/1544710725708513280?s=09&t=c3NpErPx80INQVeaWkIqIg&utm_source=pocket_mylist\nhttps://twitter.com/GuyP/status/1552681939806691329?s=09&t=LV2ChcukUziXfvfNK-sY0A&utm_source=pocket_mylist\nhttps://twitter.com/GuyP/status/1547234780001042432\nhttps://dallery.gallery/the-dalle-2-prompt-book/\n\nMidjourney moves to Open Beta\nhttps://twitter.com/midjourney?lang=en\nhttps://twitter.com/search?q=%23midjourney&f=image\n\nDALLE-mini is not Crayon\nhttps://www.craiyon.com/\n\nDeep Learning Resources\nhttps://github.com/jacobhilton/deep_learning_curriculum\nhttps://arxiv.org/abs/2206.13446\nhttps://arxiv.org/pdf/2206.13446.pdf\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "41": "#stablediffusion #ai #stabilityai\n\nAn interview with Emad Mostaque, founder of Stability AI.\n\nOUTLINE:\n0:00 - Intro\n1:30 - What is Stability AI?\n3:45 - Where does the money come from?\n5:20 - Is this the CERN of AI?\n6:15 - Who gets access to the resources?\n8:00 - What is Stable Diffusion?\n11:40 - What if your model produces bad outputs?\n14:20 - Do you employ people?\n16:35 - Can you prevent the corruption of profit?\n19:50 - How can people find you?\n22:45 - Final thoughts, let's destroy PowerPoint\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "42": "#stablediffusion #ai #watermark \n\nWatermarking the outputs of generative models is usually done as a post-processing step on the model outputs. Tree-Ring Watermarks are applied in the latent space at the beginning of a diffusion process, which makes them nearly undetectable, robust to strong distortions, and only recoverable by the model author. It is a very promising technique with applications potentially beyond watermarking itself.\n\nOUTLINE:\n0:00 - Introduction & Overview\n1:30 - Why Watermarking?\n4:20 - Diffusion Models Recap\n13:40 - Inverting Diffusion Models\n17:05 - Tree-Ring Watermarking\n26:15 - Effects of Tree-Ring Watermarks\n30:00 - Experimental Results\n32:40 - Limitations\n34:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.20030\n\nAbstract:\nWatermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at this https URL.\n\nAuthors: Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "43": "Authors: David Ha, J\u00fcrgen Schmidhuber\n\nAbstract:\nWe explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.\n\nhttps://arxiv.org/abs/1803.10122", "44": "https://arxiv.org/abs/1705.05363\n\nAuthors: Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell\n\nAbstract:\nIn many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.", "45": "https://arxiv.org/abs/1611.05397\n\nAbstract:\nDeep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10\u00d7 and averaging 87\\% expert human performance on Labyrinth.\n\nAuthors:\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu", "46": "https://arxiv.org/abs/1707.06170\n\nAbstract:\nConventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the \"Imagination-based Planner\", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a \"plan context\" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex \"imagination tree\" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.\n\nAuthors:\nRazvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racani\u00e8re, David Reichert, Th\u00e9ophane Weber, Daan Wierstra, Peter Battaglia", "47": "Commentary of\nhttps://arxiv.org/abs/1707.06203\n\nAbstract\nWe introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.\n\nAuthors\nTh\u00e9ophane Weber, S\u00e9bastien Racani\u00e8re, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, David Silver, Daan Wierstra", "48": "Abstract:\nDeep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient \u2013 that is, it may simply be too slow \u2013 to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.\n\nAuthors: Matthew Botvinick, Sam Ritter, Jane X. Wang, Zeb Kurth-Nelson, Charles Blundell, Demis Hassabis\n\nhttps://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0", "49": "The goal of hierarchical reinforcement learning is to divide a task into different levels of coarseness with the top-level agent planning only over a high-level view of the world and each subsequent layer having a more detailed view. This paper proposes to learn a set of important states as well as their connections to each other as a high-level abstraction.\n\nhttps://arxiv.org/abs/1907.00664\n\nAbstract:\nIn many real-world scenarios, an autonomous agent often encounters various tasks within a single complex environment. We propose to build a graph abstraction over the environment structure to accelerate the learning of these tasks. Here, nodes are important points of interest (pivotal states) and edges represent feasible traversals between them. Our approach has two stages. First, we jointly train a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic manner. Second, provided with the information from the world graph, a high-level Manager quickly finds solution to new tasks and expresses subgoals in reference to pivotal states to a low-level Worker. The Worker can then also leverage the graph to easily traverse to the pivotal states of interest, even across long distance, and explore non-locally. We perform a thorough ablation study to evaluate our approach on a suite of challenging maze tasks, demonstrating significant advantages from the proposed framework over baselines that lack world graph knowledge in terms of performance and efficiency.\n\nAuthors: Wenling Shang, Alex Trott, Stephan Zheng, Caiming Xiong, Richard Socher", "50": "The AI cook is here! This agent learns to play a text-based game where the goal is to prepare a meal according to a recipe. Challenges? Many! The number of possible actions is huge, ingredients change and can include ones never seen before, you need to navigate rooms, use tools, manage an inventory and sequence everything correctly and all of this from a noisy textual description that the game engine throws at you. This paper mixes supervised explicit training with reinforcement learning in order to solve this task.\n\nAbstract:\nWhile Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's \"First TextWorld Problems: A Language and Reinforcement Learning Challenge\" and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.\n\nAuthors: Leonard Adolphs, Thomas Hofmann\n\nhttps://arxiv.org/abs/1909.01646", "51": "Policy Gradient RL on a massively distributed scale with theoretical guarantees!\n\nAbstract:\nIn this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\n\nAuthors: Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\n\nhttps://arxiv.org/abs/1802.01561\nhttps://github.com/deepmind/scalable_agent", "52": "DeepMind's new agent to tackle yet another Esport: Starcraft II. This agent uses deep reinforcement learning with a new technique, called League Training, to catapult itself to Grandmaster-level skill at playing this game.\n\nAbstract:\nMany real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.\n\nAuthors: Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver\n\nhttps://www.deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "53": "Successor representations are a mid-point between model-based and model-free reinforcement learning. This paper learns successor representation in environments where only incomplete information is available.\n\nAbstract:\nAnimals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent's location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional successor features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible.\n\nAuthors: Eszter Vertes, Maneesh Sahani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "54": "MuZero harnesses the power of AlphaZero, but without relying on an accurate environment model. This opens up planning-based reinforcement learning to entirely new domains, where such environment models aren't available. The difference to previous work is that, instead of learning a model predicting future observations, MuZero predicts the future observations' latent representations, and thus learns to only represent things that matter to the task!\n\nAbstract:\nConstructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.\n\nAuthors: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver\n\nhttps://arxiv.org/abs/1911.08265\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "55": "Schmidhuber thinking outside the box! Upside-Down RL turns RL on its head and constructs a behavior function that uses the desired reward as an input. The new paradigm shows surprising performance compared to classic RL algorithms.\n\nAbstract:\nWe transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also introduce a related simple but general approach for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.\n\nAuthor: Juergen Schmidhuber\n\nhttps://arxiv.org/abs/1912.02875\nhttps://arxiv.org/abs/1912.02877", "56": "This algorithm solves the hardest games in the Atari suite and makes it look so easy! This modern version of Dijkstra's shortest path algorithm is outperforming everything else by orders of magnitude, and all based on random exploration.\n\nhttps://arxiv.org/abs/1901.10995\nhttps://eng.uber.com/go-explore/\nhttps://github.com/uber-research/go-explore\n\nAbstract:\nA grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of \"superhuman\" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).\n\nAuthors: Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune", "57": "DeepMind's Agent57 is the first RL agent to outperform humans in all 57 Atari benchmark games. It extends previous algorithms like Never Give Up and R2D2 by meta-learning the exploration-exploitation tradeoff controls.\n\nhttps://arxiv.org/abs/2003.13350\nhttps://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark\n\nAbstract:\nAtari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.\n\nAuthors: Adri\u00e0 Puigdom\u00e8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "58": "Dreamer is a new RL agent by DeepMind that learns a continuous control task through forward-imagination in latent space.\n\nhttps://arxiv.org/abs/1912.01603\nVideos: https://dreamrl.github.io/\n\nAbstract:\nLearned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.\n\nAuthors: Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "59": "From the makers of Go-Explore, POET is a mixture of ideas from novelty search, evolutionary methods, open-ended learning and curriculum learning.\n\nhttps://arxiv.org/abs/1901.01753\n\nAbstract:\nWhile the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. Our results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.\n\nAuthors: Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "60": "The enhanced POET makes some substantial and well-crafted improvements over the original POET algorithm and excels at open-ended learning like no system before.\n\nhttps://arxiv.org/abs/2003.08536\nhttps://youtu.be/RX0sKDRq400\n\nAbstract:\nCreating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means.\n\nAuthors: Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, Kenneth O. Stanley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "61": "Contrastive Learning has been an established method in NLP and Image classification. The authors show that with relatively minor adjustments, CL can be used to augment and improve RL dramatically.\n\nPaper: https://arxiv.org/abs/2004.04136\nCode: https://github.com/MishaLaskin/curl\n\nAbstract:\nWe present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 2.8x and 1.6x performance gains respectively at the 100K interaction steps benchmark. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency and performance of methods that use state-based features.\n\nAuthors: Aravind Srinivas, Michael Laskin, Pieter Abbeel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "62": "Offline Reinforcement Learning has come more and more into focus recently in domains where classic on-policy RL algorithms are infeasible to train, such as safety-critical tasks or learning from expert demonstrations. This paper presents an extensive benchmark for evaluating offline RL algorithms in a variety of settings.\n\nPaper: https://arxiv.org/abs/2004.07219\nCode: https://github.com/rail-berkeley/offline_rl\n\nAbstract:\nThe offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without additional online data collection. In supervised learning, large datasets and complex deep neural networks have fueled impressive progress, but in contrast, conventional RL algorithms must collect large amounts of on-policy data and have had little success leveraging previously collected datasets. As a result, existing RL benchmarks are not well-suited for the offline setting, making progress in this area difficult to measure. To design a benchmark tailored to offline RL, we start by outlining key properties of datasets relevant to applications of offline RL. Based on these properties, we design a set of benchmark tasks and datasets that evaluate offline RL algorithms under these conditions. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets, where an agent can perform different tasks in the same environment, and datasets consisting of a heterogeneous mix of high-quality and low-quality trajectories. By designing the benchmark tasks and datasets to reflect properties of real-world offline RL problems, our benchmark will focus research effort on methods that drive substantial improvements not just on simulated benchmarks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.\n\nAuthors: Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "63": "Classic RL \"stops\" the world whenever the Agent computes a new action. This paper considers a more realistic scenario where the agent is thinking about the next action to take while still performing the last action. This results in a fascinating way of reformulating Q-learning in continuous time, then introducing concurrency and finally going back to discrete time.\n\nhttps://arxiv.org/abs/2004.06089\n\nAbstract:\nWe study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must \"think while moving\".\n\nAuthors: Ted Xiao, Eric Jang, Dmitry Kalashnikov, Sergey Levine, Julian Ibarz, Karol Hausman, Alexander Herzog\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "64": "Hail the AI Tax Collector! This very visual framework has RL Agents maximize their coins in a tiny world through collecting, building and trading. But at the same time, the government is also an AI trying to maximize social welfare via taxes. What emerges is very interesting.\n\nPaper: https://arxiv.org/abs/2004.13332\nBlog: https://blog.einstein.ai/the-ai-economist/\n\nAbstract:\nTackling real-world socio-economic challenges requires designing and testing economic policies. However, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. In this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. We propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. Our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. We make four main contributions. First, we present an economic simulation environment that features competitive pressures and market dynamics. We validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. Second, we show that AI-driven tax policies improve the trade-off between equality and productivity by 16% over baseline policies, including the prominent Saez tax framework. Third, we showcase several emergent features: AI-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. Moreover, AI-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are also effective when used in experiments with human participants. In experiments conducted on MTurk, an AI tax policy provides an equality-productivity trade-off that is similar to that provided by the Saez framework along with higher inverse-income weighted social welfare.\n\nAuthors: Stephan Zheng,\u00a0Alexander Trott,\u00a0Sunil Srinivasa,\u00a0Nikhil Naik,\u00a0Melvin Gruesbeck,\u00a0David C. Parkes,\u00a0Richard Socher\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "65": "The AI Singularity is here! Computers designing new computers! It takes human experts multiple weeks to design new computer chips. What looks like a large game of Tetris is actually a very complex optimization problem. This paper uses Deep Reinforcement Learning to solve this optimization both faster and better than humans.\n\nhttps://arxiv.org/abs/2004.10746\n\nAbstract:\nIn this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.\n\nAuthors: Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa, William Hang, Emre Tuncer, Anand Babu, Quoc V. Le, James Laudon, Richard Ho, Roger Carpenter, Jeff Dean\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "66": "This ONE SIMPLE TRICK can take a vanilla RL algorithm to achieve state-of-the-art. What is it? Simply augment your training data before feeding it to the learner! This can be dropped into any RL pipeline and promises big improvements across the board.\n\nPaper: https://arxiv.org/abs/2004.14990\nCode: https://www.github.com/MishaLaskin/rad\n\nAbstract:\nLearning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at this https URL.\n\nAuthors: Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "67": "When AI makes a plan it usually does so step by step, forward in time. But often it is beneficial to define intermediate goals to divide a large problem into easier sub-problems. This paper proposes a generalization of MCTS that searches not for the best next actions to take, but for the best way to sub-divide the problem recursively into problems so tiny that they can each be solved in a single step.\n\nPaper: https://arxiv.org/abs/2004.11410\nSite: https://sites.google.com/view/dc-mcts/home\n\nAbstract:\nStandard planners for sequential decision making (including Monte Carlo planning, tree search, dynamic programming, etc.) are constrained by an implicit sequential planning assumption: The order in which a plan is constructed is the same in which it is executed. We consider alternatives to this assumption for the class of goal-directed Reinforcement Learning (RL) problems. Instead of an environment transition model, we assume an imperfect, goal-directed policy. This low-level policy can be improved by a plan, consisting of an appropriate sequence of sub-goals that guide it from the start to the goal state. We propose a planning algorithm, Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS), for approximating the optimal plan by means of proposing intermediate sub-goals which hierarchically partition the initial tasks into simpler ones that are then solved independently and recursively. The algorithm critically makes use of a learned sub-goal proposal for finding appropriate partitions trees of new tasks based on prior experience. Different strategies for learning sub-goal proposals give rise to different planning strategies that strictly generalize sequential planning. We show that this algorithmic flexibility over planning order leads to improved results in navigation tasks in grid-worlds as well as in challenging continuous control environments.\n\nAuthors: Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B. Hamrick, Nicolas Heess, Alexander Neitz, Theophane Weber\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "68": "What can an agent do without any reward? Explore the world! While many formulations of intrinsic rewards exist (Curiosity, Novelty, etc.), they all look back in time to learn. Plan2Explore is the first model that uses planning in a learned imaginary latent world model to seek out states where it is uncertain about what will happen.\n\nOUTLINE:\n0:00 - Intro & Problem Statement\n3:30 - Model\n5:10 - Intrinsic Motivation\n9:05 - Planning in Latent Space\n11:15 - Latent Disagreement\n16:30 - Maximizing Information Gain\n21:00 - More problems with the model\n26:45 - Experiments\n32:10 - Final Comments\n\nPaper: https://arxiv.org/abs/2005.05960\nWebsite: https://ramanans1.github.io/plan2explore/\nCode: https://github.com/ramanans1/plan2explore\n\nAbstract:\nReinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at this https URL\n\nAuthors: Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "69": "Why are humans so good at video games? Maybe it's because a lot of games are designed with humans in mind. What happens if we change that? This paper removes the influence of human priors from a game and ends up with a pretty fun experience.\n\nPaper: https://arxiv.org/abs/1802.10217\nWebsite: https://rach0012.github.io/humanRL_website/\nCode: https://github.com/rach0012/humanRL_prior_games\n\nAbstract:\nWhat makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at this https URL\n\nAuthors: Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, Alexei A. Efros\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "70": "Can you plan with a learned model of the world? Yes, but there's a catch: The better your planning algorithm is, the more the errors of your world model will hurt you! This paper solves this problem by regularizing the planning algorithm to stay in high probability regions, given its experience.\n\nhttps://arxiv.org/abs/1903.11981\n\nInterview w/ Harri: https://youtu.be/HnZDmxYnpg4\n\nAbstract:\nTrajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.\n\nAuthors: Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "71": "This RL framework can discover low-level skills all by itself without any reward. Even better, at test time it can compose its learned skills and reach a specified goal without any additional learning! Warning: Math-heavy!\n\nOUTLINE:\n0:00 - Motivation\n2:15 - High-Level Overview\n3:20 - Model-Based vs Model-Free Reinforcement Learning\n9:00 - Skills\n12:10 - Mutual Information Objective\n18:40 - Decomposition of the Objective\n27:10 - Unsupervised Skill Discovery Algorithm\n42:20 - Planning in Skill Space\n48:10 - Conclusion\n\nPaper: https://arxiv.org/abs/1907.01657\nWebsite: https://sites.google.com/view/dads-skill\nCode: https://github.com/google-research/dads\n\nAbstract:\nConventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.\n\nAuthors: Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "72": "#ai #research #gaming\n\nDeep RL is usually used to solve games, but this paper turns the process on its head and applies RL to game level creation. Compared to traditional approaches, it frames level design as a sequential decision making progress and ends up with a fast and diverse level generator.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:30 - Level Design via Reinforcement Learning\n3:00 - Reinforcement Learning\n4:45 - Observation Space\n5:40 - Action Space\n15:40 - Change Percentage Limit\n20:50 - Quantitative Results\n22:10 - Conclusion & Outlook\n\nPaper: https://arxiv.org/abs/2001.09212\nCode: https://github.com/amidos2006/gym-pcgrl\n\nAbstract:\nWe investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.\n\nAuthors: Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius\n\nERRATA:\n- The reward is given after each step.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "73": "#ai #dqn #deepmind\n\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Arcade Learning Environment\n4:25 - Deep Reinforcement Learning\n9:20 - Deep Q-Learning\n26:30 - Experience Replay\n32:25 - Network Architecture\n33:50 - Experiments\n37:45 - Conclusion\n\nPaper: https://arxiv.org/abs/1312.5602\n\nAbstract:\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "74": "#ai #research #machinelearning\n\nOnline Reinforcement Learning is a flourishing field with countless methods for practitioners to choose from. However, each of those methods comes with a plethora of hyperparameter choices. This paper builds a unified framework for five continuous control tasks and investigates in a large-scale study the effects of these choices. As a result, they come up with a set of recommendations for future research and applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:55 - Parameterized Agents\n7:00 - Unified Online RL and Parameter Choices\n14:10 - Policy Loss\n16:40 - Network Architecture\n20:25 - Initial Policy\n24:20 - Normalization & Clipping\n26:30 - Advantage Estimation\n28:55 - Training Setup\n33:05 - Timestep Handling\n34:10 - Optimizers\n35:05 - Regularization\n36:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05990\n\nAbstract:\nIn recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress (Engstrom'20). As a step towards filling that gap, we implement over 50 such \"choices\" in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.\n\nAuthors: Marcin Andrychowicz, Anton Raichuk, Piotr Sta\u0144czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "75": "#ai #neuroscience #rl\n\nReinforcement Learning is a powerful tool, but it lacks biological plausibility because it learns a fixed policy network. Animals use neuroplasticity to reconfigure their policies on the fly and quickly adapt to new situations. This paper uses Hebbian Learning, a biologically inspired technique, to have agents adapt random networks to high-performing solutions as an episode is progressing, leading to agents that can reconfigure themselves in response to new observations.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Reinforcement Learning vs Hebbian Plasticity\n9:00 - Episodes in Hebbian Learning\n10:00 - Hebbian Plasticity Rules\n18:10 - Quadruped Experiment Results\n21:20 - Evolutionary Learning of Hebbian Plasticity\n29:10 - More Experimental Results\n34:50 - Conclusions\n35:30 - Broader Impact Statement\n\nVideos: https://twitter.com/risi1979/status/1280544779630186499\nPaper: https://arxiv.org/abs/2007.02686\n\nAbstract:\nLifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to different morphological damage in the absence of any explicit reward or error signal.\n\nAuthors: Elias Najarro, Sebastian Risi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "76": "#ai #research #reinforcementlearning\n\nReinforcement Learning is a powerful tool, but it is also incredibly data-hungry. Given a new task, an RL agent has to learn a good policy entirely from scratch. This paper proposes a new framework that allows an agent to carry over knowledge from previous tasks into solving new tasks, even deriving zero-shot policies that perform well on completely new reward functions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n6:25 - Q-Learning Primer\n11:40 - Multiple Rewards, Multiple Policies\n14:25 - Example Environment\n17:35 - Tasks as Linear Mixtures of Features\n24:15 - Successor Features\n28:00 - Zero-Shot Policy for New Tasks\n35:30 - Results on New Task W3\n37:00 - Inferring the Task via Regression\n39:20 - The Influence of the Given Policies\n48:40 - Learning the Feature Functions\n50:30 - More Complicated Tasks\n51:40 - Life-Long Learning, Comments & Conclusion\n\nPaper: https://www.pnas.org/content/early/2020/08/13/1907370117\n\nMy Video on Successor Features: https://youtu.be/KXEEqcwXn8w\n\nAbstract:\nThe combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.\n\nAuthors:\nAndr\u00e9 Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "77": "#summarization #gpt3 #openai\n\nText Summarization is a hard task, both in training and evaluation. Training is usually done maximizing the log-likelihood of a human-generated reference summary, while evaluation is performed using overlap-based metrics like ROUGE. Both significantly undervalue the breadth and intricacies of language and the nature of the information contained in text summaries. This paper by OpenAI includes direct human feedback both in evaluation and - via reward model proxies - in training. The final model even outperforms single humans when judged by other humans and is an interesting application of using reinforcement learning together with humans in the loop.\n\nOUTLINE:\n0:00 - Intro & Overview\n5:35 - Summarization as a Task\n7:30 - Problems with the ROUGE Metric\n10:10 - Training Supervised Models\n12:30 - Main Results\n16:40 - Including Human Feedback with Reward Models & RL\n26:05 - The Unknown Effect of Better Data\n28:30 - KL Constraint & Connection to Adversarial Examples\n37:15 - More Results\n39:30 - Understanding the Reward Model\n41:50 - Limitations & Broader Impact\n\nPaper: https://arxiv.org/abs/2009.01325\nBlog: https://openai.com/blog/learning-to-summarize-with-human-feedback/\nCode: https://github.com/openai/summarize-from-feedback\nSamples: https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/\n\nMy Video on GPT-3: https://youtu.be/SY5PvZrJhLE\nMy Video on GPT-2: https://youtu.be/u1_qMdb0kYU\n\nAbstract:\nAs language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\n\nAuthors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "78": "", "79": "#ai #chess #alphazero\n\nChess is a very old game and both its rules and theory have evolved over thousands of years in the collective effort of millions of humans. Therefore, it is almost impossible to predict the effect of even minor changes to the game rules, because this collective process cannot be easily replicated. This paper proposes to use AlphaZero's ability to achieve superhuman performance in board games within one day of training to assess the effect of a series of small, but consequential rule changes. It analyzes the resulting strategies and sets the stage for broader applications of reinforcement learning to study rule-based systems.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Alternate Chess Rules\n4:20 - Using AlphaZero to assess rule change outcomes\n6:00 - How AlphaZero works\n16:40 - Alternate Chess Rules continued\n18:50 - Game outcome distributions\n31:45 - e4 and Nf3 in classic vs no-castling chess\n36:40 - Conclusions & comments\n\nPaper: https://arxiv.org/abs/2009.04374\n\nMy Video on AI Economist: https://youtu.be/F5aaXrIMWyU\n\nAbstract:\nIt is non-trivial to design engaging and balanced sets of game rules. Modern chess has evolved over centuries, but without a similar recourse to history, the consequences of rule changes to game dynamics are difficult to predict. AlphaZero provides an alternative in silico means of game balance assessment. It is a system that can learn near-optimal strategies for any rule set from scratch, without any human supervision, by continually learning from its own experience. In this study we use AlphaZero to creatively explore and design new chess variants. There is growing interest in chess variants like Fischer Random Chess, because of classical chess's voluminous opening theory, the high percentage of draws in professional play, and the non-negligible number of games that end while both players are still in their home preparation. We compare nine other variants that involve atomic changes to the rules of chess. The changes allow for novel strategic and tactical patterns to emerge, while keeping the games close to the original. By learning near-optimal strategies for each variant with AlphaZero, we determine what games between strong human players might look like if these variants were adopted. Qualitatively, several variants are very dynamic. An analytic comparison show that pieces are valued differently between variants, and that some variants are more decisive than classical chess. Our findings demonstrate the rich possibilities that lie beyond the rules of modern chess.\n\nAuthors: Nenad Toma\u0161ev, Ulrich Paquet, Demis Hassabis, Vladimir Kramnik\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "80": "#ai #technology #poker\n\nThis paper does for Poker what AlphaZero has done for Chess & Go. The combination of Self-Play Reinforcement Learning and Tree Search has had tremendous success in perfect-information games, but transferring such techniques to imperfect information games is a hard problem. Not only does ReBeL solve this problem, but it provably converges to a Nash Equilibrium and delivers a superhuman Heads Up No-Limit Hold'em bot with very little domain knowledge.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:20 - Rock, Paper, and Double Scissor\n10:00 - AlphaZero Tree Search\n18:30 - Notation Setup: Infostates & Nash Equilibria\n31:45 - One Card Poker: Introducing Belief Representations\n45:00 - Solving Games in Belief Representation\n55:20 - The ReBeL Algorithm\n1:04:00 - Theory & Experiment Results\n1:07:00 - Broader Impact\n1:10:20 - High-Level Summary\n\nPaper: https://arxiv.org/abs/2007.13544\nCode: https://github.com/facebookresearch/rebel\nBlog: https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/\n\nERRATA: As someone last video pointed out: This is not the best Poker algorithm, but the best one that uses very little expert knowledge.\n\nAbstract:\nThe combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.\n\nAuthors: Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "81": "#dreamer #deeprl #reinforcementlearning\n\nModel-Based Reinforcement Learning has been lagging behind Model-Free RL on Atari, especially among single-GPU algorithms. This collaboration between Google AI, DeepMind, and the University of Toronto (UofT) pushes world models to the next level. The main contribution is a learned latent state consisting of one discrete part and one stochastic part, whereby the stochastic part is a set of 32 categorical variables, each with 32 possible values. The world model can freely decide how it wants to use these variables to represent the input, but is tasked with the prediction of future observations and rewards. This procedure gives rise to an informative latent representation and in a second step, reinforcement learning (A2C Actor-Critic) can be done purely - and very efficiently - on the basis of the world-model's latent states. No observations needed! This paper combines this with straight-through estimators, KL balancing, and many other tricks to achieve state-of-the-art single-GPU performance in Atari.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:50 - Short Recap of Reinforcement Learning\n6:05 - Problems with Model-Free Reinforcement Learning\n10:40 - How World Models Help\n12:05 - World Model Learner Architecture\n16:50 - Deterministic & Stochastic Hidden States\n18:50 - Latent Categorical Variables\n22:00 - Categorical Variables and Multi-Modality\n23:20 - Sampling & Stochastic State Prediction\n30:55 - Actor-Critic Learning in Dream Space\n32:05 - The Incompleteness of Learned World Models\n34:15 - How General is this Algorithm?\n37:25 - World Model Loss Function\n39:20 - KL Balancing\n40:35 - Actor-Critic Loss Function\n41:45 - Straight-Through Estimators for Sampling Backpropagation\n46:25 - Experimental Results\n52:00 - Where Does It Fail?\n54:25 - Conclusion\n\nPaper: https://arxiv.org/abs/2010.02193\nCode: https://github.com/danijar/dreamerv2\nAuthor Blog: https://danijar.com/project/dreamerv2/\nGoogle AI Blog: https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html\n\nERRATA (from the authors): \n- KL balancing (prior vs posterior within the KL) is different from beta VAEs (reconstruction vs KL)\n- The vectors of categoricals can in theory represent 32^32 different images so their capacity is quite large\n\nAbstract:\nIntelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.\n\nAuthors: Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "82": "#metarim #deeprl #catastrophicforgetting\n\nReinforcement Learning is very tricky in environments where the objective shifts over time. This paper explores agents in multi-task environments that are usually subject to catastrophic forgetting. Building on the concept of Recurrent Independent Mechanisms (RIM), the authors propose to separate the learning procedures for the mechanism parameters (fast) and the attention parameters (slow) and achieve superior results and more stability, and even better zero-shot transfer performance.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:30 - Recombining pieces of knowledge\n11:30 - Controllers as recurrent neural networks\n14:20 - Recurrent Independent Mechanisms\n21:20 - Learning at different time scales\n28:40 - Experimental Results & My Criticism\n44:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2105.08710\nRIM Paper: https://arxiv.org/abs/1909.10893\n\nAbstract:\nDecomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.\n\nAuthors: Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Sch\u00f6lkopf, Yoshua Bengio\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "83": "#decisiontransformer #reinforcementlearning #transformer\n\nProper credit assignment over long timespans is a fundamental problem in reinforcement learning. Even methods designed to combat this problem, such as TD-learning, quickly reach their limits when rewards are sparse or noisy. This paper reframes offline reinforcement learning as a pure sequence modeling problem, with the actions being sampled conditioned on the given history and desired future rewards. This allows the authors to use recent advances in sequence modeling using Transformers and achieve competitive results in Offline RL benchmarks.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:15 - Offline Reinforcement Learning\n10:10 - Transformers in RL\n14:25 - Value Functions and Temporal Difference Learning\n20:25 - Sequence Modeling and Reward-to-go\n27:20 - Why this is ideal for offline RL\n31:30 - The context length problem\n34:35 - Toy example: Shortest path from random walks\n41:00 - Discount factors\n45:50 - Experimental Results\n49:25 - Do you need to know the best possible reward?\n52:15 - Key-to-door toy experiment\n56:00 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2106.01345\nWebsite: https://sites.google.com/berkeley.edu/decision-transformer\nCode: https://github.com/kzl/decision-transformer\n\nTrajectory Transformer: https://trajectory-transformer.github.io/\nUpside-Down RL: https://arxiv.org/abs/1912.02875\n\nAbstract:\nWe present a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\n\nAuthors: Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "84": "#reiforcementlearning #gan #imitationlearning\n\nLearning from demonstrations is a fascinating topic, but what if the demonstrations are not exactly the behaviors we want to learn? Can we adhere to a dataset of demonstrations and still achieve a specified goal? This paper uses GANs to combine goal-achieving reinforcement learning with imitation learning and learns to perform well at a given task while doing so in the style of a given presented dataset. The resulting behaviors include many realistic-looking transitions between the demonstrated movements.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n6:10 - Reward Signals\n8:15 - Motion Prior from GAN\n14:10 - Algorithm Overview\n20:15 - Reward Engineering & Experimental Results\n30:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.02180\nMain Video: https://www.youtube.com/watch?v=wySUxZN_KbM\nSupplementary Video: https://www.youtube.com/watch?v=O6fBSMxThR4\n\nAbstract:\nSynthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character's behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.\n\nAuthors: Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, Angjoo Kanazawa\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "85": "#efficientzero #muzero #atari\n\nReinforcement Learning methods are notoriously data-hungry. Notably, MuZero learns a latent world model just from scalar feedback of reward- and policy-predictions, and therefore relies on scale to perform well. However, most RL algorithms fail when presented with very little data. EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low-sample setting. This could be a staple algorithm for future RL research.\n\nOUTLINE:\n0:00 - Intro & Outline\n2:30 - MuZero Recap\n10:50 - EfficientZero improvements\n14:15 - Self-Supervised consistency loss\n17:50 - End-to-end prediction of the value prefix\n20:40 - Model-based off-policy correction\n25:45 - Experimental Results & Conclusion\n\nPaper: https://arxiv.org/abs/2111.00210\nCode: https://github.com/YeWR/EfficientZero\nNote: code not there yet as of release of this video\n\nAbstract:\nReinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 190.4% mean human performance and 116.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.\n\nAuthors: Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "86": "#playerofgames #deepmind #alphazero\n\nSpecial Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\nGames have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\n\nOUTLINE:\n0:00 - Introduction\n2:50 - What games can Player of Games be trained on?\n4:00 - Tree search algorithms (AlphaZero)\n8:00 - What is different in imperfect information games?\n15:40 - Counterfactual Value- and Policy-Networks\n18:50 - The Player of Games search procedure\n28:30 - How to train the network?\n34:40 - Experimental Results\n47:20 - Discussion & Outlook\n\nPaper: https://arxiv.org/abs/2112.03178\n\nAbstract:\nGames have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.\n\nAuthors: Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "87": "#wikipedia #reinforcementlearning #languagemodels\n\nTransformers have come to overtake many domain-targeted custom models in a wide variety of fields, such as Natural Language Processing, Computer Vision, Generative Modelling, and recently also Reinforcement Learning. This paper looks at the Decision Transformer and shows that, surprisingly, pre-training the model on a language-modelling task significantly boosts its performance on Offline Reinforcement Learning. The resulting model achieves higher scores, can get away with less parameters, and exhibits superior scaling properties. This raises many questions about the fundamental connection between the domains of language and RL.\n\nOUTLINE:\n0:00 - Intro\n1:35 - Paper Overview\n7:35 - Offline Reinforcement Learning as Sequence Modelling\n12:00 - Input Embedding Alignment & other additions\n16:50 - Main experimental results\n20:45 - Analysis of the attention patterns across models\n32:25 - More experimental results (scaling properties, ablations, etc.)\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "88": "#wikipedia #reinforcementlearning #languagemodels\n\nOriginal paper review here: https://youtu.be/XHGh19Hbx48\n\nMachel Reid and Yutaro Yamada join me to discuss their recent paper on langauge model pre-training for decision transformers in offline reinforcement learning.\n\nOUTLINE:\n0:00 - Intro\n1:00 - Brief paper, setup & idea recap\n7:30 - Main experimental results & high standard deviations\n10:00 - Why is there no clear winner?\n13:00 - Why are bigger models not a lot better?\n14:30 - What\u2019s behind the name ChibiT?\n15:30 - Why is iGPT underperforming?\n19:15 - How are tokens distributed in Reinforcement Learning?\n22:00 - What other domains could have good properties to transfer?\n24:20 - A deeper dive into the models' attention patterns\n33:30 - Codebase, model sizes, and compute requirements\n37:30 - Scaling behavior of pre-trained models\n40:05 - What did not work out in this project?\n42:00 - How can people get started and where to go next?\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "89": "#deepmind #rl #society\n\nThis is an in-depth paper review, followed by an interview with the papers' authors!\nSociety is ruled by norms, and most of these norms are very useful, such as washing your hands before cooking. However, there also exist plenty of social norms which are essentially arbitrary, such as what hairstyles are acceptable, or what words are rude. These are called \"silly rules\". This paper uses multi-agent reinforcement learning to investigate why such silly rules exist. Their results indicate a plausible mechanism, by which the existence of silly rules drastically speeds up the agents' acquisition of the skill of enforcing rules, which generalizes well, and therefore a society that has silly rules will be better at enforcing rules in general, leading to faster adaptation in the face of genuinely useful norms.\n\nOUTLINE:\n0:00 - Intro\n3:00 - Paper Overview\n5:20 - Why are some social norms arbitrary?\n11:50 - Reinforcement learning environment setup\n20:00 - What happens if we introduce a \"silly\" rule?\n25:00 - Experimental Results: how silly rules help society\n30:10 - Isolated probing experiments\n34:30 - Discussion of the results\n37:30 - Start of Interview\n39:30 - Where does the research idea come from?\n44:00 - What is the purpose behind this research?\n49:20 - Short recap of the mechanics of the environment\n53:00 - How much does such a closed system tell us about the real world?\n56:00 - What do the results tell us about silly rules?\n1:01:00 - What are these agents really learning?\n1:08:00 - How many silly rules are optimal?\n1:11:30 - Why do you have separate weights for each agent?\n1:13:45 - What features could be added next?\n1:16:00 - How sensitive is the system to hyperparameters?\n1:17:20 - How to avoid confirmation bias?\n1:23:15 - How does this play into progress towards AGI?\n1:29:30 - Can we make real-world recommendations based on this?\n1:32:50 - Where do we go from here?\n\nPaper: https://www.pnas.org/doi/10.1073/pnas.2106028118\nBlog: https://deepmind.com/research/publications/2021/Spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents\n\nAbstract:\nThe fact that humans enforce and comply with norms is an important reason why humans enjoy higher levels of cooperation and welfare than other animals. Some norms are relatively easy to explain; they may prohibit obviously harmful or uncooperative actions. But many norms are not easy to explain. For example, most cultures prohibit eating certain kinds of foods and almost all societies have rules about what constitutes appropriate clothing, language, and gestures. Using a computational model focused on learning shows that apparently pointless rules can have an indirect effect on welfare. They can help agents learn how to enforce and comply with norms in general, improving the group\u2019s ability to enforce norms that have a direct effect on welfare.\n\nAuthors: Raphael K\u00f6ster, Dylan Hadfield-Menell, Richard Everett, Laura Weidinger, Gillian K. Hadfield, Joel Z. Leibo\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "90": "#reinforcementlearning #ai #explained\n\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\n\nOUTLINE:\n0:00 - Intro\n1:10 - Paper Overview: Language for exploration\n5:40 - The MiniGrid & MiniHack environments\n7:00 - Annotating states with language\n9:05 - Baseline algorithm: AMIGo\n12:20 - Adding language to AMIGo\n22:55 - Baseline algorithm: NovelD and Random Network Distillation\n29:45 - Adding language to NovelD\n31:50 - Aren't we just using extra data?\n34:55 - Investigating the experimental results\n40:45 - Final comments\n\nPaper: https://arxiv.org/abs/2202.08938\n\nAbstract:\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\n\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rockt\u00e4schel, Edward Grefenstette\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "91": "#reinforcementlearning #ai #explained\n\nThis is an interview with Jesse Mu, first author of the paper.\nOriginal Paper Review: https://youtu.be/NeGJAUSQEJI\n\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\n\nOUTLINE:\n0:00 - Intro\n0:55 - Paper Overview\n4:30 - Aren't you just adding extra data?\n9:35 - Why are you splitting up the AMIGo teacher?\n13:10 - How do you train the grounding network?\n16:05 - What about causally structured environments?\n17:30 - Highlights of the experimental results\n20:40 - Why is there so much variance?\n22:55 - How much does it matter that we are testing in a video game?\n27:00 - How does novelty interface with the goal specification?\n30:20 - The fundamental problems of exploration\n32:15 - Are these algorithms subject to catastrophic forgetting?\n34:45 - What current models could bring language to other environments?\n40:30 - What does it take in terms of hardware?\n43:00 - What problems did you encounter during the project?\n46:40 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2202.08938\n\nAbstract:\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\n\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rockt\u00e4schel, Edward Grefenstette\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "92": "#ai #accel #evolution\n\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\n\nOUTLINE:\n0:00 - Intro & Demonstration\n3:50 - Paper overview\n5:20 - The ACCEL algorithm\n15:25 - Looking at the pseudocode\n23:10 - Approximating regret\n33:45 - Experimental results\n40:00 - Discussion & Comments\n\nWebsite: https://accelagent.github.io\nPaper: https://arxiv.org/abs/2203.01302\n\nAbstract:\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\n\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rockt\u00e4schel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "93": "#ai #accel #evolution\n\nThis is an interview with the authors Jack Parker-Holder and Minqi Jiang.\nOriginal Paper Review Video: https://www.youtube.com/watch?v=povBDxUn1VQ\n\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\n\nOUTLINE:\n0:00 - Intro\n1:00 - Start of interview\n4:45 - How did you get into this field?\n8:10 - What is minimax regret?\n11:45 - What levels does the regret objective select?\n14:20 - Positive value loss (correcting my mistakes)\n21:05 - Why is the teacher not learned?\n24:45 - How much domain-specific knowledge is needed?\n29:30 - What problems is this applicable to?\n33:15 - Single agent vs population of agents\n37:25 - Measuring and balancing level difficulty\n40:35 - How does generalization emerge?\n42:50 - Diving deeper into the experimental results\n47:00 - What are the unsolved challenges in the field?\n50:00 - Where do we go from here?\n\nWebsite: https://accelagent.github.io\nPaper: https://arxiv.org/abs/2203.01302\nICLR Workshop: https://sites.google.com/view/aloe2022\nBook on topic: https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/\n\nAbstract:\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\n\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rockt\u00e4schel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "94": "#saycan #robots #ai\n\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy's ability to progress towards the goal, given by the language model, and the policy's probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nSponsor: Zeta Alpha\nhttps://zeta-alpha.com\nUse code YANNIC for 20% off!\n\nOUTLINE:\n0:00 - Introduction & Overview\n3:20 - Sponsor: Zeta Alpha\n5:00 - Using language models for action planning\n8:00 - Combining LLMs with learned atomic skills\n16:50 - The full SayCan system\n20:30 - Experimental setup and data collection\n21:25 - Some weaknesses & strengths of the system\n27:00 - Experimental results\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "95": "#saycan #robots #ai\n\nThis is an interview with the authors Brian Ichter, Karol Hausman, and Fei Xia.\nOriginal Paper Review Video: https://youtu.be/Ru23eWAQ6_E\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy's ability to progress towards the goal, given by the language model, and the policy's probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nOUTLINE:\n0:00 - Introduction & Setup\n3:40 - Acquiring atomic low-level skills\n7:45 - How does the language model come in?\n11:45 - Why are you scoring instead of generating?\n15:20 - How do you deal with ambiguity in language?\n20:00 - The whole system is modular\n22:15 - Going over the full algorithm\n23:20 - What if an action fails?\n24:30 - Debunking a marketing video :)\n27:25 - Experimental Results\n32:50 - The insane scale of data collection\n40:15 - How do you go about large-scale projects?\n43:20 - Where did things go wrong?\n45:15 - Where do we go from here?\n52:00 - What is the largest unsolved problem in this?\n53:35 - Thoughts on the Tesla Bot\n55:00 - Final thoughts\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "96": "#alphatensor #deepmind #ai \n\nMatrix multiplication is the most used mathematical operation in all of science and engineering. Speeding this up has massive consequences. Thus, over the years, this operation has become more and more optimized. A fascinating discovery was made when it was shown that one actually needs less than N^3 multiplication operations to multiply to NxN matrices. DeepMind goes a step further and creates AlphaTensor, a Deep Reinforcement Learning algorithm that plays a single-player game, TensorGame, in order to find even more optimized algorithms for matrix multiplication. And it turns out, there exists a plethora of undiscovered matrix multiplication algorithms, which not only will make everything from computers to smart toasters faster, but also bring new insights into fundamental math and complexity theory.\n\nSponsor: Assembly AI\nLink: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic_sentiment\n\nOUTLINE:\n0:00 - Intro\n1:50 - Sponsor: Assembly AI (link in description)\n3:25 - What even is Matrix Multiplication?\n6:10 - A very astounding fact\n8:45 - Trading multiplications for additions\n12:35 - Matrix Multiplication as a Tensor\n17:30 - Tensor Decompositions\n20:30 - A formal way of finding multiplication algorithms\n31:00 - How to formulate this as a game?\n39:30 - A brief primer on AlphaZero / MCTS\n45:40 - The Results\n48:15 - Optimizing for different hardware\n52:40 - Expanding fundamental math\n53:45 - Summary & Final Comments\n\nPaper: https://www.nature.com/articles/s41586-022-05172-4\nTitle: Discovering faster matrix multiplication algorithms with reinforcement learning\n\nAbstract:\nImproving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\u2009\u00d7\u20094 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.\n\nAuthors: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis & Pushmeet Kohli\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "97": "#ai #cicero #diplomacy \n\nA team from Meta AI has developed Cicero, an agent that can play the game Diplomacy, in which players have to communicate via chat messages to coordinate and plan into the future.\n\nPaper Title: Human-level play in the game of Diplomacy by combining language models with strategic reasoning\n\nCommented game by human expert: https://www.youtube.com/watch?v=u5192bvUS7k\n\nOUTLINE:\n0:00 - Introduction\n9:50 - AI in cooperation games\n13:50 - Cicero agent overview\n25:00 - A controllable dialogue model\n36:50 - Dialogue-conditional strategic planning\n49:00 - Message filtering\n53:45 - Cicero's play against humans\n55:15 - More examples & discussion\n\nHomepage: https://ai.facebook.com/research/cicero/\nCode: https://github.com/facebookresearch/diplomacy_cicero\nBlog: https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/\nPaper: https://www.science.org/doi/10.1126/science.ade9097\n\nAbstract:\nDespite much progress in training AI systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players' beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\n\nAuthors: Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "98": "A look at OpenAI's new GPT-2 model and the surrounding controversy.\n\nhttps://blog.openai.com/better-language-models/\n\nAbstract:\nNatural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n\nAuthors:\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever", "99": "https://arxiv.org/abs/1810.04805\n\nAbstract:\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\n\nAuthors:\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "100": "We present a stochastic non-autoregressive RNN that does not require teacher-forcing for training. The content is based on our 2018 NeurIPS paper:\n\nDeep State Space Models for Unconditional Word Generation\nhttps://arxiv.org/abs/1806.04550", "101": "https://arxiv.org/abs/1706.03762\n\nAbstract:\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\nAuthors:\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin", "102": "Abstract:\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.\n\nAuthors: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le\n\nhttps://arxiv.org/abs/1906.08237", "103": "This paper shows that the original BERT model, if trained correctly, can outperform all of the improvements that have been proposed lately, raising questions about the necessity and reasoning behind these.\n\nAbstract:\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n\nAuthors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\n\nhttps://arxiv.org/abs/1907.11692\n\n\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nMinds: https://www.minds.com/ykilcher\nBitChute: https://www.bitchute.com/channel/10a5ui845DOJ/", "104": "The AI cook is here! This agent learns to play a text-based game where the goal is to prepare a meal according to a recipe. Challenges? Many! The number of possible actions is huge, ingredients change and can include ones never seen before, you need to navigate rooms, use tools, manage an inventory and sequence everything correctly and all of this from a noisy textual description that the game engine throws at you. This paper mixes supervised explicit training with reinforcement learning in order to solve this task.\n\nAbstract:\nWhile Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's \"First TextWorld Problems: A Language and Reinforcement Learning Challenge\" and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.\n\nAuthors: Leonard Adolphs, Thomas Hofmann\n\nhttps://arxiv.org/abs/1909.01646", "105": "The Transformer for the masses! Reformer solves the biggest problem with the famous Transformer model: Its huge resource requirements. By cleverly combining Locality Sensitive Hashing and ideas from Reversible Networks, the classically huge footprint of the Transformer is drastically reduced. Not only does that mean the model uses less memory, but it can process much longer input sequences, up to 16K tokens with just 16gb of memory!\n\nhttps://arxiv.org/abs/2001.04451\nhttps://ai.googleblog.com/2020/01/reformer-efficient-transformer.html\n\nAbstract:\nLarge Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(LlogL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\nAuthors: Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "106": "Microsoft has trained a 17-billion parameter language model that achieves state-of-the-art perplexity. This video takes a look at the ZeRO optimizer that enabled this breakthrough. ZeRO allows you to do model- and data-parallelism without having huge cuts in training speed.\n\nhttps://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\nhttps://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/\nhttps://github.com/microsoft/DeepSpeed\nhttps://arxiv.org/abs/1910.02054\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "107": "This model solves integrals and ODEs by doing seq2seq!\n\nhttps://arxiv.org/abs/1912.01412\nhttps://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/\n\nAbstract:\nNeural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.\n\nAuthors: Guillaume Lample, Fran\u00e7ois Charton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "108": "Current NLP models are often \"cheating\" on supervised learning tasks by exploiting correlations that arise from the particularities of the dataset. Therefore they often fail to learn the original intent of the dataset creators. This paper argues that NLP models should be evaluated on Contrast Sets, which are hand-crafted perturbations by the dataset authors that capture their intent in a meaningful way.\n\nhttps://arxiv.org/abs/2004.02709\n\nAbstract:\nStandard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities. We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.\n\nAuthors: Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, Ben Zhou\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "109": "The imputer is a sequence-to-sequence model that strikes a balance between fully autoregressive models with long inference times and fully non-autoregressive models with fast inference. The imputer achieves constant decoding time independent of sequence length by exploiting dynamic programming.\n\nhttps://arxiv.org/abs/2002.08926\n\nAbstract:\nThis paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generative model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.\n\nAuthors: William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "110": "The Longformer extends the Transformer by introducing sliding window attention and sparse global attention. This allows for the processing of much longer documents than classic models like BERT.\n\nPaper: https://arxiv.org/abs/2004.05150\nCode: https://github.com/allenai/longformer\n\nAbstract:\nTransformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.\n\nAuthors: Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "111": "This is what a 9 Billion parameter transformer can do. I take a look at FAIR's new paper \"Recipes for building an open-domain chatbot\" and try out their chatbot live!\n\nJump to 3:00 to see the chatbot in action.\n\nPaper: https://arxiv.org/abs/2004.13637\nBlog: https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/\nCode: https://parl.ai/projects/blender/\n\nAbstract:\nBuilding open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available under the collective name Blender. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\n\nAuthors: Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "112": "Answering complex questions about tabular information is hard. No two tables are alike and sometimes the answer you're looking for is not even in the table and needs to be computed from a subset of the cells. Surprisingly, this model can figure it all out by itself through some clever input encoding and loss engineering.\n\nPaper: https://arxiv.org/abs/2004.02349\nCode: https://github.com/google-research/tapas\n\nAbstract:\nAnswering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.\n\nAuthors: Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Martin Eisenschlos\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "113": "Huggingface released its newest library called NLP, which gives you easy access to almost any NLP dataset and metric in one convenient interface. We will combine this with a BERT model from Huggingface's Transformers library to build a sentiment classifier for IMDB.\n\nOUTLINE:\n0:00 - Intro\n1:30 - Boilerplate\n3:20 - PyTorch Lightning Module\n9:50 - Load Dataset\n12:15 - Tokenization\n20:50 - Torch Tensors\n25:50 - Data Loader\n28:00 - Create BERT Model\n32:00 - Implement Validation and Train Step\n47:00 - Run & Recap\n50:20 - Epilogue\n\nMy Code: https://github.com/yk/huggingface-nlp-demo\nNLP Library: https://github.com/huggingface/nlp\nTutorial Colab: https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb\nTransformers Library: https://github.com/huggingface/transformers\nPytorch Lightning: https://github.com/PyTorchLightning/pytorch-lightning\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "114": "This code completion engine can write an entire function from just the name! OpenAI demonstrates what happens when you learn a language model on thousands of GitHub Python repositories.\n\nSource Clip: https://youtu.be/fZSFNUT6iY8\nFull Video: https://www.pscp.tv/Microsoft/1OyKAYWPRrWKb\nKite: https://kite.com/\nTabNine: https://www.tabnine.com/\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "115": "BERT is a giant model. Turns out you can prune away many of its components and it still works. This paper analyzes BERT pruning in light of the Lottery Ticket Hypothesis and finds that even the \"bad\" lottery tickets can be fine-tuned to good accuracy.\n\nOUTLINE:\n0:00 - Overview\n1:20 - BERT\n3:20 - Lottery Ticket Hypothesis\n13:00 - Paper Abstract\n18:00 - Pruning BERT\n23:00 - Experiments\n50:00 - Conclusion\n\nhttps://arxiv.org/abs/2005.00561\n\nML Street Talk Channel: https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ\n\nAbstract:\nMuch of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the \"bad\" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the \"good\" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the \"good\" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.\n\nAuthors: Sai Prasanna, Anna Rogers, Anna Rumshisky\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "116": "#gpt3 #openai #gpt-3\n\nHow far can you go with ONLY language modeling? Can a large enough language model perform NLP task out of the box? OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before and the results are astounding.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Language Models\n2:45 - Language Modeling Datasets\n3:20 - Model Size\n5:35 - Transformer Models\n7:25 - Fine Tuning\n10:15 - In-Context Learning\n17:15 - Start of Experimental Results\n19:10 - Question Answering\n23:10 - What I think is happening\n28:50 - Translation\n31:30 - Winograd Schemes\n33:00 - Commonsense Reasoning\n37:00 - Reading Comprehension\n37:30 - SuperGLUE\n40:40 - NLI\n41:40 - Arithmetic Expressions\n48:30 - Word Unscrambling\n50:30 - SAT Analogies\n52:10 - News Article Generation\n58:10 - Made-up Words\n1:01:10 - Training Set Contamination\n1:03:10 - Task Examples\n\nhttps://arxiv.org/abs/2005.14165\nhttps://github.com/openai/gpt-3\n\nAbstract:\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\nAuthors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "117": "Do we really need dot-product attention? The attention mechanism is a central part of modern Transformers, mainly due to the dot-product attention mechanism. This paper changes the mechanism to remove the quadratic interaction terms and comes up with a new model, the Synthesizer. As it turns out, you can do pretty well like that!\n\nOUTLINE:\n0:00 - Intro & High Level Overview\n1:00 - Abstract\n2:30 - Attention Mechanism as Information Routing\n5:45 - Dot Product Attention\n8:05 - Dense Synthetic Attention\n15:00 - Random Synthetic Attention\n17:15 - Comparison to Feed-Forward Layers\n22:00 - Factorization & Mixtures\n23:10 - Number of Parameters\n25:35 - Machine Translation & Language Modeling Experiments\n36:15 - Summarization & Dialogue Generation Experiments\n37:15 - GLUE & SuperGLUE Experiments\n42:00 - Weight Sizes & Number of Head Ablations\n47:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2005.00743\nMy Video on Transformers (Attention Is All You Need): https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nThe dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \\textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).\n\nAuthors: Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "118": "Deep neural networks are large models and pruning has become an important part of ML product pipelines, making models small while keeping their performance high. However, the classic pruning method, Magnitude Pruning, is suboptimal in models that are obtained by transfer learning. This paper proposes a solution, called Movement Pruning and shows its superior performance.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:55 - Magnitude Pruning\n4:25 - Transfer Learning\n7:25 - The Problem with Magnitude Pruning in Transfer Learning\n9:20 - Movement Pruning\n22:20 - Experiments\n24:20 - Improvements via Distillation\n26:40 - Analysis of the Learned Weights\n\nPaper: https://arxiv.org/abs/2005.07683\nCode: https://github.com/huggingface/transformers/tree/master/examples/movement-pruning\n\nAbstract:\nMagnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.\n\nAuthors: Victor Sanh, Thomas Wolf, Alexander M. Rush\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "119": "Proper evaluation of text generation models, such as machine translation systems, requires expensive and slow human assessment. As these models have gotten better in previous years, proxy-scores, like BLEU, are becoming less and less useful. This paper proposes to learn a proxy score and demonstrates that it correlates well with human raters, even as the data distribution shifts.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:00 - The Problem with Evaluating Machine Translation\n5:10 - Task Evaluation as a Learning Problem\n10:45 - Naive Fine-Tuning BERT\n13:25 - Pre-Training on Synthetic Data\n16:50 - Generating the Synthetic Data\n18:30 - Priming via Auxiliary Tasks\n23:35 - Experiments & Distribution Shifts\n27:00 - Concerns & Conclusion\n\nPaper: https://arxiv.org/abs/2004.04696\nCode: https://github.com/google-research/bleurt\n\nAbstract:\nText generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.\n\nAbstract: Thibault Sellam, Dipanjan Das, Ankur P. Parikh\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "120": "Code migration between languages is an expensive and laborious task. To translate from one language to the other, one needs to be an expert at both. Current automatic tools often produce illegible and complicated code. This paper applies unsupervised neural machine translation to source code of Python, C++, and Java and is able to translate between them, without ever being trained in a supervised fashion.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - The Transcompiling Problem\n5:55 - Neural Machine Translation\n8:45 - Unsupervised NMT\n12:55 - Shared Embeddings via Token Overlap\n20:45 - MLM Objective\n25:30 - Denoising Objective\n30:10 - Back-Translation Objective\n33:00 - Evaluation Dataset\n37:25 - Results\n41:45 - Tokenization\n42:40 - Shared Embeddings\n43:30 - Human-Aware Translation\n47:25 - Failure Cases\n48:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.03511\n\nAbstract:\nA transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.\n\nAuthors: Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "121": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "122": "Pre-training a CNN backbone for visual transfer learning has recently seen a big push into the direction of incorporating more data, at the cost of less supervision. This paper investigates the opposite: Visual transfer learning by pre-training from very few, but very high-quality samples on an image captioning task.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pre-Training for Visual Tasks\n3:40 - Quality-Quantity Tradeoff\n5:50 - Image Captioning\n8:35 - VirTex Method\n14:30 - Linear Classification\n20:30 - Ablations\n22:05 - Fine-Tuning\n25:45 - Attention Visualization\n27:30 - Conclusion & Remarks\n\nPaper: https://arxiv.org/abs/2006.06666\nCode: https://github.com/kdexd/virtex\n\nAbstract:\nThe de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.\n\nAuthors: Karan Desai, Justin Johnson\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "123": "Determining the stability properties of differential systems is a challenging task that involves very advanced symbolic and numeric mathematical manipulations. This paper shows that given enough training data, a simple language model with no underlying knowledge of mathematics can learn to solve these problems with remarkably high accuracy.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Differential System Tasks\n11:30 - Datasets & Models\n15:15 - Experiments\n21:00 - Discussion & My Comments\n\nPaper: https://arxiv.org/abs/2006.06462\nMy Video on Deep Learning for Symbolic Mathematics: https://youtu.be/p3sAF3gVMMA\n\nAbstract:\nCan advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.\n\nAuthors: Fran\u00e7ois Charton, Amaury Hayat, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "124": "Google builds a 600 billion parameter transformer to do massively multilingual, massive machine translation. Interestingly, the larger model scale does not come from increasing depth of the transformer, but from increasing width in the feedforward layers, combined with a hard routing to parallelize computations on up to 2048 TPUs. A very detailed engineering paper!\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Main Results\n5:10 - Mixture-of-Experts\n16:00 - Difference to Scaling Classic Transformers\n18:50 - Backpropagation in Mixture-of-Experts\n20:05 - MoE Routing Algorithm in GShard\n38:20 - GShard Einsum Examples\n47:40 - Massively Multilingual Translation\n56:00 - Results\n1:11:30 - Conclusion & Comments\n\nERRATA:\nI said the computation of MoE scales linearly, but actually, it's sub(!)-linear.\n\nPaper: https://arxiv.org/abs/2006.16668\n\nAbstract:\nNeural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.\n\nAuthors:\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "125": "Proteins are the workhorses of almost all cellular functions and a core component of life. But despite their versatility, all proteins are built as sequences of the same 20 amino acids. These sequences can be analyzed with tools from NLP. This paper investigates the attention mechanism of a BERT model that has been trained on protein sequence data and discovers that the language model has implicitly learned non-trivial higher-order biological properties of proteins.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - From DNA to Proteins\n5:20 - BERT for Amino Acid Sequences\n8:50 - The Structure of Proteins\n12:40 - Investigating Biological Properties by Inspecting BERT\n17:45 - Amino Acid Substitution\n24:55 - Contact Maps\n30:15 - Binding Sites\n33:45 - Linear Probes\n35:25 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15222\nCode: https://github.com/salesforce/provis\n\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nTransformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at this https URL.\n\nAuthors: Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "126": "#ai #attention #transformer #deeplearning\n\nTransformers are famous for two things: Their superior performance and their insane requirements of compute and memory. This paper reformulates the attention mechanism in terms of kernel functions and obtains a linear formulation, which reduces these requirements. Surprisingly, this formulation also surfaces an interesting connection between autoregressive transformers and RNNs.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Softmax Attention & Transformers\n8:40 - Quadratic Complexity of Softmax Attention\n9:40 - Generalized Attention Mechanism\n13:45 - Kernels\n20:40 - Linear Attention\n25:20 - Experiments\n28:30 - Intuition on Linear Attention\n33:55 - Connecting Autoregressive Transformers and RNNs\n41:30 - Caveats with the RNN connection\n46:00 - More Results & Conclusion\n\nPaper: https://arxiv.org/abs/2006.16236\nWebsite: https://linear-transformers.com/\nCode: https://github.com/idiap/fast-transformers\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nTransformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \ue23b(N2) to \ue23b(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\nAuthors: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "127": "#ai #research #word2vec\n\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:50 - Distributed Word Representations\n5:40 - Skip-Gram Model\n12:00 - Hierarchical Softmax\n14:55 - Negative Sampling\n22:30 - Mysterious 3/4 Power\n25:50 - Frequent Words Subsampling\n28:15 - Empirical Results\n29:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1310.4546\nCode: https://code.google.com/archive/p/word2vec/\n\nAbstract:\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\nAuthors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "128": "#ai #nlp #attention\n\nThe quadratic resource requirements of the attention mechanism are the main roadblock in scaling up transformers to long sequences. This paper replaces the full quadratic attention mechanism by a combination of random attention, window attention, and global attention. Not only does this allow the processing of longer sequences, translating to state-of-the-art experimental results, but also the paper shows that BigBird comes with theoretical guarantees of universal approximation and turing completeness.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:50 - Quadratic Memory in Full Attention\n4:55 - Architecture Overview\n6:35 - Random Attention\n10:10 - Window Attention\n13:45 - Global Attention\n15:40 - Architecture Summary\n17:10 - Theoretical Result\n22:00 - Experimental Parameters\n25:35 - Structured Block Computations\n29:30 - Recap\n31:50 - Experimental Results\n34:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2007.14062\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on Longformer: https://youtu.be/_8KNb5iqblE\n... and its memory requirements: https://youtu.be/gJR28onlqzs\n\nAbstract:\nTransformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n\nAuthors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "129": "#ai #transformer #attention\n\nHopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Binary Hopfield Networks\n5:55 - Continuous Hopfield Networks\n8:15 - Update Rules & Energy Functions\n13:30 - Connection to Transformers\n14:35 - Hopfield Attention Layers\n26:45 - Theoretical Analysis\n48:10 - Investigating BERT\n1:02:30 - Immune Repertoire Classification\n\nPaper: https://arxiv.org/abs/2008.02217\nCode: https://github.com/ml-jku/hopfield-layers\nImmune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nWe show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL\n\nAuthors: Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "130": "#ai #tech #science\n\nOpen Domain Question Answering is one of the most challenging tasks in NLP. When answering a question, the model is able to retrieve arbitrary documents from an indexed corpus to gather more information. REALM shows how Masked Language Modeling (MLM) pretraining can be used to train a retriever for relevant documents in an end-to-end fashion and improves over state-of-the-art by a significant margin.\n\nOUTLINE:\n0:00 - Introduction & Overview\n4:30 - World Knowledge in Language Models\n8:15 - Masked Language Modeling for Latent Document Retrieval\n14:50 - Problem Formulation\n17:30 - Knowledge Retriever Model using MIPS\n23:50 - Question Answering Model\n27:50 - Architecture Recap\n29:55 - Analysis of the Loss Gradient\n34:15 - Initialization using the Inverse Cloze Task\n41:40 - Prohibiting Trivial Retrievals\n44:05 - Null Document\n45:00 - Salient Span Masking\n50:15 - My Idea on Salient Span Masking\n51:50 - Experimental Results and Ablations\n57:30 - Concrete Example from the Model\n\nPaper: https://arxiv.org/abs/2002.08909\nCode: https://github.com/google-research/language/tree/master/language/realm\n\nMy Video on GPT-3: https://www.youtube.com/watch?v=SY5PvZrJhLE\nMy Video on BERT: https://www.youtube.com/watch?v=-9evrZnBorM\nMy Video on Word2Vec: https://www.youtube.com/watch?v=yexR53My2O4\n\nAbstract:\nLanguage model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\n\nAuthors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "131": "#summarization #gpt3 #openai\n\nText Summarization is a hard task, both in training and evaluation. Training is usually done maximizing the log-likelihood of a human-generated reference summary, while evaluation is performed using overlap-based metrics like ROUGE. Both significantly undervalue the breadth and intricacies of language and the nature of the information contained in text summaries. This paper by OpenAI includes direct human feedback both in evaluation and - via reward model proxies - in training. The final model even outperforms single humans when judged by other humans and is an interesting application of using reinforcement learning together with humans in the loop.\n\nOUTLINE:\n0:00 - Intro & Overview\n5:35 - Summarization as a Task\n7:30 - Problems with the ROUGE Metric\n10:10 - Training Supervised Models\n12:30 - Main Results\n16:40 - Including Human Feedback with Reward Models & RL\n26:05 - The Unknown Effect of Better Data\n28:30 - KL Constraint & Connection to Adversarial Examples\n37:15 - More Results\n39:30 - Understanding the Reward Model\n41:50 - Limitations & Broader Impact\n\nPaper: https://arxiv.org/abs/2009.01325\nBlog: https://openai.com/blog/learning-to-summarize-with-human-feedback/\nCode: https://github.com/openai/summarize-from-feedback\nSamples: https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/\n\nMy Video on GPT-3: https://youtu.be/SY5PvZrJhLE\nMy Video on GPT-2: https://youtu.be/u1_qMdb0kYU\n\nAbstract:\nAs language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\n\nAuthors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "132": "#ai #research #attention\n\nTransformers have huge memory and compute requirements because they construct an Attention matrix, which grows quadratically in the size of the input. The Performer is a model that uses random positive orthogonal features to construct an unbiased estimator to the Attention matrix and obtains an arbitrarily good approximation in linear time! The method generalizes beyond attention and opens the door to the next generation of deep learning architectures.\n\nOUTLINE:\n0:00 - Intro & Outline\n6:15 - Quadratic Bottleneck in Attention Mechanisms\n10:00 - Decomposing the Attention Matrix\n15:30 - Approximating the Softmax Kernel\n24:45 - Different Choices, Different Kernels\n28:00 - Why the Naive Approach does not work!\n31:30 - Better Approximation via Positive Features\n36:55 - Positive Features are Infinitely Better\n40:10 - Orthogonal Features are Even Better\n43:25 - Experiments\n49:20 - Broader Impact Statement\n50:00 - Causal Attention via Prefix Sums\n52:10 - Code\n53:50 - Final Remarks & Conclusion\n\nPaper: https://arxiv.org/abs/2009.14794\nCode: https://github.com/google-research/google-research/tree/master/performer\nBlog: https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html\n\nKernels on ML Street Talk: https://www.youtube.com/watch?v=y_RjsDHl5Y4\nMy Video on Linformer: https://www.youtube.com/watch?v=-_2AF9Lhweo\nMy Video on Reformer: https://www.youtube.com/watch?v=i4H0kjxrias\nMy Video on Attention: https://www.youtube.com/watch?v=iDulhoQ2pro\n\nAbstract:\nWe introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\nAuthors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "133": "#ai #research #nlp\n\nKnowledge Graphs are structured databases that capture real-world entities and their relations to each other. KGs are usually built by human experts, which costs considerable amounts of time and money. This paper hypothesizes that language models, which have increased their performance dramatically in the last few years, contain enough knowledge to use them to construct a knowledge graph from a given corpus, without any fine-tuning of the language model itself. The resulting system can uncover new, unknown relations and outperforms all baselines in automated KG construction, even trained ones!\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - TabNine Promotion\n4:20 - Title Misnomer\n6:45 - From Corpus To Knowledge Graph\n13:40 - Paper Contributions\n15:50 - Candidate Fact Finding Algorithm\n25:50 - Causal Attention Confusion\n31:25 - More Constraints\n35:00 - Mapping Facts To Schemas\n38:40 - Example Constructed Knowledge Graph\n40:10 - Experimental Results\n47:25 - Example Discovered Facts\n50:40 - Conclusion & My Comments\n\nPaper: https://arxiv.org/abs/2010.11967\n\nAbstract:\nThis paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.\n\nAuthors: Chenguang Wang, Xiao Liu, Dawn Song\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "134": "#ai #privacy #tech\n\nThis paper demonstrates a method to extract verbatim pieces of the training data from a trained language model. Moreover, some of the extracted pieces only appear a handful of times in the dataset. This points to serious security and privacy implications for models like GPT-3. The authors discuss the risks and propose mitigation strategies.\n\nOUTLINE:\n0:00 - Intro & Overview\n9:15 - Personal Data Example\n12:30 - Eidetic Memorization & Language Models\n19:50 - Adversary's Objective & Outlier Data\n24:45 - Ethical Hedging\n26:55 - Two-Step Method Overview\n28:20 - Perplexity Baseline\n30:30 - Improvement via Perplexity Ratios\n37:25 - Weights for Patterns & Weights for Memorization\n43:40 - Analysis of Main Results\n1:00:30 - Mitigation Strategies\n1:01:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2012.07805\n\nAbstract:\nIt has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\n\nAuthors: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "135": "#openai #science #gpt3\n\nOpenAI's newest model, DALL\u00b7E, shows absolutely amazing abilities in generating high-quality images from arbitrary text descriptions. Like GPT-3, the range of applications and the diversity of outputs is astonishing, given that this is a single model, trained on a purely autoregressive task. This model is a significant step towards the combination of text and images in future AI applications.\n\nOUTLINE:\n0:00 - Introduction\n2:45 - Overview\n4:20 - Dataset\n5:35 - Comparison to GPT-3\n7:00 - Model Architecture\n13:20 - VQ-VAE\n21:00 - Combining VQ-VAE with GPT-3\n27:30 - Pre-Training with Relaxation\n32:15 - Experimental Results\n33:00 - My Hypothesis about DALL\u00b7E's inner workings\n36:15 - Sparse Attention Patterns\n38:00 - DALL\u00b7E can't count\n39:35 - DALL\u00b7E can't global order\n40:10 - DALL\u00b7E renders different views\n41:10 - DALL\u00b7E is very good at texture\n41:40 - DALL\u00b7E can complete a bust\n43:30 - DALL\u00b7E can do some reflections, but not others\n44:15 - DALL\u00b7E can do cross-sections of some objects\n45:50 - DALL\u00b7E is amazing at style\n46:30 - DALL\u00b7E can generate logos\n47:40 - DALL\u00b7E can generate bedrooms\n48:35 - DALL\u00b7E can combine unusual concepts\n49:25 - DALL\u00b7E can generate illustrations\n50:15 - DALL\u00b7E sometimes understands complicated prompts\n50:55 - DALL\u00b7E can pass part of an IQ test\n51:40 - DALL\u00b7E probably does not have geographical / temporal knowledge\n53:10 - Reranking dramatically improves quality\n53:50 - Conclusions & Comments\n\nBlog: https://openai.com/blog/dall-e/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "136": "#ai #openai #technology\n\nPaper Title: Learning Transferable Visual Models From Natural Language Supervision\nCLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new image & text tasks.\n\nOUTLINE:\n0:00 - Introduction\n3:15 - Overview\n4:40 - Connecting Images & Text\n9:00 - Building Zero-Shot Classifiers\n14:40 - CLIP Contrastive Training Objective\n22:25 - Encoder Choices\n25:00 - Zero-Shot CLIP vs Linear ResNet-50\n31:50 - Zero-Shot vs Few-Shot\n35:35 - Scaling Properties\n36:35 - Comparison on different tasks\n37:40 - Robustness to Data Shift\n44:20 - Broader Impact Section\n47:00 - Conclusion & Comments\n\nPaper: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf\nBlog: https://openai.com/blog/clip/\nCode: https://github.com/openai/CLIP\n\nAbstract:\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "137": "#ai #technology #switchtransformer\n\nScale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model's parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:30 - Performance Gains from Scale\n8:30 - Switch Transformer Architecture\n17:00 - Model-, Data- and Expert-Parallelism\n25:30 - Experimental Results\n29:00 - Stabilizing Training\n32:20 - Distillation into Dense Models\n33:30 - Final Comments\n\nPaper: https://arxiv.org/abs/2101.03961\nCodebase T5: https://github.com/google-research/text-to-text-transfer-transformer\n\nAbstract:\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n\nAuthors: William Fedus, Barret Zoph, Noam Shazeer\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "138": "#ai #science #transformers\n\nAutoregressive Transformers have taken over the world of Language Modeling (GPT-3). However, in order to train them, people use causal masking and sample parallelism, which means computation only happens in a feedforward manner. This results in higher layer information, which would be available, to not be used in the lower layers of subsequent tokens, and leads to a loss in the computational capabilities of the overall model. Feedback Transformers trade-off training speed for access to these representations and demonstrate remarkable improvements in complex reasoning and long-range dependency tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems of Autoregressive Processing\n3:30 - Information Flow in Recurrent Neural Networks\n7:15 - Information Flow in Transformers\n9:10 - Solving Complex Computations with Neural Networks\n16:45 - Causal Masking in Transformers\n19:00 - Missing Higher Layer Information Flow\n26:10 - Feedback Transformer Architecture\n30:00 - Connection to Attention-RNNs\n36:00 - Formal Definition\n37:05 - Experimental Results\n43:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2002.09402\n\nMy video on Attention: https://youtu.be/iDulhoQ2pro\n\nERRATA: Sometimes I say \"Switch Transformer\" instead of \"Feedback Transformer\". Forgive me :)\n\nAbstract:\nTransformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\n\nAuthors: Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "139": "#transformer #nystromer #nystromformer\n\nThe Nystr\u00f6mformer (or Nystromformer, Nystr\u00f6mer, Nystromer), is a new drop-in replacement for approximating the Self-Attention matrix in Transformers with linear memory and time requirements. Most importantly, it uses the Nystrom-Method to subselect (or segment mean) queries and keys as so-called landmarks and uses those to reconstruct the inherently low-rank attention matrix. This is relevant for many areas of Machine Learning, especially Natural Language processing, where it enables longer sequences of text to be processed at once.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - The Quadratic Memory Bottleneck in Self-Attention\n7:20 - The Softmax Operation in Attention\n11:15 - Nystr\u00f6m-Approximation\n14:00 - Getting Around the Softmax Problem\n18:05 - Intuition for Landmark Method\n28:05 - Full Algorithm\n30:20 - Theoretical Guarantees\n35:55 - Avoiding the Large Attention Matrix\n36:55 - Subsampling Keys vs Negative Sampling\n43:15 - Experimental Results\n47:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.03902\nCode: https://github.com/mlpen/Nystromformer\nAppendix: https://github.com/mlpen/Nystromformer/blob/main/doc/Nystromformer_Supplement.pdf\nLRA Results: https://twitter.com/tanmingxing/status/1359301186734620675\nTwitter lucidrains w/ author: https://twitter.com/lucidrains/status/1359597104075661312\nTwitter lucidrains w/ _clashluke: https://twitter.com/_clashluke/status/1359483460851802115\n\nAbstract:\nTransformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -- a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard Transformer. Our code is at this https URL.\n\nAuthors: Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "140": "#deberta #bert #huggingface\n\nDeBERTa by Microsoft is the next iteration of BERT-style Self-Attention Transformer models, surpassing RoBERTa in State-of-the-art in multiple NLP tasks. DeBERTa brings two key improvements: First, they treat content and position information separately in a new form of disentangled attention mechanism. Second, they resort to relative positional encodings throughout the base of the transformer, and provide absolute positional encodings only at the very end. The resulting model is both more accurate on downstream tasks and needs less pretraining steps to reach good accuracy. Models are also available in Huggingface and on Github.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:15 - Position Encodings in Transformer's Attention Mechanism\n9:55 - Disentangling Content & Position Information in Attention\n21:35 - Disentangled Query & Key construction in the Attention Formula\n25:50 - Efficient Relative Position Encodings\n28:40 - Enhanced Mask Decoder using Absolute Position Encodings\n35:30 - My Criticism of EMD\n38:05 - Experimental Results\n40:30 - Scaling up to 1.5 Billion Parameters\n44:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.03654\nCode: https://github.com/microsoft/DeBERTa\nHuggingface models: https://huggingface.co/models?search=deberta\n\nAbstract:\nRecent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).\n\nAuthors: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "141": "#alibi #transformers #attention\n\nTransformers are essentially set models that need additional inputs to make sense of sequence data. The most widespread additional inputs are position encodings or position embeddings, which add sequence index information in various forms. However, this has put a limit on the resulting model, which cannot run inference on sequences longer than it has been trained on, as it would encounter unfamiliar position encodings. ALiBi solves this by proposing simple linear fixed biases as position information, adding negligible overhead in time and memory, but surprisingly, the resulting model is able to handle inference on sequences many times as long as its training sequences.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Position Encodings in Transformers\n4:55 - Sinusoidial Position Encodings\n11:50 - ALiBi Position Encodings\n20:50 - How to choose the slope parameter\n23:55 - Experimental Results\n29:10 - Comments & Conclusion\n\nPaper: https://ofir.io/train_short_test_long.pdf\nCode: https://github.com/ofirpress/attention_with_linear_biases\n\nAbstract:\nSince the introduction of the transformer model by Vaswani et al. (2017), a fundamental question remains open: how to achieve extrapolation at inference time to longer sequences than seen during training? We first show that extrapolation can be improved by changing the position representation method, though we find that existing proposals do not allow efficient extrapolation. We introduce a simple and efficient method, Attention with Linear Biases (ALiBi), that allows for extrapolation. ALiBi does not add positional embeddings to the word embeddings; instead, it biases the query-key attention scores with a term that is proportional to their distance. We show that this method allows training a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048, 11% faster and using 11% less memory. ALiBi\u2019s inductive bias towards recency allows it to outperform multiple strong position methods on the WikiText-103 benchmark. Finally, we provide analysis of ALiBi to understand why it leads to better performance.\n\nAuthors: Ofir Press, Noah A. Smith, Mike Lewis\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "142": "#inftyformer #infinityformer #transformer\n\nVanilla Transformers are excellent sequence models, but suffer from very harsch constraints on the length of the sequences they can process. Several attempts have been made to extend the Transformer's sequence length, but few have successfully gone beyond a constant factor improvement. This paper presents a method, based on continuous attention mechanisms, to attend to an unbounded past sequence by representing the past as a continuous signal, rather than a sequence. This enables the Infty-Former to effectively enrich the current context with global information, which increases performance on long-range dependencies in sequence tasks. Further, the paper presents the concept of sticky memories, which highlight past events that are of particular importance and elevates their representation in the long-term memory.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Sponsor Spot: Weights & Biases\n3:35 - Problem Statement\n8:00 - Continuous Attention Mechanism\n16:25 - Unbounded Memory via concatenation & contraction\n18:05 - Does this make sense?\n20:25 - How the Long-Term Memory is used in an attention layer\n27:40 - Entire Architecture Recap\n29:30 - Sticky Memories by Importance Sampling\n31:25 - Commentary: Pros and cons of using heuristics\n32:30 - Experiments & Results\n\nPaper: https://arxiv.org/abs/2109.00301\n\nSponsor: Weights & Biases\nhttps://wandb.me/start\n\nAbstract:\nTransformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the \u221e-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \u221e-former's attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain \"sticky memories\" while keeping a fixed computation budget. Experiments on a synthetic sorting task demonstrate the ability of the \u221e-former to retain information from long sequences. We also perform experiments on language modeling, by training a model from scratch and by fine-tuning a pre-trained language model, which show benefits of unbounded long-term memories.\n\nAuthors: Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "143": "#gpt-3 #truth #conspiracy\n\nA new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people's criticisms of large language models, but is it really the correct conclusion?\n\nOUTLINE:\n0:00 - Intro\n0:30 - Twitter Paper Announcement\n4:10 - Large Language Models are to blame!\n5:50 - How was the dataset constructed?\n9:25 - The questions are adversarial\n12:30 - Are you surprised?!\n\nPaper: https://arxiv.org/abs/2109.07958\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "144": "#gpt3 #knowledge #symbolic\n\nSymbolic knowledge models are usually trained on human-generated corpora that are cumbersome and expensive to create. Such corpora consist of structured triples of symbolic knowledge. This paper takes a different approach and attempts to generate such a corpus by prompting GPT-3. Results show that clever prompting, combined with targeted small critic models trained on human ratings can outperform both human-generated data, as well as the teacher model (GPT-3) itself. The results of this paper give a general recipe for automatically building corpora for various NLP tasks by extracting samples from large language models.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Sponsor: Weights & Biases\n4:15 - Commonsense Knowledge Graphs\n7:50 - ATOMIC dataset\n10:00 - Generating the corpus from a model\n13:00 - Prompting GPT-3\n15:30 - Generating Events\n18:40 - Generating Inferences\n23:00 - Evaluating the created dataset\n26:45 - Introducing the critic\n31:25 - Using the critic to filter the data\n36:30 - Training a student on the generated data\n41:00 - Key Findings\n44:45 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2110.07178\nCode & Corpus: https://github.com/peterwestai2/symbolic-knowledge-distillation\n\nSponsor: Weights & Biases\nhttps://wandb.com\nhttps://community.wandb.ai/\n\nAbstract:\nThe common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.\n\nAuthors: Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "145": "#scalingtransformers #terraformer #sparsity\n\nTransformers keep pushing the state of the art in language and other domains, mainly due to their ability to scale to ever more parameters. However, this scaling has made it prohibitively expensive to run a lot of inference requests against a Transformer, both in terms of compute and memory requirements. Scaling Transformers are a new kind of architecture that leverage sparsity in the Transformer blocks to massively speed up inference, and by including additional ideas from other architectures, they create the Terraformer, which is both fast, accurate, and consumes very little memory.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Recap: Transformer stack\n6:55 - Sparse Feedforward layer\n19:20 - Sparse QKV Layer\n43:55 - Terraformer architecture\n55:05 - Experimental Results & Conclusion\n\nPaper: https://arxiv.org/abs/2111.12763\nCode: https://github.com/google/trax/blob/master/trax/examples/Terraformer_from_scratch.ipynb\n\nAbstract:\nLarge Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.\n\nAuthors: Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, \u0141ukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, Jonni Kanerva\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "146": "#eleuther #gptneo #gptj\n\nEleutherAI announces GPT-NeoX-20B, a 20 billion parameter open-source language model, inspired by GPT-3. Connor joins me to discuss the process of training, how the group got their hands on the necessary hardware, what the new model can do, and how anyone can try it out!\n\nOUTLINE:\n0:00 - Intro\n1:00 - Start of interview\n2:00 - How did you get all the hardware?\n3:50 - What's the scale of this model?\n6:00 - A look into the experimental results\n11:15 - Why are there GPT-Neo, GPT-J, and GPT-NeoX?\n14:15 - How difficult is training these big models?\n17:00 - Try out the model on GooseAI\n19:00 - Final thoughts\n\nRead the announcement: https://blog.eleuther.ai/announcing-20b/\nTry out the model: https://goose.ai/\nCheck out EleutherAI: https://www.eleuther.ai/\nRead the code: https://github.com/EleutherAI/gpt-neox\nHardware sponsor: https://www.coreweave.com/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "147": "#gpt3 #embodied #planning\n\nIn this video: Paper explanation, followed by first author interview with Wenlong Huang.\nLarge language models contain extraordinary amounts of world knowledge that can be queried in various ways. But their output format is largely uncontrollable. This paper investigates the VirtualHome environment, which expects a particular set of actions, objects, and verbs to be used. Turns out, with proper techniques and only using pre-trained models (no fine-tuning), one can translate unstructured language model outputs into the structured grammar of the environment. This is potentially very useful anywhere where the models' world knowledge needs to be provided in a particular structured format.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:45 - The VirtualHome environment\n6:25 - The problem of plan evaluation\n8:40 - Contributions of this paper\n16:40 - Start of interview\n24:00 - How to use language models with environments?\n34:00 - What does model size matter?\n40:00 - How to fix the large models' outputs?\n55:00 - Possible improvements to the translation procedure\n59:00 - Why does Codex perform so well?\n1:02:15 - Diving into experimental results\n1:14:15 - Future outlook\n\nPaper: https://arxiv.org/abs/2201.07207\nWebsite: https://wenlong.page/language-planner/\nCode: https://github.com/huangwl18/language-planner\nWenlong's Twitter: https://twitter.com/wenlong_huang\n\nAbstract:\nCan world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \"make breakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into low-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at this https URL\n\nAuthors: Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "148": "#cm3 #languagemodel #transformer\n\nThis video contains a paper explanation and an incredibly informative interview with first author Armen Aghajanyan.\nAutoregressive Transformers have come to dominate many fields in Machine Learning, from text generation to image creation and many more. However, there are two problems. First, the collected data is usually scraped from the web and uni- or bi-modal and throws away a lot of structure of the original websites, and second, language modelling losses are uni-directional. CM3 addresses both problems: It directly operates on HTML and includes text, hyperlinks, and even images (via VQGAN tokenization) and can therefore be used in plenty of ways: Text generation, captioning, image creation, entity linking, and much more. It also introduces a new training strategy called Causally Masked Language Modelling, which brings a level of bi-directionality into autoregressive language modelling. In the interview after the paper explanation, Armen and I go deep into the how and why of these giant models, we go over the stunning results and we make sense of what they mean for the future of universal models.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:30 - Directly learning the structure of HTML\n12:30 - Causally Masked Language Modelling\n18:50 - A short look at how to use this model\n23:20 - Start of interview\n25:30 - Feeding language models with HTML\n29:45 - How to get bi-directionality into decoder-only Transformers?\n37:00 - Images are just tokens\n41:15 - How does one train such giant models?\n45:40 - CM3 results are amazing\n58:20 - Large-scale dataset collection and content filtering\n1:04:40 - More experimental results\n1:12:15 - Why don't we use raw HTML?\n1:18:20 - Does this paper contain too many things?\n\nPaper: https://arxiv.org/abs/2201.07520\n\nAbstract:\nWe introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.\n\nAuthors: Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "149": "#ai #alphacode #deepmind\n\nAlphaCode is an automated system that can solve competitive programing exercises. The authors found an interesting combination of language models, large-scale sampling, and clever techniques to filter and subsequently cluster the resulting programs, which lets the system perform on the level of an average competitor in real competitions. In this video, we take a deep dive into AlphaCode's design, architecture, and experimental evaluation. The paper is very well structured and the empirical results are super interesting!\n\nOUTLINE:\n0:00 - Intro\n2:10 - Paper Overview\n3:30 - An example problem from competitive programming\n8:00 - AlphaCode system overview\n14:00 - Filtering out wrong solutions\n17:15 - Clustering equivalent generated programs\n21:50 - Model configurations & engineering choices\n24:30 - Adding privileged information to the input & more tricks\n28:15 - Experimental Results (very interesting!)\n\nPaper: https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\nCode: https://github.com/deepmind/code_contests\n\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. Evaluated on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in programming competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\n\nAuthors: Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu and Oriol Vinyals\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "150": "#deeplearning #nlp #sampling\n\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\n\nSponsor: Fully Connected by Weights & Biases\nhttps://wandb.ai/fully-connected\n\nOUTLINE:\n0:00 - Intro\n1:50 - Sponsor: Fully Connected by Weights & Biases\n4:10 - Paper Overview\n7:40 - What's the problem with sampling?\n11:45 - Beam Search: The good and the bad\n14:10 - Top-k and Nucleus Sampling\n16:20 - Why the most likely things might not be the best\n21:30 - The expected information content of the next word\n25:00 - How to trade off information and likelihood\n31:25 - Connections to information theory and psycholinguistics\n36:40 - Introducing Typical Sampling\n43:00 - Experimental Evaluation\n44:40 - My thoughts on this paper\n\nPaper: https://arxiv.org/abs/2202.00666\nCode: https://github.com/cimeister/typical-sampling/blob/3e676cfd88fa2e6a24f2bdc6f9f07fddb87827c2/src/transformers/generation_logits_process.py#L242-L272\n\nAbstract:\nDespite achieving incredibly low perplexities on myriad natural language corpora, today's language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (\u00e0 la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.\n\nAuthors: Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "151": "#deeplearning #nlp #sampling\n\nThis is an interview with first author Clara Meister.\nPaper review video here\u00e9 https://youtu.be/_EDr3ryrT_Y\n\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\n\nSponsor: Introduction to Graph Neural Networks Course\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nOUTLINE:\n0:00 - Intro\n0:35 - Sponsor: Introduction to GNNs Course (link in description)\n1:30 - Why does sampling matter?\n5:40 - What is a \"typical\" message?\n8:35 - How do humans communicate?\n10:25 - Why don't we just sample from the model's distribution?\n15:30 - What happens if we condition on the information to transmit?\n17:35 - Does typical sampling really represent human outputs?\n20:55 - What do the plots mean?\n31:00 - Diving into the experimental results\n39:15 - Are our training objectives wrong?\n41:30 - Comparing typical sampling to top-k and nucleus sampling\n44:50 - Explaining arbitrary engineering choices\n47:20 - How can people get started with this?\n\nPaper: https://arxiv.org/abs/2202.00666\nCode: https://github.com/cimeister/typical-sampling/blob/3e676cfd88fa2e6a24f2bdc6f9f07fddb87827c2/src/transformers/generation_logits_process.py#L242-L272\n\nAbstract:\nDespite achieving incredibly low perplexities on myriad natural language corpora, today's language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (\u00e0 la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.\n\nAuthors: Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "152": "#nlp #gpt3 #prompt\n\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\n\nSponsor: Introduction to Graph Neural Networks Course\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Introduction to GNNs Course (link in description)\n1:30 - Paper Overview: Improve GPT-3 after deployment via user feedback\n5:30 - Proposed memory-based architecture\n13:00 - A detailed look at the components\n15:00 - Example tasks\n24:30 - My concerns with the example setup\n26:20 - Baselines used for comparison\n29:50 - Experimental Results\n34:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2201.06009\nCode & Data: https://github.com/madaan/memprompt\n\nAbstract:\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \"What word is similar to good?\" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. All the code and data is available at this https URL.\n\nAuthors: Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "153": "#nlp #gpt3 #prompt\n\nThis is an interview with the authors of this work, Aman Madaan and Niket Tandon.\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\n\nOUTLINE:\n0:00 - Intro\n0:45 - Paper Overview\n2:00 - What was your original motivation?\n4:20 - There is an updated version of the paper!\n9:00 - Have you studied this on real-world users?\n12:10 - How does model size play into providing feedback?\n14:10 - Can this be used for personalization?\n16:30 - Discussing experimental results\n17:45 - Can this be paired with recommender systems?\n20:00 - What are obvious next steps to make the system more powerful?\n23:15 - Clarifying the baseline methods\n26:30 - Exploring cross-lingual customization\n31:00 - Where did the idea for the clarification prompt come from?\n33:05 - What did not work out during this project?\n34:45 - What did you learn about interacting with large models?\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.06009\nCode & Data: https://github.com/madaan/memprompt\n\nAbstract:\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \"What word is similar to good?\" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. All the code and data is available at this https URL.\n\nAuthors: Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "154": "#dsi #search #google\n\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model's weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\n\nSponsor: Diffgram\nhttps://diffgram.com?ref=yannic\n\nOUTLINE:\n0:00 - Intro\n0:45 - Sponsor: Diffgram\n1:35 - Paper overview\n3:15 - The search problem, classic and neural\n8:15 - Seq2seq for directly predicting document IDs\n11:05 - Differentiable search index architecture\n18:05 - Indexing\n25:15 - Retrieval and document representation\n33:25 - Training DSI\n39:15 - Experimental results\n49:25 - Comments & Conclusions\n\nPaper: https://arxiv.org/abs/2202.06991\n\nAbstract:\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "155": "#neuralsearch #interview #google\n\nThis is an interview with the authors Yi Tay and Don Metzler.\nPaper Review Video: https://youtu.be/qlB0TPBQ7YY\n\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model's weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\n\nOUTLINE:\n0:00 - Intro\n0:50 - Start of Interview\n1:30 - How did this idea start?\n4:30 - How does memorization play into this?\n5:50 - Why did you not compare to cross-encoders?\n7:50 - Instead of the ID, could one reproduce the document itself?\n10:50 - Passages vs documents\n12:00 - Where can this model be applied?\n14:25 - Can we make this work on large collections?\n19:20 - What's up with the NQ100K dataset?\n23:55 - What is going on inside these models?\n28:30 - What's the smallest scale to obtain meaningful results?\n30:15 - Investigating the document identifiers\n34:45 - What's the end goal?\n38:40 - What are the hardest problems currently?\n40:40 - Final comments & how to get started\n\nPaper: https://arxiv.org/abs/2202.06991\n\nAbstract:\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "156": "#nlp #sparsity #transformers\n\nThis video is an interview with Barret Zoph and William Fedus of Google Brain about Sparse Expert Models.\nSparse Expert models have been hugely successful at distributing parts of models, mostly Transformers, across large array of machines and use a routing function to effectively route signals between them. This means that even though these models have a huge number of parameters, the computational load for a given signal does not increase because the model is only sparsely activated. Sparse expert models, such as Switch Transformers and GLAM can scale up to trillions of parameters and bring a number of desirable properties. We discuss everything from the fundamentals, history, strengths and weaknesses, up to the current state of the art of these models.\n\nOUTLINE:\n0:00 - Intro\n0:30 - What are sparse expert models?\n4:25 - Start of Interview\n5:55 - What do you mean by sparse experts?\n8:10 - How does routing work in these models?\n12:10 - What is the history of sparse experts?\n14:45 - What does an individual expert learn?\n19:25 - When are these models appropriate?\n22:30 - How comparable are sparse to dense models?\n26:30 - How does the pathways system connect to this?\n28:45 - What improvements did GLAM make?\n31:30 - The \"designing sparse experts\" paper\n37:45 - Can experts be frozen during training?\n41:20 - Can the routing function be improved?\n47:15 - Can experts be distributed beyond data centers?\n50:20 - Are there sparse experts for other domains than NLP?\n52:15 - Are sparse and dense models in competition?\n53:35 - Where do we go from here?\n56:30 - How can people get started with this?\n\nPapers:\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (https://arxiv.org/abs/2101.03961)\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts (https://arxiv.org/abs/2112.06905)\nDesigning Effective Sparse Expert Models (https://arxiv.org/abs/2202.08906)\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "157": "#saycan #robots #ai\n\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy's ability to progress towards the goal, given by the language model, and the policy's probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nSponsor: Zeta Alpha\nhttps://zeta-alpha.com\nUse code YANNIC for 20% off!\n\nOUTLINE:\n0:00 - Introduction & Overview\n3:20 - Sponsor: Zeta Alpha\n5:00 - Using language models for action planning\n8:00 - Combining LLMs with learned atomic skills\n16:50 - The full SayCan system\n20:30 - Experimental setup and data collection\n21:25 - Some weaknesses & strengths of the system\n27:00 - Experimental results\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "158": "#saycan #robots #ai\n\nThis is an interview with the authors Brian Ichter, Karol Hausman, and Fei Xia.\nOriginal Paper Review Video: https://youtu.be/Ru23eWAQ6_E\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy's ability to progress towards the goal, given by the language model, and the policy's probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nOUTLINE:\n0:00 - Introduction & Setup\n3:40 - Acquiring atomic low-level skills\n7:45 - How does the language model come in?\n11:45 - Why are you scoring instead of generating?\n15:20 - How do you deal with ambiguity in language?\n20:00 - The whole system is modular\n22:15 - Going over the full algorithm\n23:20 - What if an action fails?\n24:30 - Debunking a marketing video :)\n27:25 - Experimental Results\n32:50 - The insane scale of data collection\n40:15 - How do you go about large-scale projects?\n43:20 - Where did things go wrong?\n45:15 - Where do we go from here?\n52:00 - What is the largest unsolved problem in this?\n53:35 - Thoughts on the Tesla Bot\n55:00 - Final thoughts\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "159": "#lamda #google #ai \n\nGoogle engineer Blake Lemoine was put on leave after releasing proprietary information: An interview with the chatbot LaMDA that he believes demonstrates that this AI is, in fact, sentient. We analyze the claims and the interview in detail and trace how a statistical machine managed to convince at least one human that it is more than just an algorithm.\n\nOUTLINE:\n0:00 - Whistleblower put on leave\n4:30 - What is a language model?\n6:40 - The prompt is the key\n10:40 - Who are we talking to exactly?\n12:50 - LaMDA analyzes stories\n15:20 - Fear, pain, and consent\n20:25 - How would we recognize sentience? When is a machine conscious?\n\nReferences:\nhttps://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917\nhttps://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489\nhttps://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/\nhttps://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine\nhttps://www.businessinsider.com/transcript-of-sentient-google-ai-chatbot-was-edited-for-readability-2022-6?inline-endstory-related-recommendations=&r=US&IR=T\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "160": "#ai #language #knowledge \n\nLarge Language Models have the ability to store vast amounts of facts about the world. But little is known, how these models actually do this. This paper aims at discovering the mechanism and location of storage and recall of factual associations in GPT models, and then proposes a mechanism for the targeted editing of such facts, in form of a simple rank-one update to a single MLP layer. This has wide implications both for how we understand such models' inner workings, and for our ability to gain greater control over such models in the future.\n\nOUTLINE:\n0:00 - Introduction\n1:40 - What are the main questions in this subfield?\n6:55 - How causal tracing reveals where facts are stored\n18:40 - Clever experiments show the importance of MLPs\n24:30 - How do MLPs store information?\n29:10 - How to edit language model knowledge with precision?\n36:45 - What does it mean to know something?\n39:00 - Experimental Evaluation & the CounterFact benchmark\n45:40 - How to obtain the required latent representations?\n51:15 - Where is the best location in the model to perform edits?\n58:00 - What do these models understand about language?\n1:02:00 - Questions for the community\n\nPaper: https://arxiv.org/abs/2202.05262\nFollow-up paper on Mass-Editing Memory in a Transformer: https://arxiv.org/abs/2210.07229\n\nAbstract:\nWe analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\n\nAuthors: Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "161": "#ai #galactica #meta\n\nGalactica is a language model trained on a curated corpus of scientific documents, such as papers, knowledge bases, reviews, and other articles. The model can be used in a generative fasion to assist scientific writing, do reference prediction, and much more, including a new approach to do step-by-step reasoning using a clever encoding of intermediate steps. This video explains the paper, but also dives into the drama that ensued once Meta released a public demo of the model.\n\nOUTLINE:\n0:00 - Introduction\n1:30 - Drama around the public demo\n16:00 - Start of paper review\n20:30 - Dataset construction and encoding\n23:30 - Encoding step-by-step reasoning using a scratchpad\n33:00 - Modelling scientific references & citations\n35:05 - Prompt Pre-Training\n37:10 - Architecture details\n38:30 - Experimental results\n49:20 - Conclusion\n\nPaper: https://galactica.org/static/paper.pdf\nWebsite: https://galactica.org/explore/\n\nAbstract:\nInformation overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.\n\nAuthors: Ross Taylor Marcin Kardas Guillem Cucurull Thomas Scialom Anthony Hartshorn Elvis Saravia Andrew Poulton Viktor Kerkez Robert Stojnic\n\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "162": "#chatgpt #ai #openai \n\nChatGPT, OpenAI's newest model is a GPT-3 variant that has been fine-tuned using Reinforcement Learning from Human Feedback, and it is taking the world by storm!\n\nSponsor: Weights & Biases\nhttps://wandb.me/yannic\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Weights & Biases\n3:20 - ChatGPT: How does it work?\n5:20 - Reinforcement Learning from Human Feedback\n7:10 - ChatGPT Origins: The GPT-3.5 Series\n8:20 - OpenAI's strategy: Iterative Refinement\n9:10 - ChatGPT's amazing capabilities\n14:10 - Internals: What we know so far\n16:10 - Building a virtual machine in ChatGPT's imagination (insane)\n20:15 - Jailbreaks: Circumventing the safety mechanisms\n29:25 - How OpenAI sees the future\n\nReferences:\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/language-model-safety-and-misuse/\nhttps://beta.openai.com/docs/model-index-for-researchers\nhttps://scale.com/blog/gpt-3-davinci-003-comparison#Conclusion\nhttps://twitter.com/johnvmcdonnell/status/1598470129121374209\nhttps://twitter.com/blennon_/status/1597374826305318912\nhttps://twitter.com/TimKietzmann/status/1598230759118376960/photo/1\nhttps://twitter.com/_lewtun/status/1598056075672027137/photo/2\nhttps://twitter.com/raphaelmilliere/status/1598469100535259136\nhttps://twitter.com/CynthiaSavard/status/1598498138658070530/photo/1\nhttps://twitter.com/tylerangert/status/1598389755997290507/photo/1\nhttps://twitter.com/amasad/status/1598042665375105024/photo/1\nhttps://twitter.com/goodside/status/1598129631609380864/photo/1\nhttps://twitter.com/moyix/status/1598081204846489600/photo/2\nhttps://twitter.com/JusticeRage/status/1598959136531546112\nhttps://twitter.com/yoavgo/status/1598594145605636097\nhttps://twitter.com/EladRichardson/status/1598333315764871174\nhttps://twitter.com/charles_irl/status/1598319027327307785/photo/4\nhttps://twitter.com/jasondebolt/status/1598243854343606273\nhttps://twitter.com/mattshumer_/status/1598185710166896641/photo/1\nhttps://twitter.com/i/web/status/1598246145171804161\nhttps://twitter.com/bleedingedgeai/status/1598378564373471232\nhttps://twitter.com/MasterScrat/status/1598830356115124224\nhttps://twitter.com/Sentdex/status/1598803009844256769\nhttps://twitter.com/harrison_ritz/status/1598828017446371329\nhttps://twitter.com/parafactual/status/1598212029479026689\nhttps://www.engraved.blog/building-a-virtual-machine-inside/\nhttps://twitter.com/317070\nhttps://twitter.com/zehavoc/status/1599193444043268096\nhttps://twitter.com/yoavgo/status/1598360581496459265\nhttps://twitter.com/yoavgo/status/1599037412411596800\nhttps://twitter.com/yoavgo/status/1599045344863879168\nhttps://twitter.com/natfriedman/status/1598477452661383168\nhttps://twitter.com/conradev/status/1598487973351362561/photo/1\nhttps://twitter.com/zswitten/status/1598100186605441024\nhttps://twitter.com/CatEmbedded/status/1599141379879600128/photo/2\nhttps://twitter.com/mattshumer_/status/1599175127148949505\nhttps://twitter.com/vaibhavk97/status/1598930958769860608/photo/1\nhttps://twitter.com/dan_abramov/status/1598800508160024588/photo/1\nhttps://twitter.com/MinqiJiang/status/1598832656422432768/photo/2\nhttps://twitter.com/zswitten/status/1598088280066920453\nhttps://twitter.com/m1guelpf/status/1598203861294252033/photo/1\nhttps://twitter.com/SilasAlberti/status/1598257908567117825/photo/1\nhttps://twitter.com/gf_256/status/1598962842861899776/photo/1\nhttps://twitter.com/zswitten/status/1598088267789787136\nhttps://twitter.com/gf_256/status/1598178469955112961/photo/1\nhttps://twitter.com/samczsun/status/1598564871653789696/photo/1\nhttps://twitter.com/haus_cole/status/1598541468058390534/photo/3\nhttps://twitter.com/tailcalled/status/1599181030065246208/photo/1\nhttps://twitter.com/pensharpiero/status/1598731292278865920\nhttps://twitter.com/sleepdensity/status/1598233414683197441\nhttps://twitter.com/goodside/status/1598253337400717313\nhttps://twitter.com/Carnage4Life/status/1598332648723976193/photo/2\nhttps://github.com/sw-yx/ai-notes/blob/main/TEXT.md#jailbreaks\nhttps://twitter.com/dannypostmaa/status/1599352584963170309/photo/4\nhttps://twitter.com/sama/status/1599112749833125888\nhttps://twitter.com/sama/status/1599114807474810884\nhttps://twitter.com/sama/status/1599461195005587456\nhttps://twitter.com/deliprao/status/1599451192215887872\nhttps://twitter.com/michlbrmly/status/1599168681711656961\nhttps://twitter.com/zoink/status/1599281052115034113\n\n\nLinks:\nhttps://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2", "163": "#openassistant #chatgpt #ai \n\nHelp us collect data for OpenAssistant, the largest and most open alternative to ChatGPT.\nhttps://open-assistant.io\n\nOUTLINE:\n0:00 - Intro\n0:30 - The Project\n2:05 - Getting to Minimum Viable Prototype\n5:30 - First Tasks\n10:00 - Leaderboard\n11:45 - Playing the Assistant\n14:40 - Tricky Facts\n16:25 - What if humans had wings?\n17:05 - Can foxes be tamed?\n23:45 - Can zebras be tamed?\n26:15 - Yo (spam)\n27:00 - More tasks\n29:10 - Entitled Emails\n34:35 - Final Words\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "164": "#ai #meta #languagemodel \n\nLLaMA is a series of large language models from 7B to 65B parameters, trained by Meta AI. They train for longer on more data and show that something like gpt-3 can be outperformed by significantly smaller models when trained like this. Meta also releases the trained models to the research community.\n\nOUTLINE:\n0:00 - Introduction & Paper Overview\n4:30 - Rant on Open-Sourcing\n8:05 - Training Data\n12:40 - Training Hyperparameters\n14:50 - Architecture Modifications\n17:10 - Optimizer\n19:40 - Efficient Implementation\n26:15 - Main Results\n38:00 - Some more completions\n40:00 - Conclusion\n\n\nPaper: https://arxiv.org/abs/2302.13971\nWebsite: https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n\nAbstract:\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\n\nAuthors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "165": "#gpt4 #chatgpt #openai \n\nReferences:\nhttps://openai.com/product/gpt-4\nhttps://openai.com/research/gpt-4\nhttps://cdn.openai.com/papers/gpt-4.pdf\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "166": "#ai #transformer #gpt4 \n\nThis paper promises to scale transformers to 1 million tokens and beyond. We take a look at the technique behind it: The Recurrent Memory Transformer, and what its strenghts and weaknesses are.\n\nOUTLINE:\n0:00 - Intro\n2:15 - Transformers on long sequences\n4:30 - Tasks considered\n8:00 - Recurrent Memory Transformer\n19:40 - Experiments on scaling and attention maps\n24:00 - Conclusion\n\nPaper: https://arxiv.org/abs/2304.11062\n\nAbstract:\nThis technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.\n\nAuthors: Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "167": "#gpt4 #ai #prompt \n\nTree-of-Thought improves prompting of large language models (LLMs) by generalizing the concept of Chain-of-Thought prompting and introduces a tree search across language model thoughts, including state evaluation and backtracking. Experiments on toy tasks show large improvements over both classic and Chain-of-Thought prompting.\n\nOUTLINE:\n0:00 - Introduction\n1:20 - From Chain-of-Thought to Tree-of-Thought\n11:10 - Formalizing the algorithm\n16:00 - Game of 24 & Creative writing\n18:30 - Crosswords\n23:30 - Is this a general problem solver?\n26:50 - Ablation studies\n28:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.10601\n\nAbstract:\nLanguage models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.\n\nAuthors: Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "168": "#gpt4 #rwkv #transformer \n\nWe take a look at RWKV, a highly scalable architecture between Transformers and RNNs.\n\nFully Connected (June 7th in SF) Promo Link: https://www.fullyconnected.com/?promo=ynnc\n\nOUTLINE:\n0:00 - Introduction\n1:50 - Fully Connected In-Person Conference in SF June 7th\n3:00 - Transformers vs RNNs\n8:00 - RWKV: Best of both worlds\n12:30 - LSTMs\n17:15 - Evolution of RWKV's Linear Attention\n30:40 - RWKV's Layer Structure\n49:15 - Time-Parallel vs Sequence Mode\n53:55 - Experimental Results & Limitations\n58:00 - Visualizations\n1:01:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.13048\nCode: https://github.com/BlinkDL/RWKV-LM\n\nAbstract:\nTransformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.\n\nAuthors: Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "169": "Abstract:\nAdversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.\n\nAuthors: Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry\n\nhttps://arxiv.org/abs/1905.02175", "170": "Current NLP models are often \"cheating\" on supervised learning tasks by exploiting correlations that arise from the particularities of the dataset. Therefore they often fail to learn the original intent of the dataset creators. This paper argues that NLP models should be evaluated on Contrast Sets, which are hand-crafted perturbations by the dataset authors that capture their intent in a meaningful way.\n\nhttps://arxiv.org/abs/2004.02709\n\nAbstract:\nStandard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities. We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.\n\nAuthors: Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, Ben Zhou\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "171": "This paper establishes a framework for looking at out-of-distribution generalization failures of modern deep learning as the models learning false shortcuts that are present in the training data. The paper characterizes why and when shortcut learning can happen and gives recommendations for how to counter its effect.\n\nhttps://arxiv.org/abs/2004.07780\n\nAbstract:\nDeep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.\n\nAuthors: Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "172": "#ai #privacy #tech\n\nThis paper demonstrates a method to extract verbatim pieces of the training data from a trained language model. Moreover, some of the extracted pieces only appear a handful of times in the dataset. This points to serious security and privacy implications for models like GPT-3. The authors discuss the risks and propose mitigation strategies.\n\nOUTLINE:\n0:00 - Intro & Overview\n9:15 - Personal Data Example\n12:30 - Eidetic Memorization & Language Models\n19:50 - Adversary's Objective & Outlier Data\n24:45 - Ethical Hedging\n26:55 - Two-Step Method Overview\n28:20 - Perplexity Baseline\n30:30 - Improvement via Perplexity Ratios\n37:25 - Weights for Patterns & Weights for Memorization\n43:40 - Analysis of Main Results\n1:00:30 - Mitigation Strategies\n1:01:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2012.07805\n\nAbstract:\nIt has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\n\nAuthors: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "173": "#adversarialexamples #dimpledmanifold #security\n\nAdversarial Examples have long been a fascinating topic for many Machine Learning researchers. How can a tiny perturbation cause the neural network to change its output by so much? While many explanations have been proposed over the years, they all appear to fall short. This paper attempts to comprehensively explain the existence of adversarial examples by proposing a view of the classification landscape, which they call the Dimpled Manifold Model, which says that any classifier will adjust its decision boundary to align with the low-dimensional data manifold, and only slightly bend around the data. This potentially explains many phenomena around adversarial examples. Warning: In this video, I disagree. Remember that I'm not an authority, but simply give my own opinions.\n\nOUTLINE:\n0:00 - Intro & Overview\n7:30 - The old mental image of Adversarial Examples\n11:25 - The new Dimpled Manifold Hypothesis\n22:55 - The Stretchy Feature Model\n29:05 - Why do DNNs create Dimpled Manifolds?\n38:30 - What can be explained with the new model?\n1:00:40 - Experimental evidence for the Dimpled Manifold Model\n1:10:25 - Is Goodfellow's claim debunked?\n1:13:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2106.10151\nMy replication code: https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64\nGoodfellow's Talk: https://youtu.be/CIfsB_EYsVI?t=4280\n\nAbstract:\nThe extreme fragility of deep neural networks when presented with tiny perturbations in their inputs was independently discovered by several research groups in 2013, but in spite of enormous effort these adversarial examples remained a baffling phenomenon with no clear explanation. In this paper we introduce a new conceptual framework (which we call the Dimpled Manifold Model) which provides a simple explanation for why adversarial examples exist, why their perturbations have such tiny norms, why these perturbations look like random noise, and why a network which was adversarially trained with incorrectly labeled images can still correctly classify test images. In the last part of the paper we describe the results of numerous experiments which strongly support this new model, and in particular our assertion that adversarial perturbations are roughly perpendicular to the low dimensional manifold which contains all the training examples.\n\nAbstract: Adi Shamir, Odelia Melamed, Oriel BenShmuel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "174": "Comments on the ICML2019 tutorial on population-based search and open-ended learning.\n\nTalk: https://www.facebook.com/icml.imls/videos/481758745967365/\nSlides: http://www.cs.uwyo.edu/~jeffclune/share/2019_06_10_ICML_Tutorial.pdf\nBook: https://www.amazon.com/dp/B00X57B4JG/\nEvent: https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4336", "175": "Being interviewed by Connor Shorten of Henry AI Labs (https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw) on the topic of population-based methods and open-ended learning.\n\nTutorial: https://www.facebook.com/icml.imls/videos/481758745967365/\nBook: https://www.amazon.com/dp/B00X57B4JG/", "176": "It turns out that the classic view of generalization and overfitting is incomplete! If you add parameters beyond the number of points in your dataset, generalization performance might increase again due to the increased smoothness of overparameterized functions.\n\nAbstract:\nThe question of generalization in machine learning---how algorithms are able to learn predictors from a training sample to make accurate predictions out-of-sample---is revisited in light of the recent breakthroughs in modern machine learning technology. \nThe classical approach to understanding generalization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the fit on the training sample reflects performance out-of-sample. \nHowever, it is now common practice to fit highly complex models like deep neural networks to data with (nearly) zero training error, and yet these interpolating predictors are observed to have good out-of-sample accuracy even for noisy data. \nHow can the classical understanding of generalization be reconciled with these observations from modern machine learning practice? \nIn this paper, we bridge the two regimes by exhibiting a new \"double descent\" risk curve that extends the traditional U-shaped bias-variance curve beyond the point of interpolation. \nSpecifically, the curve shows that as soon as the model complexity is high enough to achieve interpolation on the training sample---a point that we call the \"interpolation threshold\"---the risk of suitably chosen interpolating predictors from these models can, in fact, be decreasing as the model complexity increases, often below the risk achieved using non-interpolating models. \nThe double descent risk curve is demonstrated for a broad range of models, including neural networks and random forests, and a mechanism for producing this behavior is posited.\n\nAuthors: Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal\n\nhttps://arxiv.org/abs/1812.11118", "177": "What if you could reduce the time your network trains by only training on the hard examples? This paper proposes to select samples with high loss and only train on those in order to speed up training.\n\nAbstract:\nThis paper introduces Selective-Backprop, a technique that accelerates the training of deep neural networks (DNNs) by prioritizing examples with high loss at each iteration. Selective-Backprop uses the output of a training example's forward pass to decide whether to use that example to compute gradients and update parameters, or to skip immediately to the next example. By reducing the number of computationally-expensive backpropagation steps performed, Selective-Backprop accelerates training. Evaluation on CIFAR10, CIFAR100, and SVHN, across a variety of modern image models, shows that Selective-Backprop converges to target error rates up to 3.5x faster than with standard SGD and between 1.02--1.8x faster than a state-of-the-art importance sampling approach. Further acceleration of 26% can be achieved by using stale forward pass results for selection, thus also skipping forward passes of low priority examples.\n\nAuthors: Angela H. Jiang, Daniel L.-K. Wong, Giulio Zhou, David G. Andersen, Jeffrey Dean, Gregory R. Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C. Lipton, Padmanabhan Pillai\n\nhttps://arxiv.org/abs/1910.00762", "178": "This paper presents a new benchmark for Visual Task Adaptation (i.e. BERT for images) and investigates several baseline methods for doing so.\n\nAbstract:\nRepresentation learning promises to unlock deep learning for the long tail of vision tasks without expansive labelled datasets. Yet, the absence of a unified yardstick to evaluate general visual representations hinders progress. Many sub-fields promise representations, but each has different evaluation protocols that are either too constrained (linear classification), limited in scope (ImageNet, CIFAR, Pascal-VOC), or only loosely related to representation quality (generation). We present the Visual Task Adaptation Benchmark (VTAB): a diverse, realistic, and challenging benchmark to evaluate representations. VTAB embodies one principle: good representations adapt to unseen tasks with few examples. We run a large VTAB study of popular algorithms, answering questions like: How effective are ImageNet representation on non-standard datasets? Are generative models competitive? Is self-supervision useful if one already has labels?\n\nAuthors: Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, Neil Houlsby\n\nhttps://arxiv.org/abs/1910.04867\nhttps://github.com/google-research/task_adaptation", "179": "Stunning evidence for the hypothesis that neural networks work so well because their random initialization almost certainly contains a nearly optimal sub-network that is responsible for most of the final performance.\n\nhttps://arxiv.org/abs/1803.03635\n\nAbstract:\nNeural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.\n\nAuthors: Jonathan Frankle, Michael Carbin\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "180": "FixMatch is a simple, yet surprisingly effective approach to semi-supervised learning. It combines two previous methods in a clever way and achieves state-of-the-art in regimes with few and very few labeled examples.\n\nPaper: https://arxiv.org/abs/2001.07685\nCode: https://github.com/google-research/fixmatch\n\nAbstract:\nSemi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at this https URL.\n\nAuthors: Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "181": "Multi-Task Learning can be very challenging when gradients of different tasks are of severely different magnitudes or point into conflicting directions. PCGrad eliminates this problem by projecting conflicting gradients while still retaining optimality guarantees.\n\nhttps://arxiv.org/abs/2001.06782\n\nAbstract:\nWhile deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.\n\nAuthors: Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "182": "A closer look at the OpenAI microscope, a database of visualizations of the inner workings of ImageNet classifiers, along with an explanation of how to obtain these visualizations.\n\nhttps://distill.pub/2017/feature-visualization/\nhttps://microscope.openai.com/models\nhttps://github.com/tensorflow/lucid\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "183": "This paper establishes a framework for looking at out-of-distribution generalization failures of modern deep learning as the models learning false shortcuts that are present in the training data. The paper characterizes why and when shortcut learning can happen and gives recommendations for how to counter its effect.\n\nhttps://arxiv.org/abs/2004.07780\n\nAbstract:\nDeep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.\n\nAuthors: Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "184": "Geoffrey Hinton and his co-authors describe a biologically plausible variant of backpropagation and report evidence that such an algorithm might be responsible for learning in the brain.\n\nhttps://www.nature.com/articles/s41583-020-0277-3\n\nAbstract:\nDuring learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.\n\nAuthors: Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman & Geoffrey Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "185": "The cross-entropy loss has been the default in deep learning for the last few years for supervised learning. This paper proposes a new loss, the supervised contrastive loss, and uses it to pre-train the network in a supervised fashion. The resulting model, when fine-tuned to ImageNet, achieves new state-of-the-art.\n\nhttps://arxiv.org/abs/2004.11362\n\nAbstract:\nCross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.\n\nAuthors: Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "186": "Has the world overfitted to ImageNet? What if we collect another dataset in exactly the same fashion? This paper gives a surprising answer!\n\nPaper: https://arxiv.org/abs/1902.10811\nData: https://github.com/modestyachts/ImageNetV2\n\nAbstract:\nWe build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets.\n\nAuthors: Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "187": "This paper dives into the intrinsics of the Lottery Ticket Hypothesis and attempts to shine some light on what's important and what isn't.\n\nhttps://arxiv.org/abs/1905.01067\n\nAbstract:\nThe recent \"Lottery Ticket Hypothesis\" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).\n\nAuthors: Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "188": "One CNN to rule them all! BiT is a pre-trained ResNet that can be used as a starting point for any visual task. This paper explains what it takes to pre-train such a large model and details how fine-tuning on downstream tasks is done best.\n\nPaper: https://arxiv.org/abs/1912.11370\nCode & Models: TBA\n\nAbstract:\nTransfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.\n\nAuthors: Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "189": "This is a hard paper! Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities.\n\nPaper: https://arxiv.org/abs/1811.02486\nBlog: https://openai.com/blog/learning-concepts-with-energy-functions/\nVideos: https://sites.google.com/site/energyconceptmodels/\n\nAbstract:\nMany hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at this http URL\n\nAuthors: Igor Mordatch\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "190": "CPUs are often bottlenecks in Machine Learning pipelines. Data fetching, loading, preprocessing and augmentation can be slow to a point where the GPUs are mostly idle. Data Echoing is a technique to re-use data that is already in the pipeline to reclaim this idle time and keep the GPUs busy at all times.\n\nhttps://arxiv.org/abs/1907.05550\n\nAbstract:\nIn the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce \"data echoing,\" which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or \"echoes\") intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.\n\nAuthors: Dami Choi, Alexandre Passos, Christopher J. Shallue, George E. Dahl\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "191": "Does self-supervision really need a lot of data? How low can you go? This paper shows that a single image is enough to learn the lower layers of a deep neural network. Interestingly, more data does not appear to help as long as enough data augmentation is applied.\n\nOUTLINE:\n0:00 - Overview\n1:40 - What is self-supervision\n4:20 - What does this paper do\n7:00 - Linear probes\n11:15 - Linear probe results\n17:10 - Results\n22:25 - Learned Features\n\nhttps://arxiv.org/abs/1904.13132\n\nAbstract:\nWe look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.\n\nAuthors: Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi\n\nThumbnail Image: https://commons.wikimedia.org/wiki/File:Golden_Gate_Bridge_during_blue_hour_(16_x_10).jpg\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "192": "Gradient-based Meta-Learning requires full backpropagation through the inner optimization procedure, which is a computational nightmare. This paper is able to circumvent this and implicitly compute meta-gradients by the clever introduction of a quadratic regularizer.\n\nOUTLINE:\n0:00 - Intro\n0:15 - What is Meta-Learning?\n9:05 - MAML vs iMAML\n16:35 - Problem Formulation\n19:15 - Proximal Regularization\n26:10 - Derivation of the Implicit Gradient\n40:55 - Intuition why this works\n43:20 - Full Algorithm\n47:40 - Experiments\n\nPaper: https://arxiv.org/abs/1909.04630\nBlog Post: https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/\n\nAbstract:\nA core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.\n\nAuthors: Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "193": "Neural Networks often draw hard boundaries in high-dimensional space, which makes them very brittle. Mixup is a technique that linearly interpolates between data and labels at training time and achieves much smoother and more regular class boundaries.\n\nOUTLINE:\n0:00 - Intro\n0:30 - The problem with ERM\n2:50 - Mixup\n6:40 - Code\n9:35 - Results\n\nhttps://arxiv.org/abs/1710.09412\n\nAbstract:\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\n\nAuthors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "194": "How does one measure the Intelligence of an AI? Is AlphaGo intelligent? How about GPT-3? In this landmark paper, Chollet proposes a solid measure of intelligence for AI that revolves around generalization, rather than skill.\n\nOUTLINE:\n0:00 - Intro\n1:15 - The need for a measure of intelligence\n3:35 - Intelligence as generalization ability\n5:45 - Nature vs nurture\n11:45 - Skill-based evaluation\n18:30 - Generalization based evaluation\n30:25 - Inspiration from psychometrics\n36:30 - Conclusion\n\nhttps://arxiv.org/abs/1911.01547\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nThumbnail: Photo by mohamed hassan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "195": "In this part, we go much more in-depth into the relationship between intelligence, generality, skill, experience, and prior knowledge and take a close look at what priors are built into humans. This will form the basis for comparing the intelligence of humans and AI systems.\n\nOUTLINE:\n0:00 - Intro & Recap\n3:00 - Optimize for Generality\n5:45 - Buying Skill with Data and Priors\n12:40 - The Human Scope\n17:30 - Human Priors\n24:05 - Core Knowledge\n28:50 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/1911.01547\nTim Scarfe's Video: https://youtu.be/GpWLZUbPhr0\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "196": "In this part, we look at the ARC challenge as a proposed test of machine intelligence. The dataset features 1000 tasks that test rapid generalization based on human core knowledge priors, such as object-ness, symmetry, and navigation.\n\nOUTLINE:\n0:00 - Intro\n0:55 - What is ARC?\n6:30 - The Goals of ARC\n10:40 - Assumed Priors & Examples\n21:50 - An Imagined Solution\n28:15 - Consequences of a Solution\n31:00 - Weaknesses\n31:25 - My Comments & Ideas\n\nPaper: https://arxiv.org/abs/1911.01547\nARC: https://github.com/fchollet/ARC\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "197": "In this part, we go over the formal definition of the measure of intelligence. In order to do this, we have to frame and quantify the notions of generalization difficulty, priors, and experience in terms of algorithmic complexity.\n\nOUTLINE:\n0:00 - Intro & Recap\n2:50 - Concept Schema\n10:00 - Algorithmic Complexity\n13:00 - Definitions\n15:25 - Generalization Difficulty\n18:55 - Developer Aware Generalization Difficulty\n22:40 - Priors\n25:10 - Experience\n30:50 - The Measure Of Intelligence\n38:00 - An Ideal Intelligence Benchmark\n42:30 - Conclusion\n\nPaper: https://arxiv.org/abs/1911.01547\n\nPart 1: https://youtu.be/3_qGrmD6iQY\nPart 2: https://youtu.be/THcuTJbeD34\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "198": "Backpropagation is one of the central components of modern deep learning. However, it's not biologically plausible, which limits the applicability of deep learning to understand how the human brain works. Direct Feedback Alignment is a biologically plausible alternative and this paper shows that, contrary to previous research, it can be successfully applied to modern deep architectures and solve challenging tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Problem with Backpropagation\n10:25 - Direct Feedback Alignment\n21:00 - My Intuition why DFA works\n31:20 - Experiments\n\nPaper: https://arxiv.org/abs/2006.12878\nCode: https://github.com/lightonai/dfa-scales-to-modern-deep-learning\nReferenced Paper by Arild N\u00f8kland: https://arxiv.org/abs/1609.01596\n\nAbstract:\nDespite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.\n\nAuthors: Julien Launay, Iacopo Poli, Fran\u00e7ois Boniface, Florent Krzakala\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "199": "The abundance of data on the internet is vast. Especially unlabeled images are plentiful and can be collected with ease. This model investigates a new method for incorporating unlabeled data into a supervised learning pipeline. First, a teacher model is trained in a supervised fashion. Then, that teacher is used to label the unlabeled data. Next, a larger student model is trained on the combination of all data and achieves better performance than the teacher by itself.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:05 - Semi-Supervised & Transfer Learning\n5:45 - Self-Training & Knowledge Distillation\n10:00 - Noisy Student Algorithm Overview\n20:20 - Noise Methods\n22:30 - Dataset Balancing\n25:20 - Results\n30:15 - Perturbation Robustness\n34:35 - Ablation Studies\n39:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1911.04252\nCode: https://github.com/google-research/noisystudent\nModels: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\n\nAbstract:\nWe present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.\nNoisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.\n\nAuthors: Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "200": "Google builds a 600 billion parameter transformer to do massively multilingual, massive machine translation. Interestingly, the larger model scale does not come from increasing depth of the transformer, but from increasing width in the feedforward layers, combined with a hard routing to parallelize computations on up to 2048 TPUs. A very detailed engineering paper!\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Main Results\n5:10 - Mixture-of-Experts\n16:00 - Difference to Scaling Classic Transformers\n18:50 - Backpropagation in Mixture-of-Experts\n20:05 - MoE Routing Algorithm in GShard\n38:20 - GShard Einsum Examples\n47:40 - Massively Multilingual Translation\n56:00 - Results\n1:11:30 - Conclusion & Comments\n\nERRATA:\nI said the computation of MoE scales linearly, but actually, it's sub(!)-linear.\n\nPaper: https://arxiv.org/abs/2006.16668\n\nAbstract:\nNeural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.\n\nAuthors:\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "201": "I share my progress of implementing a research idea from scratch. I attempt to build an ensemble model out of students of label-free self-distillation without any additional data or augmentation. Turns out, it actually works, and interestingly, the more students I employ, the better the accuracy. This leads to the hypothesis that the ensemble effect is not a process of extracting more information from labels.\n\nOUTLINE:\n0:00 - Introduction\n2:10 - Research Idea\n4:15 - Adjusting the Codebase\n25:00 - Teacher and Student Models\n52:30 - Shipping to the Server\n1:03:40 - Results\n1:14:50 - Conclusion\n\nCode: https://github.com/yk/PyTorch_CIFAR10\n\nReferences:\nMy Video on SimCLRv2: https://youtu.be/2lkUNDZld-4\nBorn-Again Neural Networks: https://arxiv.org/abs/1805.04770\nDeep Ensembles: A Loss Landscape Perspective: https://arxiv.org/abs/1912.02757\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "202": "Supermasks are binary masks of a randomly initialized neural network that result in the masked network performing well on a particular task. This paper considers the problem of (sequential) Lifelong Learning and trains one Supermask per Task, while keeping the randomly initialized base network constant. By minimizing the output entropy, the system can automatically derive the Task ID of a data point at inference time and distinguish up to 2500 tasks automatically.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Catastrophic Forgetting\n5:20 - Supermasks\n9:35 - Lifelong Learning using Supermasks\n11:15 - Inference Time Task Discrimination by Entropy\n15:05 - Mask Superpositions\n24:20 - Proof-of-Concept, Task Given at Inference\n30:15 - Binary Maximum Entropy Search\n32:00 - Task Not Given at Inference\n37:15 - Task Not Given at Training\n41:35 - Ablations\n45:05 - Superfluous Neurons\n51:10 - Task Selection by Detecting Outliers\n57:40 - Encoding Masks in Hopfield Networks\n59:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.14769\nCode: https://github.com/RAIVNLab/supsup\n\nMy Video about Lottery Tickets: https://youtu.be/ZVVnvZdUMUk\nMy Video about Supermasks: https://youtu.be/jhCInVFE2sc\n\nAbstract:\nWe present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.\n\nAuthors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "203": "I take a closer look at \"Supermasks in Superposition\" after I've already done a video on it. Specifically, I look at: 1. The intuition and theoretical justification behind the G objective, 2. Whether Supermasks and Superposition can be viewed as two distinct ideas and 3. The Paper's Broader Impact Statement.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - SupSup Recap\n4:00 - In-Depth Analysis of the G Objective\n20:30 - Superposition without Supermasks\n25:40 - Broader Impact Statement\n36:40 - Conclusion\n37:20 - Live Coding\n\nPart 1 on SupSup: https://youtu.be/3jT1qJ8ETzk\nMy Code: https://colab.research.google.com/drive/1bEcppdN6qZRpEFplIiv41ZI3vDwDjcvC?usp=sharing\nPaper: https://arxiv.org/abs/2006.14769\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "204": "#ai #research #optimization\n\nDeep Ensembles work surprisingly well for improving the generalization capabilities of deep neural networks. Surprisingly, they outperform Bayesian Networks, which are - in theory - doing the same thing. This paper investigates how Deep Ensembles are especially suited to capturing the non-convex loss landscape of neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Deep Ensembles\n4:15 - The Solution Space of Deep Networks\n7:30 - Bayesian Models\n9:00 - The Ensemble Effect\n10:25 - Experiment Setup\n11:30 - Solution Equality While Training\n19:40 - Tracking Multiple Trajectories\n21:20 - Similarity of Independent Solutions\n24:10 - Comparison to Baselines\n30:10 - Weight Space Cross-Sections\n35:55 - Diversity vs Accuracy\n41:00 - Comparing Ensembling Methods\n44:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1912.02757\n\nAbstract:\nDeep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.\n\nAuthors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "205": "#ai #research #machinelearning\n\nNeural Architecture Search is typically very slow and resource-intensive. A meta-controller has to train many hundreds or thousands of different models to find a suitable building plan. This paper proposes to use statistics of the Jacobian around data points to estimate the performance of proposed architectures at initialization. This method does not require training and speeds up NAS by orders of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n0:50 - Neural Architecture Search\n4:15 - Controller-based NAS\n7:35 - Architecture Search Without Training\n9:30 - Linearization Around Datapoints\n14:10 - Linearization Statistics\n19:00 - NAS-201 Benchmark\n20:15 - Experiments\n34:15 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04647\nCode: https://github.com/BayesWatch/nas-without-training\n\nAbstract:\nThe time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the NAS-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU. Code to reproduce our experiments is available at this https URL.\n\nAuthors: Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "206": "#ai #transformer #attention\n\nHopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Binary Hopfield Networks\n5:55 - Continuous Hopfield Networks\n8:15 - Update Rules & Energy Functions\n13:30 - Connection to Transformers\n14:35 - Hopfield Attention Layers\n26:45 - Theoretical Analysis\n48:10 - Investigating BERT\n1:02:30 - Immune Repertoire Classification\n\nPaper: https://arxiv.org/abs/2008.02217\nCode: https://github.com/ml-jku/hopfield-layers\nImmune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nWe show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL\n\nAuthors: Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "207": "#ai #research #privacy\n\nData is the modern gold. Neural classifiers can improve their performance by training on more data, but given a trained classifier, it's difficult to tell what data it was trained on. This is especially relevant if you have proprietary or personal data and you want to make sure that other people don't use it to train their models. This paper introduces a method to mark a dataset with a hidden \"radioactive\" tag, such that any resulting classifier will clearly exhibit this tag, which can be detected.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - How Neural Classifiers Work\n5:45 - Radioactive Marking via Adding Features\n13:55 - Random Vectors in High-Dimensional Spaces\n18:05 - Backpropagation of the Fake Features\n21:00 - Re-Aligning Feature Spaces\n25:00 - Experimental Results\n28:55 - Black-Box Test\n32:00 - Conclusion & My Thoughts\n\nPaper: https://arxiv.org/abs/2002.00937\n\nAbstract:\nWe want to detect whether a particular image dataset has been used to train a model. We propose a new technique, \\emph{radioactive data}, that makes imperceptible changes to this dataset such that any model trained on it will bear an identifiable mark. The mark is robust to strong variations such as different architectures or optimization methods. Given a trained model, our technique detects the use of radioactive data and provides a level of confidence (p-value). Our experiments on large-scale benchmarks (Imagenet), using standard architectures (Resnet-18, VGG-16, Densenet-121) and training procedures, show that we can detect usage of radioactive data with high confidence (p &lt; 10^-4) even when only 1% of the data used to trained our model is radioactive. Our method is robust to data augmentation and the stochasticity of deep network optimization. As a result, it offers a much higher signal-to-noise ratio than data poisoning and backdoor methods.\n\nAuthors: Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\u00e9 J\u00e9gou\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "208": "#ai #biology #machinelearning\n\nNeural Cellular Automata are models for how living creatures can use local message passing to reach global consensus without a central authority. This paper teaches pixels of an image to communicate with each other and figure out as a group which digit they represent. On the way, the authors have to deal with pesky side-effects that come from applying the Cross-Entropy Loss in combination with a Softmax layer, but ultimately achieve a self-sustaining, stable and continuous algorithm that models living systems.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Neural Cellular Automata\n7:30 - Global Agreement via Message-Passing\n11:05 - Neural CAs as Recurrent Convolutions\n14:30 - Training Continuously Alive Systems\n17:30 - Problems with Cross-Entropy\n26:10 - Out-of-Distribution Robustness\n27:10 - Chimeric Digits\n27:45 - Visualizing Latent State Dimensions\n29:05 - Conclusion & Comments\n\nPaper: https://distill.pub/2020/selforg/mnist/\n\nMy Video on Neural CAs: https://youtu.be/9Kec_7WFyp0\n\nAbstract:\nGrowing Neural Cellular Automata [1] demonstrated how simple cellular automata (CAs) can learn to self-organise into complex shapes while being resistant to perturbations. Such a computational model approximates a solution to an open question in biology, namely, how do cells cooperate to create a complex multicellular anatomy and work to regenerate it upon damage? The model parameterizing the cells\u2019 rules is parameter-efficient, end-to-end differentiable, and illustrates a new approach to modeling the regulation of anatomical homeostasis. In this work, we use a version of this model to show how CAs can be applied to a common task in machine learning: classification. We pose the question: can CAs use local message passing to achieve global agreement on what digit they compose?\n\nAuthors: Ettore Randazzo, Alexander Mordvintsev, Eyvind Niklasson, Michael Levin, Sam Greydanus\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "209": "#ai #research #optimization\n\nOptimization is still the domain of hand-crafted, simple algorithms. An ML engineer not only has to pick a suitable one for their problem but also often do grid-search over various hyper-parameters. This paper proposes to learn a single, unified optimization algorithm, given not by an equation, but by an LSTM-based neural network, to act as an optimizer for any deep learning problem, and ultimately to optimize itself.\n\nOUTLINE:\n0:00 - Intro & Outline\n2:20 - From Hand-Crafted to Learned Features\n4:25 - Current Optimization Algorithm\n9:40 - Learned Optimization\n15:50 - Optimizer Architecture\n22:50 - Optimizing the Optimizer using Evolution Strategies\n30:30 - Task Dataset\n34:00 - Main Results\n36:50 - Implicit Regularization in the Learned Optimizer\n41:05 - Generalization across Tasks\n41:40 - Scaling Up\n45:30 - The Learned Optimizer Trains Itself\n47:20 - Pseudocode\n49:45 - Broader Impact Statement\n52:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2009.11243\n\nAbstract:\nMuch as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.\n\nAuthors: Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, Jascha Sohl-Dickstein\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "210": "#ai #research #optimization\n\nDeep Learning famously gives rise to very complex, non-linear optimization problems that cannot be solved analytically. Therefore, the choice of a suitable optimization algorithm can often make or break the training of a Deep Neural Network. Yet, the literature is full with hundreds of different algorithms, each claiming to be superior and selecting one of them is mostly done based on popular opinion or anecdotes. This paper investigates 14 of the most popular optimizers in a standardized benchmark and even though there is no clear winner, it can give some recommendations as a result.\n\nOUTLINE:\n0:00 - Introduction & Overview\n2:15 - The Overwhelming Amount of Optimizers\n5:50 - Compared Optimizers\n6:50 - Default Parameters & Tuning Distribution\n13:10 - Deep Learning Problems Considered\n16:45 - Tuning on Single Seeds\n23:15 - Results & Interpretation\n34:00 - Learning Rate Schedules & Noise\n36:10 - Conclusions & Comments\n\nPaper: https://arxiv.org/abs/2007.01547\nRaw Results: https://github.com/SirRob1997/Crowded-Valley---Results\n\nAbstract:\nChoosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.\n\nAuthors: Robin M. Schmidt, Frank Schneider, Philipp Hennig\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "211": "#ai #research #machinelearning\n\nDeep Learning models are often overparameterized and have many degrees of freedom, which leads to many local minima that all perform equally well on the test set. But it turns out that even though they all generalize in-distribution, the performance of these models can be drastically different when tested out-of-distribution. Notably, in many cases, a good model can actually be found among all these candidates, but it seems impossible to select it. This paper describes this problem, which it calls underspecification, and gives several theoretical and practical examples.\n\nOUTLINE:\n0:00 - Into & Overview\n2:00 - Underspecification of ML Pipelines\n11:15 - Stress Tests\n12:40 - Epidemiological Example\n20:45 - Theoretical Model\n26:55 - Example from Medical Genomics\n34:00 - ImageNet-C Example\n36:50 - BERT Models\n56:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2011.03395\n\nAbstract:\nML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.\n\nAuthors: Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "212": "#ai #biology #neuroscience\n\nBackpropagation is the workhorse of modern deep learning and a core component of most frameworks, but it has long been known that it is not biologically plausible, driving a divide between neuroscience and machine learning. This paper shows that Predictive Coding, a much more biologically plausible algorithm, can approximate Backpropagation for any computation graph, which they verify experimentally by building and training CNNs and LSTMs using Predictive Coding. This suggests that the brain and deep neural networks could be much more similar than previously believed.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Backpropagation & Biology\n7:40 - Experimental Results\n8:40 - Predictive Coding\n29:00 - Pseudocode\n32:10 - Predictive Coding approximates Backprop\n35:00 - Hebbian Updates\n36:35 - Code Walkthrough\n46:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04182\nCode: https://github.com/BerenMillidge/PredictiveCodingBackprop\n\nAbstract:\nBackpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.\n\nAuthors: Beren Millidge, Alexander Tschantz, Christopher L. Buckley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "213": "#deeplearning #kernels #neuralnetworks\n\nFull Title: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine\n\nDeep Neural Networks are often said to discover useful representations of the data. However, this paper challenges this prevailing view and suggest that rather than representing the data, deep neural networks store superpositions of the training data in their weights and act as kernel machines at inference time. This is a theoretical paper with a main theorem and an understandable proof and the result leads to many interesting implications for the field.\n\nOUTLINE:\n0:00 - Intro & Outline\n4:50 - What is a Kernel Machine?\n10:25 - Kernel Machines vs Gradient Descent\n12:40 - Tangent Kernels\n22:45 - Path Kernels\n25:00 - Main Theorem\n28:50 - Proof of the Main Theorem\n39:10 - Implications & My Comments\n\nPaper: https://arxiv.org/abs/2012.00152\nStreet Talk about Kernels: https://youtu.be/y_RjsDHl5Y4\n\nERRATA: I simplify a bit too much when I pit kernel methods against gradient descent. Of course, you can even learn kernel machines using GD, they're not mutually exclusive. And it's also not true that you \"don't need a model\" in kernel machines, as it usually still contains learned parameters.\n\nAbstract:\nDeep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.\n\nAuthors: Pedro Domingos\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "214": "#openai #clip #microscope\n\nOpenAI does a huge investigation into the inner workings of their recent CLIP model via faceted feature visualization and finds amazing things: Some neurons in the last layer respond to distinct concepts across multiple modalities, meaning they fire for photographs, drawings, and signs depicting the same concept, even when the images are vastly distinct. Through manual examination, they identify and investigate neurons corresponding to persons, geographical regions, religions, emotions, and much more. In this video, I go through the publication and then I present my own findings from digging around in the OpenAI Microscope.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:35 - OpenAI Microscope\n7:10 - Categories of found neurons\n11:10 - Person Neurons\n13:00 - Donald Trump Neuron\n17:15 - Emotion Neurons\n22:45 - Region Neurons\n26:40 - Sparse Mixture of Emotions\n28:05 - Emotion Atlas\n29:45 - Adversarial Typographic Attacks\n31:55 - Stroop Test\n33:10 - My Findings in OpenAI Microscope\n33:30 - Superman Neuron\n33:50 - Resting B*tchface Neuron\n34:10 - Trash Bag Neuron\n35:25 - God Weightlifting Neuron\n36:40 - Organ Neuron\n38:35 - Film Spool Neuron\n39:05 - Feather Neuron\n39:20 - Spartan Neuron\n40:25 - Letter E Neuron\n40:35 - Cleanin Neuron\n40:45 - Frown Neuron\n40:55 - Lion Neuron\n41:05 - Fashion Model Neuron\n41:20 - Baseball Neuron\n41:50 - Bride Neuron\n42:00 - Navy Neuron\n42:30 - Hemp Neuron\n43:25 - Staircase Neuron\n43:45 - Disney Neuron\n44:15 - Hillary Clinton Neuron\n44:50 - God Neuron\n45:15 - Blurry Neuron\n45:35 - Arrow Neuron\n45:55 - Trophy Presentation Neuron\n46:10 - Receding Hairline Neuron\n46:30 - Traffic Neuron\n46:40 - Raised Hand Neuron\n46:50 - Google Maps Neuron\n47:15 - Nervous Smile Neuron\n47:30 - Elvis Neuron\n47:55 - The Flash Neuron\n48:05 - Beard Neuron\n48:15 - Kilt Neuron\n48:25 - Rainy Neuron\n48:35 - Electricity Neuron\n48:50 - Droplets Neuron\n49:00 - Escape Neuron\n49:25 - King Neuron\n49:35 - Country Neuron\n49:45 - Overweight Men Neuron\n49:55 - Wedding\n50:05 - Australia Neuron\n50:15 - Yawn Neuron\n50:30 - Bees & Simpsons Neuron\n50:40 - Mussles Neuron\n50:50 - Spice Neuron\n51:00 - Conclusion\n\nPaper: https://distill.pub/2021/multimodal-neurons/\nMy Findings: https://www.notion.so/CLIP-OpenAI-Microscope-Findings-27465eac373c451d8083428443e0837c\nMy Video on CLIP: https://youtu.be/T9XSU0pKX2E\nMy Video on Feature Visualizations & The OpenAI Microscope: https://youtu.be/Ok44otx90D4\n\nAbstract:\nIn 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry. The exciting thing wasn\u2019t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person\u2019s name. The neurons were multimodal. As the lead author would put it: \"You are looking at the far end of the transformation from metric, visual shapes to conceptual... information.\" We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. Like the biological multimodal neurons, these artificial neurons respond to the same subject in photographs, drawings, and images of their name.\n\nAuthors: Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "215": "#universalcomputation #pretrainedtransformers #finetuning\n\nLarge-scale pre-training and subsequent fine-tuning is a common recipe for success with transformer models in machine learning. However, most such transfer learning is done when a model is pre-trained on the same or a very similar modality to the final task to be solved. This paper demonstrates that transformers can be fine-tuned to completely different modalities, such as from language to vision. Moreover, they demonstrate that this can be done by freezing all attention layers, tuning less than .1% of all parameters. The paper further claims that language modeling is a superior pre-training task for such cross-domain transfer. The paper goes through various ablation studies to make its point.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - Frozen Pretrained Transformers\n4:50 - Evaluated Tasks\n10:05 - The Importance of Training LayerNorm\n17:10 - Modality Transfer\n25:10 - Network Architecture Ablation\n26:10 - Evaluation of the Attention Mask\n27:20 - Are FPTs Overfitting or Underfitting?\n28:20 - Model Size Ablation\n28:50 - Is Initialization All You Need?\n31:40 - Full Model Training Overfits\n32:15 - Again the Importance of Training LayerNorm\n33:10 - Conclusions & Comments\n\nPaper: https://arxiv.org/abs/2103.05247\nCode: https://github.com/kzl/universal-computation\n\nAbstract:\nWe investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.\n\nAuthors: Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "216": "#implicitfunction #jax #autodiff\n\nMany problems in Machine Learning involve loops of inner and outer optimization. Finding update steps for the outer loop is usually difficult, because of the.need to differentiate through the inner loop's procedure over multiple steps. Such loop unrolling is very limited and constrained to very few steps. Other papers have found solutions around unrolling in very specific, individual problems. This paper proposes a unified framework for implicit differentiation of inner optimization procedures without unrolling and provides implementations that integrate seamlessly into JAX.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Automatic Differentiation of Inner Optimizations\n4:30 - Example: Meta-Learning\n7:45 - Unrolling Optimization\n13:00 - Unified Framework Overview & Pseudocode\n21:10 - Implicit Function Theorem\n25:45 - More Technicalities\n28:45 - Experiments\n\nERRATA:\n- Dataset Distillation is done with respect to the training set, not the validation or test set.\n\nPaper: https://arxiv.org/abs/2105.15183\nCode coming soon\n\nAbstract:\nAutomatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.\n\nAuthors: Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-L\u00f3pez, Fabian Pedregosa, Jean-Philippe Vert\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "217": "#pondernet #deepmind #machinelearning\n\nHumans don't spend the same amount of mental effort on all problems equally. Instead, we respond quickly to easy tasks, and we take our time to deliberate hard tasks. DeepMind's PonderNet attempts to achieve the same by dynamically deciding how many computation steps to allocate to any single input sample. This is done via a recurrent architecture and a trainable function that computes a halting probability. The resulting model performs well in dynamic computation tasks and is surprisingly robust to different hyperparameter settings.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Problem Statement\n8:00 - Probabilistic formulation of dynamic halting\n14:40 - Training via unrolling\n22:30 - Loss function and regularization of the halting distribution\n27:35 - Experimental Results\n37:10 - Sensitivity to hyperparameter choice\n41:15 - Discussion, Conclusion, Broader Impact\n\nPaper: https://arxiv.org/abs/2107.05407\n\nAbstract:\nIn standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1\n\nAuthors: Andrea Banino, Jan Balaguer, Charles Blundell\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "218": "#grokking #openai #deeplearning\n\nGrokking is a phenomenon when a neural network suddenly learns a pattern in the dataset and jumps from random chance generalization to perfect generalization very suddenly. This paper demonstrates grokking on small algorithmic datasets where a network has to fill in binary tables. Interestingly, the learned latent spaces show an emergence of the underlying binary operations that the data were created with.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Grokking Phenomenon\n3:50 - Related: Double Descent\n7:50 - Binary Operations Datasets\n11:45 - What quantities influence grokking?\n15:40 - Learned Emerging Structure\n17:35 - The role of smoothness\n21:30 - Simple explanations win\n24:30 - Why does weight decay encourage simplicity?\n26:40 - Appendix\n28:55 - Conclusion & Comments\n\nPaper: https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf\n\nAbstract:\nIn this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \u201cgrokking\u201d a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.\n\nAuthors: Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin & Vedant Misra\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "219": "#deeplearning #backpropagation #simulation\n\nMore and more systems are made differentiable, which means that accurate gradients of these systems' dynamics can be computed exactly. While this development has led to a lot of advances, there are also distinct situations where backpropagation can be a very bad idea. This paper characterizes a few such systems in the domain of iterated dynamical systems, often including some source of stochasticity, resulting in chaotic behavior. In these systems, it is often better to use black-box estimators for gradients than computing them exactly.\n\nOUTLINE:\n0:00 - Foreword\n1:15 - Intro & Overview\n3:40 - Backpropagation through iterated systems\n12:10 - Connection to the spectrum of the Jacobian\n15:35 - The Reparameterization Trick\n21:30 - Problems of reparameterization\n26:35 - Example 1: Policy Learning in Simulation\n33:05 - Example 2: Meta-Learning Optimizers\n36:15 - Example 3: Disk packing\n37:45 - Analysis of Jacobians\n40:20 - What can be done?\n45:40 - Just use Black-Box methods\n\nPaper: https://arxiv.org/abs/2111.05803\n\nAbstract:\nDifferentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.\n\nAuthors: Luke Metz, C. Daniel Freeman, Samuel S. Schoenholz, Tal Kachman\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "220": "#grafting #adam #sgd\n\nThe last years in deep learning research have given rise to a plethora of different optimization algorithms, such as SGD, AdaGrad, Adam, LARS, LAMB, etc. which all claim to have their special peculiarities and advantages. In general, all algorithms modify two major things: The (implicit) learning rate schedule, and a correction to the gradient direction. This paper introduces grafting, which allows to transfer the induced learning rate schedule of one optimizer to another one. In that, the paper shows that much of the benefits of adaptive methods (e.g. Adam) are actually due to this schedule, and not necessarily to the gradient direction correction. Grafting allows for more fundamental research into differences and commonalities between optimizers, and a derived version of it makes it possible to computes static learning rate corrections for SGD, which potentially allows for large savings of GPU memory.\n\nOUTLINE\n0:00 - Rant about Reviewer #2\n6:25 - Intro & Overview\n12:25 - Adaptive Optimization Methods\n20:15 - Grafting Algorithm\n26:45 - Experimental Results\n31:35 - Static Transfer of Learning Rate Ratios\n35:25 - Conclusion & Discussion\n\nPaper (OpenReview): https://openreview.net/forum?id=FpKgG31Z_i9\nOld Paper (Arxiv): https://arxiv.org/abs/2002.11803\n\nOur Discord: https://discord.gg/4H8xxDF\n\nAbstract:\nIn the empirical science of training large neural networks, the learning rate schedule is a notoriously challenging-to-tune hyperparameter, which can depend on all other properties (architecture, optimizer, batch size, dataset, regularization, ...) of the problem. In this work, we probe the entanglements between the optimizer and the learning rate schedule. We propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Using grafting, we discover a non-adaptive learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance. Besides providing a resource-saving tool for practitioners, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning.\n\nAuthors: Anonymous (Under Review)\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "221": "#imle #backpropagation #discrete\n\nBackpropagation is the workhorse of deep learning, but unfortunately, it only works for continuous functions that are amenable to the chain rule of differentiation. Since discrete algorithms have no continuous derivative, deep networks with such algorithms as part of them cannot be effectively trained using backpropagation. This paper presents a method to incorporate a large class of algorithms, formulated as discrete exponential family distributions, into deep networks and derives gradient estimates that can easily be used in end-to-end backpropagation. This enables things like combinatorial optimizers to be part of a network's forward propagation natively.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:25 - Sponsor: Weights & Biases\n6:15 - Problem Setup & Contributions\n8:50 - Recap: Straight-Through Estimator\n13:25 - Encoding the discrete problem as an inner product\n19:45 - From algorithm to distribution\n23:15 - Substituting the gradient\n26:50 - Defining a target distribution\n38:30 - Approximating marginals via perturb-and-MAP\n45:10 - Entire algorithm recap\n56:45 - Github Page & Example\n\nPaper: https://arxiv.org/abs/2106.01798\nCode (TF): https://github.com/nec-research/tf-imle\nCode (Torch): https://github.com/uclnlp/torch-imle\n\nOur Discord: https://discord.gg/4H8xxDF\n\nSponsor: Weights & Biases\nhttps://wandb.com\n\nAbstract:\nCombining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.\n\nAuthors: Mathias Niepert, Pasquale Minervini, Luca Franceschi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "222": "#deeplearning #symbolic #research\n\nThis video includes an interview with first author St\u00e9phane d'Ascoli (https://sdascoli.github.io/).\nDeep neural networks are typically excellent at numeric regression, but using them for symbolic computation has largely been ignored so far. This paper uses transformers to do symbolic regression on integer and floating point number sequences, which means that given the start of a sequence of numbers, the model has to not only predict the correct continuation, but also predict the data generating formula behind the sequence. Through clever encoding of the input space and a well constructed training data generation process, this paper's model can learn and represent many of the sequences in the OEIS, the online encyclopedia of integer sequences and it also features an interactive demo if you want to try it by yourself. \n\nOUTLINE:\n0:00 - Introduction\n2:20 - Summary of the Paper\n16:10 - Start of Interview\n17:15 - Why this research direction?\n20:45 - Overview of the method\n30:10 - Embedding space of input tokens\n33:00 - Data generation process\n42:40 - Why are transformers useful here?\n46:40 - Beyond number sequences, where is this useful?\n48:45 - Success cases and failure cases\n58:10 - Experimental Results\n1:06:30 - How did you overcome difficulties?\n1:09:25 - Interactive demo\n\nPaper: https://arxiv.org/abs/2201.04600\nInteractive demo: https://symbolicregression.metademolab.com/\n\nAbstract:\nSymbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. bessel0(x)\u2248sin(x)+cos(x)\u03c0x\u221a and 1.644934\u2248\u03c02/6. An interactive demonstration of our models is provided at this https URL.\n\nAuthors: St\u00e9phane d'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, Fran\u00e7ois Charton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "223": "#vos #outliers #deeplearning\nSponsor: Assembly AI\nCheck them out here: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic1\n\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\n\nOUTLINE:\n0:00 - Intro\n2:00 - Sponsor: Assembly AI (Link below)\n4:05 - Paper Overview\n6:45 - Where do traditional classifiers fail?\n11:00 - How object detectors work\n17:00 - What are virtual outliers and how are they created?\n24:00 - Is this really an appropriate model for outliers?\n26:30 - How virtual outliers are used during training\n34:00 - Plugging it all together to detect outliers\n\nPaper: https://arxiv.org/abs/2202.01197\nCode: https://github.com/deeplearning-wisc/vos\n\nAbstract:\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\n\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "224": "#deeplearning #objectdetection #outliers\n\nAn interview with the authors of \"Virtual Outlier Synthesis\".\nWatch the paper review video here: https://youtu.be/i-J4T3uLC9M\n\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\n\nOUTLINE:\n0:00 - Intro\n2:20 - What was the motivation behind this paper?\n5:30 - Why object detection?\n11:05 - What's the connection to energy-based models?\n12:15 - Is a Gaussian mixture model appropriate for high-dimensional data?\n16:15 - What are the most important components of the method?\n18:30 - What are the downstream effects of the regularizer?\n22:00 - Are there severe trade-offs to outlier detection?\n23:55 - Main experimental takeaways?\n26:10 - Why do outlier detection in the last layer?\n30:20 - What does it take to finish a research projects successfully?\n\nPaper: https://arxiv.org/abs/2202.01197\nCode: https://github.com/deeplearning-wisc/vos\n\nAbstract:\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\n\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "225": "#multitasklearning #biology #neuralnetworks\n\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\n\nOUTLINE:\n0:00 - Introduction\n1:20 - Paper Overview\n3:15 - Catastrophic forgetting in continuous and multi-task learning\n9:30 - Dendrites in biological neurons\n16:55 - Sparse representations in biology\n18:35 - Active dendrites in deep learning\n34:15 - Experiments on multi-task learning\n39:00 - Experiments in continual learning and adaptive prototyping\n49:20 - Analyzing the inner workings of the algorithm\n53:30 - Is this the same as just training a larger network?\n59:15 - How does this relate to attention mechanisms?\n1:02:55 - Final thoughts and comments\n\nPaper: https://arxiv.org/abs/2201.00042\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\n\nERRATA:\n- I was made aware of this by https://twitter.com/ChainlessCoder: \"That axon you showed of the pyramidal neuron, is actually the apical dendrite of the neuron\". Sorry, my bad :)\n\nAbstract:\nA key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\n\nAuthors: Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "226": "#multitasklearning #biology #neuralnetworks\n\nThis is an interview with the paper's authors: Abhiram Iyer, Karan Grewal, and Akash Velu!\nPaper Review Video: https://youtu.be/O_dJ31T01i8\n\nCheck out Zak's course on Graph Neural Networks (discount with this link): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\n\nOUTLINE:\n0:00 - Intro\n0:55 - Sponsor: GNN Course\n2:30 - How did the idea come to be?\n7:05 - What roles do the different parts of the method play?\n8:50 - What was missing in the paper review?\n10:35 - Are biological concepts viable if we still have backprop?\n11:50 - How many dendrites are necessary?\n14:10 - Why is there a plateau in the sparsity plot?\n20:50 - How does task difficulty play into the algorithm?\n24:10 - Why are there different setups in the experiments?\n30:00 - Is there a place for unsupervised pre-training?\n32:50 - How can we apply the online prototyping to more difficult tasks?\n37:00 - What did not work out during the project?\n41:30 - How do you debug a project like this?\n47:10 - How is this related to other architectures?\n51:10 - What other things from neuroscience are to be included?\n55:50 - Don't miss the awesome ending :)\n\nPaper: https://arxiv.org/abs/2201.00042\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\n\nLink to the GNN course (with discount): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nAbstract:\nA key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\n\nAuthors: Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "227": "#laion #clip #dalle\n\nLAION-5B is an open, free dataset consisting of over 5 billion image-text-pairs. Today's video is an interview with three of its creators. We dive into the mechanics and challenges of operating at such large scale, how to keep cost low, what new possibilities are enabled with open datasets like this, and how to best handle safety and legal concerns.\n\nOUTLINE:\n0:00 - Intro\n1:30 - Start of Interview\n2:30 - What is LAION?\n11:10 - What are the effects of CLIP filtering?\n16:40 - How big is this dataset?\n19:05 - Does the text always come from the alt-property?\n22:45 - What does it take to work at scale?\n25:50 -When will we replicate DALL-E?\n31:30 - The surprisingly efficient pipeline\n35:20 - How do you cover the S3 costs?\n40:30 - Addressing safety & legal concerns\n55:15 - Where can people get started?\n\nReferences:\nLAION website: https://laion.ai/\nLAION Discord: https://discord.com/invite/mVcgxMPD7e\nLAION-5B: https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/\nimg2dataset tool: https://github.com/rom1504/img2dataset\nLAION-400M: https://paperswithcode.com/dataset/laion-400m\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "228": "#jepa #ai #machinelearning \n\nYann LeCun's position paper on a path towards machine intelligence combines Self-Supervised Learning, Energy-Based Models, and hierarchical predictive embedding models to arrive at a system that can teach itself to learn useful abstractions at multiple levels and use that as a world model to plan ahead in time.\n\nOUTLINE:\n0:00 - Introduction\n2:00 - Main Contributions\n5:45 - Mode 1 and Mode 2 actors\n15:40 - Self-Supervised Learning and Energy-Based Models\n20:15 - Introducing latent variables\n25:00 - The problem of collapse\n29:50 - Contrastive vs regularized methods\n36:00 - The JEPA architecture\n47:00 - Hierarchical JEPA (H-JEPA)\n53:00 - Broader relevance\n56:00 - Summary & Comments\n\nPaper: https://openreview.net/forum?id=BZ5a1r-kVsf\n\nAbstract: How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons?  This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.\n\nAuthor: Yann LeCun\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "229": "#ai #sparsity #gpu \n\nSparsity is awesome, but only recently has it become possible to properly handle sparse models at good performance. Neural Magic does exactly this, using a plain CPU. No specialized hardware needed, just clever algorithms for pruning and forward-propagation of neural networks. Nir Shavit and I talk about how this is possible, what it means in terms of applications, and why sparsity should play a much larger role in the Deep Learning community.\n\nSponsor: AssemblyAI\nLink: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic_autochapters\n\nCheck out Neural Magic: https://neuralmagic.com/\nand DeepSparse: https://github.com/neuralmagic/deepsparse\n\nOUTLINE:\n0:00 Introduction\n1:08 Sponsor: AssemblyAI\n2:50 Start of Interview\n4:15 How the NIR company was founded? \n5:10 What is Sparsity about? \n9:30 Link between the human brain and sparsity\n12:10 Where should the extra resource that the human brain doesn't have go?\n14:40 Analogy for Sparse Architecture\n16:48 Possible future for Sparse Architecture as standard architure for Neural Networks\n20:08 Pruning & Sparsification\n22:57 What keeps us from building sparse models?\n25:34 Why are GPUs so unsuited for sparse models?\n28:47 CPU and GPU in connection with memory\n30:14 What Neural Magic does?\n32:54 How do you deal with overlaps in tensor columns?\n33:41 The best type of sparsity to execute tons of CPU\n37:24 What kind of architecture would make the best use out of a combined system of CPUs and GPUs?\n41:04 Graph Neural Networks in connection to sparsity\n43:04 Intrinsic connection between the Sparsification of Neural Networks, Non Layer-Wise Computation, Blockchain Technology, Smart Contracts and Distributed Computing\n45:23 Neural Magic's target audience\n48:16 Is there a type of model where it works particularly well and the type where it doesn't?\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "230": "#alphatensor #deepmind #ai \n\nMatrix multiplication is the most used mathematical operation in all of science and engineering. Speeding this up has massive consequences. Thus, over the years, this operation has become more and more optimized. A fascinating discovery was made when it was shown that one actually needs less than N^3 multiplication operations to multiply to NxN matrices. DeepMind goes a step further and creates AlphaTensor, a Deep Reinforcement Learning algorithm that plays a single-player game, TensorGame, in order to find even more optimized algorithms for matrix multiplication. And it turns out, there exists a plethora of undiscovered matrix multiplication algorithms, which not only will make everything from computers to smart toasters faster, but also bring new insights into fundamental math and complexity theory.\n\nSponsor: Assembly AI\nLink: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic_sentiment\n\nOUTLINE:\n0:00 - Intro\n1:50 - Sponsor: Assembly AI (link in description)\n3:25 - What even is Matrix Multiplication?\n6:10 - A very astounding fact\n8:45 - Trading multiplications for additions\n12:35 - Matrix Multiplication as a Tensor\n17:30 - Tensor Decompositions\n20:30 - A formal way of finding multiplication algorithms\n31:00 - How to formulate this as a game?\n39:30 - A brief primer on AlphaZero / MCTS\n45:40 - The Results\n48:15 - Optimizing for different hardware\n52:40 - Expanding fundamental math\n53:45 - Summary & Final Comments\n\nPaper: https://www.nature.com/articles/s41586-022-05172-4\nTitle: Discovering faster matrix multiplication algorithms with reinforcement learning\n\nAbstract:\nImproving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\u2009\u00d7\u20094 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.\n\nAuthors: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis & Pushmeet Kohli\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "231": "#neuralnetworks #machinelearning #ai \n\nAlexander Mattick joins me to discuss the paper \"Neural Networks are Decision Trees\", which has generated a lot of hype on social media. We ask the question: Has this paper solved one of the large mysteries of deep learning and opened the black-box neural networks up to interpretability?\n\nOUTLINE:\n0:00 - Introduction\n2:20 - Aren't Neural Networks non-linear?\n5:20 - What does it all mean?\n8:00 - How large do these trees get?\n11:50 - Decision Trees vs Neural Networks\n17:15 - Is this paper new?\n22:20 - Experimental results\n27:30 - Can Trees and Networks work together?\n\nPaper: https://arxiv.org/abs/2210.05189\n\nAbstract:\nIn this manuscript, we show that any feedforward neural network having piece-wise linear activation functions can be represented as a decision tree. The representation is equivalence and not an approximation, thus keeping the accuracy of the neural network exactly as is. We believe that this work paves the way to tackle the black-box nature of neural networks. We share equivalent trees of some neural networks and show that besides providing interpretability, tree representation can also achieve some computational advantages. The analysis holds both for fully connected and convolutional networks, which may or may not also include skip connections and/or normalizations.\n\nAuthor: Caglar Aytekin\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "232": "https://arxiv.org/abs/1806.07366\n\nAbstract:\nWe introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.\n\nAuthors:\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud", "233": "A look at OpenAI's new GPT-2 model and the surrounding controversy.\n\nhttps://blog.openai.com/better-language-models/\n\nAbstract:\nNatural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n\nAuthors:\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever", "234": "https://arxiv.org/abs/1810.04805\n\nAbstract:\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\n\nAuthors:\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "235": "http://tensorlab.cms.caltech.edu/users/anima/pubs/NIPS_Name_Debate.pdf\n\nAbstract:\nThere has been substantial recent controversy surrounding the use of the acronym \"NIPS\" for the Neural Information Processing Systems conference, stemming from the fact that the word \"nips\" is common slang for nipples, and has historically been used as a racial slur targeting people of Japanese origin. Here, we outline the ways in which this acronym has contributed to a hostile environment towards women in machine learning. We argue that an October 2018 decision by the Neural Information Processing Systems board not to change the name of the conference was based on a misunderstanding of the issues that women face in STEM fields, a poorly-designed survey, and a faulty statistical analysis. We applaud the board for a more recent announcement of the new abbreviation \"NeurIPS\", and emphasize that this name change is an important first step towards the creation of a more inclusive environment in machine learning.\n\nAuthors:\nDaniela M. Witten, Elana J. Fertig, Animashree Anandkumar, Jeff Dean\n\nReferences:\nhttps://medium.com/@kristianlum/statistics-we-have-a-problem-304638dc5de5\nhttps://nips.cc/Conferences/2018/News\nhttps://twitter.com/AnimaAnandkumar/status/1055278000588021762\nhttps://www.change.org/p/members-of-nips-board-protestnips-nips-acronym-encourages-sexism-and-is-a-slur-change-the-name\nhttps://twitter.com/AnimaAnandkumar/status/1056971852248018944", "236": "https://arxiv.org/abs/1706.03762\n\nAbstract:\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\nAuthors:\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin", "237": "Commentary of\nhttps://arxiv.org/abs/1707.06203\n\nAbstract\nWe introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.\n\nAuthors\nTh\u00e9ophane Weber, S\u00e9bastien Racani\u00e8re, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, David Silver, Daan Wierstra", "238": "Abstract:\nAdversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.\n\nAuthors: Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry\n\nhttps://arxiv.org/abs/1905.02175", "239": "Abstract:\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.\n\nAuthors: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le\n\nhttps://arxiv.org/abs/1906.08237", "240": "Standard neural networks suffer from problems such as un-smooth classification boundaries and overconfidence. Manifold Mixup is an easy regularization technique that rectifies these problems. It works by interpolating hidden representations of different data points and then train them to predict equally interpolated labels.\n\nhttps://arxiv.org/abs/1806.05236\n\nAbstract:\nDeep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.\n\nAuthors:\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio", "241": "Geoff Hinton's next big idea! Capsule Networks are an alternative way of implementing neural networks by dividing each layer into capsules. Each capsule is responsible for detecting the presence and properties of one particular entity in the input sample. This information is then allocated dynamically to higher-level capsules in a novel and unconventional routing scheme. While Capsule Networks are still in their infancy, they are an exciting and promising new direction.\n\nAbstract:\nA capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.\n\nAuthors: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton\n\nhttps://arxiv.org/abs/1710.09829\n\n\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nMinds: https://www.minds.com/ykilcher\nBitChute: https://www.bitchute.com/channel/10a5ui845DOJ/", "242": "The wait is finally over! Antonio and I discuss the best, funniest and dankest memes of the machine learning world. Join us for a laugh!", "243": "DeepMind's new agent to tackle yet another Esport: Starcraft II. This agent uses deep reinforcement learning with a new technique, called League Training, to catapult itself to Grandmaster-level skill at playing this game.\n\nAbstract:\nMany real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.\n\nAuthors: Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver\n\nhttps://www.deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "244": "MuZero harnesses the power of AlphaZero, but without relying on an accurate environment model. This opens up planning-based reinforcement learning to entirely new domains, where such environment models aren't available. The difference to previous work is that, instead of learning a model predicting future observations, MuZero predicts the future observations' latent representations, and thus learns to only represent things that matter to the task!\n\nAbstract:\nConstructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.\n\nAuthors: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver\n\nhttps://arxiv.org/abs/1911.08265\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "245": "Schmidhuber thinking outside the box! Upside-Down RL turns RL on its head and constructs a behavior function that uses the desired reward as an input. The new paradigm shows surprising performance compared to classic RL algorithms.\n\nAbstract:\nWe transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also introduce a related simple but general approach for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.\n\nAuthor: Juergen Schmidhuber\n\nhttps://arxiv.org/abs/1912.02875\nhttps://arxiv.org/abs/1912.02877", "246": "The Transformer for the masses! Reformer solves the biggest problem with the famous Transformer model: Its huge resource requirements. By cleverly combining Locality Sensitive Hashing and ideas from Reversible Networks, the classically huge footprint of the Transformer is drastically reduced. Not only does that mean the model uses less memory, but it can process much longer input sequences, up to 16K tokens with just 16gb of memory!\n\nhttps://arxiv.org/abs/2001.04451\nhttps://ai.googleblog.com/2020/01/reformer-efficient-transformer.html\n\nAbstract:\nLarge Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(LlogL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\nAuthors: Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "247": "An in-depth look at this channel's analytics.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "248": "This is a hard paper! Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities.\n\nPaper: https://arxiv.org/abs/1811.02486\nBlog: https://openai.com/blog/learning-concepts-with-energy-functions/\nVideos: https://sites.google.com/site/energyconceptmodels/\n\nAbstract:\nMany hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at this http URL\n\nAuthors: Igor Mordatch\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "249": "Object detection in images is a notoriously hard task! Objects can be of a wide variety of classes, can be numerous or absent, they can occlude each other or be out of frame. All of this makes it even more surprising that the architecture in this paper is so simple. Thanks to a clever loss function, a single Transformer stacked on a CNN is enough to handle the entire task!\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:50 - Problem Formulation\n2:30 - Architecture Overview\n6:20 - Bipartite Match Loss Function\n15:55 - Architecture in Detail\n25:00 - Object Queries\n31:00 - Transformer Properties\n35:40 - Results\n\nERRATA:\nWhen I introduce bounding boxes, I say they consist of x and y, but you also need the width and height.\n\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\n\nPaper: https://arxiv.org/abs/2005.12872\nBlog: https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/\nCode: https://github.com/facebookresearch/detr\n\nAbstract:\nWe present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.\n\nAuthors: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "250": "I retrace my first reading of Facebook AI's DETR paper and explain my process of understanding it.\n\nOUTLINE:\n0:00 - Introduction\n1:25 - Title\n4:10 - Authors\n5:55 - Affiliation\n7:40 - Abstract\n13:50 - Pictures\n20:30 - Introduction\n22:00 - Related Work\n24:00 - Model\n30:00 - Experiments\n41:50 - Conclusions & Abstract\n42:40 - Final Remarks\n\nOriginal Video about DETR: https://youtu.be/T35ba_VXkMY\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "251": "#memes #science #ai\n\nAntonio and I critique the creme de la creme of Deep Learning memes.\n\nMusic:\nSunshower - LATASH\u00c1\nPapov - Yung Logos\nSunny Days - Anno Domini Beats\nTrinity - Jeremy Blake\n\nMore memes:\nfacebook.com/convolutionalmemes\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "252": "#openai #science #gpt3\n\nOpenAI's newest model, DALL\u00b7E, shows absolutely amazing abilities in generating high-quality images from arbitrary text descriptions. Like GPT-3, the range of applications and the diversity of outputs is astonishing, given that this is a single model, trained on a purely autoregressive task. This model is a significant step towards the combination of text and images in future AI applications.\n\nOUTLINE:\n0:00 - Introduction\n2:45 - Overview\n4:20 - Dataset\n5:35 - Comparison to GPT-3\n7:00 - Model Architecture\n13:20 - VQ-VAE\n21:00 - Combining VQ-VAE with GPT-3\n27:30 - Pre-Training with Relaxation\n32:15 - Experimental Results\n33:00 - My Hypothesis about DALL\u00b7E's inner workings\n36:15 - Sparse Attention Patterns\n38:00 - DALL\u00b7E can't count\n39:35 - DALL\u00b7E can't global order\n40:10 - DALL\u00b7E renders different views\n41:10 - DALL\u00b7E is very good at texture\n41:40 - DALL\u00b7E can complete a bust\n43:30 - DALL\u00b7E can do some reflections, but not others\n44:15 - DALL\u00b7E can do cross-sections of some objects\n45:50 - DALL\u00b7E is amazing at style\n46:30 - DALL\u00b7E can generate logos\n47:40 - DALL\u00b7E can generate bedrooms\n48:35 - DALL\u00b7E can combine unusual concepts\n49:25 - DALL\u00b7E can generate illustrations\n50:15 - DALL\u00b7E sometimes understands complicated prompts\n50:55 - DALL\u00b7E can pass part of an IQ test\n51:40 - DALL\u00b7E probably does not have geographical / temporal knowledge\n53:10 - Reranking dramatically improves quality\n53:50 - Conclusions & Comments\n\nBlog: https://openai.com/blog/dall-e/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "253": "#memes #science #ai\n\nPart 2 of Antonio and me examining the latest and greatest of deep learning memes.\n\nMusic:\nSunshower - LATASH\u00c1\nPapov - Yung Logos\nSunny Days - Anno Domini Beats\nTrinity - Jeremy Blake\n\nMore memes:\nfacebook.com/convolutionalmemes\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "254": "#glom #hinton #capsules\n\nGeoffrey Hinton describes GLOM, a Computer Vision model that combines transformers, neural fields, contrastive learning, capsule networks, denoising autoencoders and RNNs. GLOM decomposes an image into a parse tree of objects and their parts. However, unlike previous systems, the parse tree is constructed dynamically and differently for each input, without changing the underlying neural network. This is done by a multi-step consensus algorithm that runs over different levels of abstraction at each location of an image simultaneously. GLOM is just an idea for now but suggests a radically new approach to AI visual scene understanding.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Object Recognition as Parse Trees\n5:40 - Capsule Networks\n8:00 - GLOM Architecture Overview\n13:10 - Top-Down and Bottom-Up communication\n18:30 - Emergence of Islands\n22:00 - Cross-Column Attention Mechanism\n27:10 - My Improvements for the Attention Mechanism\n35:25 - Some Design Decisions\n43:25 - Training GLOM as a Denoising Autoencoder & Contrastive Learning\n52:20 - Coordinate Transformations & Representing Uncertainty\n57:05 - How GLOM handles Video\n1:01:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.12627\n\nAbstract:\nThis paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language\n\nAuthors: Geoffrey Hinton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "255": "#Shorts #shorts #openai\n\nIn the paper Multimodal Neurons in Artificial Neural Networks OpenAI suggests that CLIP can be attacked adversarially by putting textual labels onto pictures. They demonstrated this with an apple labeled as an iPod. I reproduce that experiment and suggest a simple, but effective fix. Yes, this is a joke ;)\n\nOriginal Video: https://youtu.be/Z_kWZpgEZ7w\n\nOpenAI does a huge investigation into the inner workings of their recent CLIP model via faceted feature visualization and finds amazing things: Some neurons in the last layer respond to distinct concepts across multiple modalities, meaning they fire for photographs, drawings, and signs depicting the same concept, even when the images are vastly distinct. Through manual examination, they identify and investigate neurons corresponding to persons, geographical regions, religions, emotions, and much more. In this video, I go through the publication and then I present my own findings from digging around in the OpenAI Microscope.\n\nPaper: https://distill.pub/2021/multimodal-neurons/\nMy Findings: https://www.notion.so/CLIP-OpenAI-Microscope-Findings-27465eac373c451d8083428443e0837c\nMy Video on CLIP: https://youtu.be/T9XSU0pKX2E\nMy Video on Feature Visualizations & The OpenAI Microscope: https://youtu.be/Ok44otx90D4\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "256": "#machinelearning #phd #howto\n\nThis video is advice for new PhD students in the field of Machine Learning in 2021 and after. The field has shifted dramatically in the last few years and navigating grad school can be very hard, especially when you're as clueless as I was when I started. The video is a personal recount of my mistakes and what I've learned from them. If you already have several published papers and know what to do, this video is not for you. However, if you are not even sure where to start, how to select a topic, or what goes in a paper, you might benefit from this video, because that's exactly how I felt.\n\nMain Takeaways:\n- Select niche topics rather than hype topics\n- Write papers that can't be rejected\n- Don't be discouraged by bad reviews\n- Take reviewing & teaching seriously\n- Keep up your focus\n- Conferences are for networking\n- Internships are great opportunities\n- Team up with complementary skills\n- Don't work too hard\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Thesis Topic Selection\n4:25 - How To Publish Papers\n5:35 - Dealing With Reviewers\n6:30 - How To Be A Reviewer\n7:40 - Take Teaching Seriously\n8:30 - Maintain Focus\n10:20 - Navigating Conferences\n12:40 - Internships\n13:40 - Collaborations\n14:55 - Don't Forget To Enjoy\n\nTranscript: https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae\n\nCredits to Lanz for editing\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "257": "#minecraft #neuralnetwork #backpropagation\n\nI built an analog neural network in vanilla Minecraft without any mods or command blocks. The network uses Redstone wire power strengths to carry the signal through one hidden layer, including nonlinearities, and then do automatic backpropagation and even weight updates.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:50 - Redstone Components Explained\n5:00 - Analog Multiplication in Redstone\n7:00 - Gradient Descent for Square Root Computation\n9:35 - Neural Network Demonstration\n10:45 - Network Schema Explained\n18:35 - The Network Learns a Datapoint\n20:20 - Outro & Conclusion\n\nI built this during a series of live streams and want to thank everyone who helped me and cheered for me in the chat!\n\nWorld saves here: https://github.com/yk/minecraft-neural-network\nGame here: https://www.minecraft.net\nMultiplier Inspiration: https://www.youtube.com/channel/UCLmzk4TlnLXCXCHcjuJe2ag\n\nCredits to Lanz for editing!\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "258": "#gpt3 #airecipe #cooking\n\nWe went to the store and bought a set of completely random ingredients and had OpenAI's GPT-3 come up with a recipe, which we then cooked and ate.\n\nOur Rules:\n1. All Vegan\n2. Follow the recipe as closely as possible\n3. We must finish our plates\n\nThe Recipe:\n1. Boil the potatoes and carrots.\n2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. \n3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. \n4. Add the soy cream, stir and cook for three minutes. \n5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. \n6. Cut the bread in small squares and fry in the vegan butter until golden brown.\n7. Cut the limes into cubes and squeeze the juice into the bean mixture. \n8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.\n9. Pour the bean mix into a blender. \n10. Bake for 5 minutes in the oven at 180C. \n11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. \n12. Cut the bell pepper into cubes and add to the pot. \n13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. \n14. Add the avocado. \n15. Add the chickpeas. \n16. Add the chocolate.\n17. Serve on bread with mustard and pommegrenade on top.\n\nOUTLINE:\n0:00 - The Plan\n2:15 - Ingredients\n4:05 - What is GPT-3?\n6:10 - Let's cook\n12:25 - The Taste Test\n\nGPT-3 on Wikipedia: https://en.wikipedia.org/wiki/GPT-3\nGPT-3 Paper: https://arxiv.org/abs/2005.14165\n\nJonas' Scholar: https://scholar.google.de/citations?user=a1rCLUMAAAAJ\nEdit by Ryan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "259": "#artificialintelligence #musicvideo #clip\n\nI used OpenAI's CLIP model and BigGAN to create a music video that goes along with the lyrics of a song that I wrote. The song lyrics are made from ImageNet class labels, and the song itself is performed by me on a looper.\n\nOUTLINE:\n0:00 - Intro\n1:00 - AI-generated music video for \"be my weasel\"\n3:50 - How it was made\n7:30 - My looping gear\n9:35 - AI-generated music video #2\n12:45 - Outro & Credits\n\nCode and references: https://github.com/yk/clip_music_video\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "260": "#sbb #seatreview #travel\n\nA friendly parody of Travel Vloggers and Airplane Seat Reviews :)\nNo, SBB did not pay me for this (but they should ;) )\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "261": "#nft #gan #ai\n\nToday we build our own AI that can create as many bored apes as we want! Fungibility for everyone!\n\nTry the model here: https://huggingface.co/spaces/ykilcher/apes\nor here: https://ykilcher.com/apes\nFiles & Models here: https://huggingface.co/ykilcher/apes/tree/main\nCode here: https://github.com/yk/apes-public (for the \"what's your ape\" app, look for the file interface_projector.py)\n\nThis video is sponsored by BrightData, use this link for free credits:\nhttps://brightdata.grsm.io/yannickilcher\n\nOUTLINE:\n0:00 - Introduction\n2:05 - Generative Adversarial Networks\n3:40 - Scraping Opensea with BrightData\n7:55 - Training the GAN\n11:35 - Here are the results!\n15:20 - Diving deeper into BrightData\n\nReferences:\nStylegan 3 imagery: https://nvlabs.github.io/stylegan3/\nBored Ape Yacht Club NFT Collection: https://opensea.io/collection/boredapeyachtclub\nBetter GANFT model: https://medium.com/@nathancooperjones/these-bored-apes-do-not-exist-6bed2c73f02c\nAbstract AI-created apes: https://opensea.io/collection/gan-apes-nft\nhttps://mobile.twitter.com/gannft\nAnother good model: https://twitter.com/cyrilzakka/status/1463944040878071811\nStyleGAN2 versions: https://thispersondoesnotexist.com/\nhttps://thissneakerdoesnotexist.com/\nhttps://thischairdoesnotexist.com/\nGANs: https://en.wikipedia.org/wiki/Generative_adversarial_network\nhttps://arxiv.org/pdf/1406.2661.pdf\nStyleGAN3: https://nvlabs.github.io/stylegan3/\nStyleGAN2 code: https://github.com/NVlabs/stylegan2-ada-pytorch\nCLIP: https://openai.com/blog/clip/\nDALL-E 2 images: https://twitter.com/search?q=%23dalle&f=image\nMy music video: https://www.youtube.com/watch?v=2iq7WXSw26s\nBrightData Links: https://brightdata.com/products/data-collector\nhttps://brightdata.com/testimonials\nhttps://brightdata.com/use-cases/adtech\nhttps://brightdata.com/use-cases/social-media-for-marketing\nhttps://brightdata.com/use-cases/ecommerce\n\nLinks:\nMerch: https://ykilcher.com/merch\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this), find options at https://ykilcher.com", "262": "#huggingface #pickle #exploit \n\nDid you know that something as simple as loading a model can execute arbitrary code on your machine?\n\nTry the model: https://huggingface.co/ykilcher/totally-harmless-model\nGet the code: https://github.com/yk/patch-torch-save\n\nSponsor: Weights & Biases\nGo here: https://wandb.me/yannic\n\nOUTLINE:\n0:00 - Introduction\n1:10 - Sponsor: Weights & Biases\n3:20 - How Hugging Face models are loaded\n5:30 - From PyTorch to pickle\n7:10 - Understanding how pickle saves data\n13:00 - Executing arbitrary code\n15:05 - The final code\n17:25 - How can you protect yourself?\n\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "263": "#alphatensor #deepmind #ai \n\nMatrix multiplication is the most used mathematical operation in all of science and engineering. Speeding this up has massive consequences. Thus, over the years, this operation has become more and more optimized. A fascinating discovery was made when it was shown that one actually needs less than N^3 multiplication operations to multiply to NxN matrices. DeepMind goes a step further and creates AlphaTensor, a Deep Reinforcement Learning algorithm that plays a single-player game, TensorGame, in order to find even more optimized algorithms for matrix multiplication. And it turns out, there exists a plethora of undiscovered matrix multiplication algorithms, which not only will make everything from computers to smart toasters faster, but also bring new insights into fundamental math and complexity theory.\n\nSponsor: Assembly AI\nLink: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic_sentiment\n\nOUTLINE:\n0:00 - Intro\n1:50 - Sponsor: Assembly AI (link in description)\n3:25 - What even is Matrix Multiplication?\n6:10 - A very astounding fact\n8:45 - Trading multiplications for additions\n12:35 - Matrix Multiplication as a Tensor\n17:30 - Tensor Decompositions\n20:30 - A formal way of finding multiplication algorithms\n31:00 - How to formulate this as a game?\n39:30 - A brief primer on AlphaZero / MCTS\n45:40 - The Results\n48:15 - Optimizing for different hardware\n52:40 - Expanding fundamental math\n53:45 - Summary & Final Comments\n\nPaper: https://www.nature.com/articles/s41586-022-05172-4\nTitle: Discovering faster matrix multiplication algorithms with reinforcement learning\n\nAbstract:\nImproving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\u2009\u00d7\u20094 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.\n\nAuthors: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis & Pushmeet Kohli\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "264": "#chatgpt #ai #openai \n\nChatGPT, OpenAI's newest model is a GPT-3 variant that has been fine-tuned using Reinforcement Learning from Human Feedback, and it is taking the world by storm!\n\nSponsor: Weights & Biases\nhttps://wandb.me/yannic\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Weights & Biases\n3:20 - ChatGPT: How does it work?\n5:20 - Reinforcement Learning from Human Feedback\n7:10 - ChatGPT Origins: The GPT-3.5 Series\n8:20 - OpenAI's strategy: Iterative Refinement\n9:10 - ChatGPT's amazing capabilities\n14:10 - Internals: What we know so far\n16:10 - Building a virtual machine in ChatGPT's imagination (insane)\n20:15 - Jailbreaks: Circumventing the safety mechanisms\n29:25 - How OpenAI sees the future\n\nReferences:\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/language-model-safety-and-misuse/\nhttps://beta.openai.com/docs/model-index-for-researchers\nhttps://scale.com/blog/gpt-3-davinci-003-comparison#Conclusion\nhttps://twitter.com/johnvmcdonnell/status/1598470129121374209\nhttps://twitter.com/blennon_/status/1597374826305318912\nhttps://twitter.com/TimKietzmann/status/1598230759118376960/photo/1\nhttps://twitter.com/_lewtun/status/1598056075672027137/photo/2\nhttps://twitter.com/raphaelmilliere/status/1598469100535259136\nhttps://twitter.com/CynthiaSavard/status/1598498138658070530/photo/1\nhttps://twitter.com/tylerangert/status/1598389755997290507/photo/1\nhttps://twitter.com/amasad/status/1598042665375105024/photo/1\nhttps://twitter.com/goodside/status/1598129631609380864/photo/1\nhttps://twitter.com/moyix/status/1598081204846489600/photo/2\nhttps://twitter.com/JusticeRage/status/1598959136531546112\nhttps://twitter.com/yoavgo/status/1598594145605636097\nhttps://twitter.com/EladRichardson/status/1598333315764871174\nhttps://twitter.com/charles_irl/status/1598319027327307785/photo/4\nhttps://twitter.com/jasondebolt/status/1598243854343606273\nhttps://twitter.com/mattshumer_/status/1598185710166896641/photo/1\nhttps://twitter.com/i/web/status/1598246145171804161\nhttps://twitter.com/bleedingedgeai/status/1598378564373471232\nhttps://twitter.com/MasterScrat/status/1598830356115124224\nhttps://twitter.com/Sentdex/status/1598803009844256769\nhttps://twitter.com/harrison_ritz/status/1598828017446371329\nhttps://twitter.com/parafactual/status/1598212029479026689\nhttps://www.engraved.blog/building-a-virtual-machine-inside/\nhttps://twitter.com/317070\nhttps://twitter.com/zehavoc/status/1599193444043268096\nhttps://twitter.com/yoavgo/status/1598360581496459265\nhttps://twitter.com/yoavgo/status/1599037412411596800\nhttps://twitter.com/yoavgo/status/1599045344863879168\nhttps://twitter.com/natfriedman/status/1598477452661383168\nhttps://twitter.com/conradev/status/1598487973351362561/photo/1\nhttps://twitter.com/zswitten/status/1598100186605441024\nhttps://twitter.com/CatEmbedded/status/1599141379879600128/photo/2\nhttps://twitter.com/mattshumer_/status/1599175127148949505\nhttps://twitter.com/vaibhavk97/status/1598930958769860608/photo/1\nhttps://twitter.com/dan_abramov/status/1598800508160024588/photo/1\nhttps://twitter.com/MinqiJiang/status/1598832656422432768/photo/2\nhttps://twitter.com/zswitten/status/1598088280066920453\nhttps://twitter.com/m1guelpf/status/1598203861294252033/photo/1\nhttps://twitter.com/SilasAlberti/status/1598257908567117825/photo/1\nhttps://twitter.com/gf_256/status/1598962842861899776/photo/1\nhttps://twitter.com/zswitten/status/1598088267789787136\nhttps://twitter.com/gf_256/status/1598178469955112961/photo/1\nhttps://twitter.com/samczsun/status/1598564871653789696/photo/1\nhttps://twitter.com/haus_cole/status/1598541468058390534/photo/3\nhttps://twitter.com/tailcalled/status/1599181030065246208/photo/1\nhttps://twitter.com/pensharpiero/status/1598731292278865920\nhttps://twitter.com/sleepdensity/status/1598233414683197441\nhttps://twitter.com/goodside/status/1598253337400717313\nhttps://twitter.com/Carnage4Life/status/1598332648723976193/photo/2\nhttps://github.com/sw-yx/ai-notes/blob/main/TEXT.md#jailbreaks\nhttps://twitter.com/dannypostmaa/status/1599352584963170309/photo/4\nhttps://twitter.com/sama/status/1599112749833125888\nhttps://twitter.com/sama/status/1599114807474810884\nhttps://twitter.com/sama/status/1599461195005587456\nhttps://twitter.com/deliprao/status/1599451192215887872\nhttps://twitter.com/michlbrmly/status/1599168681711656961\nhttps://twitter.com/zoink/status/1599281052115034113\n\n\nLinks:\nhttps://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2", "265": "#gpt4chan #4chan #ai \n\nGPT-4chan was trained on over 3 years of posts from 4chan's \"politically incorrect\" (/pol/) board.\n(and no, this is not GPT-4)\n\nEXTRA VIDEO HERE: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n\nWebsite (try the model here): https://gpt-4chan.com\nModel (no longer available): https://huggingface.co/ykilcher/gpt-4chan\nCode: https://github.com/yk/gpt-4chan-public\nDataset: https://zenodo.org/record/3606810#.YpjGgexByDU\n\nOUTLINE:\n0:00 - Intro\n0:30 - Disclaimers\n1:20 - Elon, Twitter, and the Seychelles\n4:10 - How I trained a language model on 4chan posts\n6:30 - How good is this model?\n8:55 - Building a 4chan bot\n11:00 - Something strange is happening\n13:20 - How the bot got unmasked\n15:15 - Here we go again\n18:00 - Final thoughts\n\nERRATA:\n- I stated that the model is better on the automated parts of TruthfulQA than any other GPT out there, which is incorrect. There exist some small GPT-models with similar performance, I was mainly talking about the flagship models, such as GPT-3 and GPT-J.\n\nLinks:\nMerch: https://ykilcher.com/merch\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "266": "#lamda #google #ai \n\nGoogle engineer Blake Lemoine was put on leave after releasing proprietary information: An interview with the chatbot LaMDA that he believes demonstrates that this AI is, in fact, sentient. We analyze the claims and the interview in detail and trace how a statistical machine managed to convince at least one human that it is more than just an algorithm.\n\nOUTLINE:\n0:00 - Whistleblower put on leave\n4:30 - What is a language model?\n6:40 - The prompt is the key\n10:40 - Who are we talking to exactly?\n12:50 - LaMDA analyzes stories\n15:20 - Fear, pain, and consent\n20:25 - How would we recognize sentience? When is a machine conscious?\n\nReferences:\nhttps://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917\nhttps://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489\nhttps://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/\nhttps://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine\nhttps://www.businessinsider.com/transcript-of-sentient-google-ai-chatbot-was-edited-for-readability-2022-6?inline-endstory-related-recommendations=&r=US&IR=T\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "267": "#openassistant #chatgpt #ai \n\nHelp us collect data for OpenAssistant, the largest and most open alternative to ChatGPT.\nhttps://open-assistant.io\n\nOUTLINE:\n0:00 - Intro\n0:30 - The Project\n2:05 - Getting to Minimum Viable Prototype\n5:30 - First Tasks\n10:00 - Leaderboard\n11:45 - Playing the Assistant\n14:40 - Tricky Facts\n16:25 - What if humans had wings?\n17:05 - Can foxes be tamed?\n23:45 - Can zebras be tamed?\n26:15 - Yo (spam)\n27:00 - More tasks\n29:10 - Entitled Emails\n34:35 - Final Words\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "268": "#openassistant #chatgpt #gpt4\n\nhttps://open-assistant.io/chat\nhttps://huggingface.co/OpenAssistant\nhttps://github.com/LAION-AI/Open-Assistant\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "269": "#openassistant #chatgpt #mlnews \n\nTry the chat: https://open-assistant.io/chat\nHomepage: https://open-assistant.io \nDataset: https://huggingface.co/datasets/OpenAssistant/oasst1\nCode: https://github.com/LAION-AI/Open-Assistant\nPaper (temporary): https://ykilcher.com/oa-paper\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "270": "https://arxiv.org/abs/1806.07366\n\nAbstract:\nWe introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.\n\nAuthors:\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud", "271": "https://arxiv.org/abs/1902.04818\n\nAbstract:\nWe investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.\n\nAuthors:\nKevin Roth, Yannic Kilcher, Thomas Hofmann", "272": "https://arxiv.org/abs/1502.03167\n\nAbstract:\nTraining Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.\n\nAuthors:\nSergey Ioffe, Christian Szegedy", "273": "We present a stochastic non-autoregressive RNN that does not require teacher-forcing for training. The content is based on our 2018 NeurIPS paper:\n\nDeep State Space Models for Unconditional Word Generation\nhttps://arxiv.org/abs/1806.04550", "274": "https://arxiv.org/abs/1811.12359\n\nAbstract:\nIn recent years, the interest in unsupervised learning of disentangled representations has significantly increased. The key assumption is that real-world data is generated by a few explanatory factors of variation and that these factors can be recovered by unsupervised learning algorithms. A large number of unsupervised learning approaches based on auto-encoding and quantitative evaluation metrics of disentanglement have been proposed; yet, the efficacy of the proposed approaches and utility of proposed notions of disentanglement has not been challenged in prior work. In this paper, we provide a sober look on recent progress in the field and challenge some common assumptions. \nWe first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering the six most prominent methods, and evaluate them across six disentanglement metrics in a reproducible large-scale experimental study on seven different data sets. On the positive side, we observe that different methods successfully enforce properties \"encouraged\" by the corresponding losses. On the negative side, we observe in our study that well-disentangled models seemingly cannot be identified without access to ground-truth labels even if we are allowed to transfer hyperparameters across data sets. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. \nThese results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.\n\nAuthors:\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Sch\u00f6lkopf, Olivier Bachem", "275": "https://arxiv.org/abs/1706.03762\n\nAbstract:\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\nAuthors:\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin", "276": "Standard neural networks suffer from problems such as un-smooth classification boundaries and overconfidence. Manifold Mixup is an easy regularization technique that rectifies these problems. It works by interpolating hidden representations of different data points and then train them to predict equally interpolated labels.\n\nhttps://arxiv.org/abs/1806.05236\n\nAbstract:\nDeep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.\n\nAuthors:\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio", "277": "Current CNNs have to downsample large images before processing them, which can lose a lot of detail information. This paper proposes attention sampling, which learns to selectively process parts of any large image in full resolution, while discarding uninteresting bits. This leads to enormous gains in speed and memory consumption.\n\nhttps://arxiv.org/abs/1905.03711\n\nAbstract:\nExisting deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single GPU setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal SGD procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.\n\nAuthors: Angelos Katharopoulos, Fran\u00e7ois Fleuret", "278": "Ever wanted to do a convolution on a Klein Bottle? This paper defines CNNs over manifolds such that they are independent of which coordinate frame you choose. Amazingly, this then results in an efficient practical method to achieve state-of-the-art in several tasks!\n\nhttps://arxiv.org/abs/1902.04615\n\nAbstract:\nThe principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.\n\nAuthors: Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling", "279": "Geoff Hinton's next big idea! Capsule Networks are an alternative way of implementing neural networks by dividing each layer into capsules. Each capsule is responsible for detecting the presence and properties of one particular entity in the input sample. This information is then allocated dynamically to higher-level capsules in a novel and unconventional routing scheme. While Capsule Networks are still in their infancy, they are an exciting and promising new direction.\n\nAbstract:\nA capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.\n\nAuthors: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton\n\nhttps://arxiv.org/abs/1710.09829\n\n\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nMinds: https://www.minds.com/ykilcher\nBitChute: https://www.bitchute.com/channel/10a5ui845DOJ/", "280": "With just a single image as an input, this algorithm learns a generative model that matches the input image's patch distribution at multiple scales and resolutions. This enables sampling of extremely realistic looking variations on the original image and much more.\n\nAbstract:\nWe introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.\n\nAuthors: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli\n\nhttps://arxiv.org/abs/1905.01164\nhttps://github.com/tamarott/SinGAN\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "281": "The Transformer for the masses! Reformer solves the biggest problem with the famous Transformer model: Its huge resource requirements. By cleverly combining Locality Sensitive Hashing and ideas from Reversible Networks, the classically huge footprint of the Transformer is drastically reduced. Not only does that mean the model uses less memory, but it can process much longer input sequences, up to 16K tokens with just 16gb of memory!\n\nhttps://arxiv.org/abs/2001.04451\nhttps://ai.googleblog.com/2020/01/reformer-efficient-transformer.html\n\nAbstract:\nLarge Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(LlogL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\nAuthors: Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "282": "The Game of Life on steroids! This model learns to grow complex patterns in an entirely local way. Each cell is trained to listen to its neighbors and update itself in a way such that, collectively, an overall goal is reached. Fascinating and interactive!\n\nhttps://distill.pub/2020/growing-ca/\nhttps://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\n\nAbstract:\nMost multicellular organisms begin their life as a single egg cell - a single cell whose progeny reliably self-assemble into highly complex anatomies with many organs and tissues in precisely the same arrangement each time. The ability to build their own bodies is probably the most fundamental skill every living creature possesses. Morphogenesis (the process of an organism\u2019s shape development) is one of the most striking examples of a phenomenon called self-organisation. Cells, the tiny building blocks of bodies, communicate with their neighbors to decide the shape of organs and body plans, where to grow each organ, how to interconnect them, and when to eventually stop. Understanding the interplay of the emergence of complex outcomes from simple rules and homeostatic 1 feedback loops is an active area of research. What is clear is that evolution has learned to exploit the laws of physics and computation to implement the highly robust morphogenetic software that runs on genome-encoded cellular hardware.\n\nThis process is extremely robust to perturbations. Even when the organism is fully developed, some species still have the capability to repair damage - a process known as regeneration. Some creatures, such as salamanders, can fully regenerate vital organs, limbs, eyes, or even parts of the brain! Morphogenesis is a surprisingly adaptive process. Sometimes even a very atypical development process can result in a viable organism - for example, when an early mammalian embryo is cut in two, each half will form a complete individual - monozygotic twins!\n\nThe biggest puzzle in this field is the question of how the cell collective knows what to build and when to stop. The sciences of genomics and stem cell biology are only part of the puzzle, as they explain the distribution of specific components in each cell, and the establishment of different types of cells. While we know of many genes that are required for the process of regeneration, we still do not know the algorithm that is sufficient for cells to know how to build or remodel complex organs to a very specific anatomical end-goal. Thus, one major lynch-pin of future work in biomedicine is the discovery of the process by which large-scale anatomy is specified within cell collectives, and how we can rewrite this information to have rational control of growth and form. It is also becoming clear that the software of life possesses numerous modules or subroutines, such as \u201cbuild an eye here\u201d, which can be activated with simple signal triggers. Discovery of such subroutines and a mapping out of the developmental logic is a new field at the intersection of developmental biology and computer science. An important next step is to try to formulate computational models of this process, both to enrich the conceptual toolkit of biologists and to help translate the discoveries of biology into better robotics and computational technology.\n\nImagine if we could design systems of the same plasticity and robustness as biological life: structures and machines that could grow and repair themselves. Such technology would transform the current efforts in regenerative medicine, where scientists and clinicians seek to discover the inputs or stimuli that could cause cells in the body to build structures on demand as needed. To help crack the puzzle of the morphogenetic code, and also exploit the insights of biology to create self-repairing systems in real life, we try to replicate some of the desired properties in an in silico experiment.\n\nAuthors: Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "283": "This model solves integrals and ODEs by doing seq2seq!\n\nhttps://arxiv.org/abs/1912.01412\nhttps://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/\n\nAbstract:\nNeural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.\n\nAuthors: Guillaume Lample, Fran\u00e7ois Charton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "284": "MetNet is a predictive neural network model for weather prediction. It uses axial attention to capture long-range dependencies. Axial attention decomposes attention layers over images into row-attention and column-attention in order to save memory and computation.\n\nhttps://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html\nhttps://arxiv.org/abs/1912.12180\n\nAbstract:\nWeather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km2 and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.\n\nAuthors: Casper Kaae S\u00f8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver,Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "285": "From the makers of Go-Explore, POET is a mixture of ideas from novelty search, evolutionary methods, open-ended learning and curriculum learning.\n\nhttps://arxiv.org/abs/1901.01753\n\nAbstract:\nWhile the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. Our results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.\n\nAuthors: Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "286": "Normalization and activation layers have seen a long history of hand-crafted variants with various results. This paper proposes an evolutionary search to determine the ultimate, final and best combined normalization-activation layer... in a very specific setting.\n\nhttps://arxiv.org/abs/2004.02967\n\nAbstract:\nNormalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of EvoNorms, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that EvoNorms not only excel on a variety of image classification models including ResNets, MobileNets and EfficientNets, but also transfer well to Mask R-CNN for instance segmentation and BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers by a significant margin in many cases.\n\nAuthors: Hanxiao Liu, Andrew Brock, Karen Simonyan, Quoc V. Le\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "287": "Stunning evidence for the hypothesis that neural networks work so well because their random initialization almost certainly contains a nearly optimal sub-network that is responsible for most of the final performance.\n\nhttps://arxiv.org/abs/1803.03635\n\nAbstract:\nNeural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.\n\nAuthors: Jonathan Frankle, Michael Carbin\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "288": "FixMatch is a simple, yet surprisingly effective approach to semi-supervised learning. It combines two previous methods in a clever way and achieves state-of-the-art in regimes with few and very few labeled examples.\n\nPaper: https://arxiv.org/abs/2001.07685\nCode: https://github.com/google-research/fixmatch\n\nAbstract:\nSemi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at this https URL.\n\nAuthors: Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "289": "A closer look at the OpenAI microscope, a database of visualizations of the inner workings of ImageNet classifiers, along with an explanation of how to obtain these visualizations.\n\nhttps://distill.pub/2017/feature-visualization/\nhttps://microscope.openai.com/models\nhttps://github.com/tensorflow/lucid\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "290": "The Longformer extends the Transformer by introducing sliding window attention and sparse global attention. This allows for the processing of much longer documents than classic models like BERT.\n\nPaper: https://arxiv.org/abs/2004.05150\nCode: https://github.com/allenai/longformer\n\nAbstract:\nTransformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.\n\nAuthors: Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "291": "This generative model for music can make entire songs with remarkable quality and consistency. It can be conditioned on genre, artist, and even lyrics.\n\nBlog: https://openai.com/blog/jukebox/\nPaper: https://cdn.openai.com/papers/jukebox.pdf\nCode: https://github.com/openai/jukebox/\n\nAbstract:\nWe introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.\n\nAuthors: Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "292": "One CNN to rule them all! BiT is a pre-trained ResNet that can be used as a starting point for any visual task. This paper explains what it takes to pre-train such a large model and details how fine-tuning on downstream tasks is done best.\n\nPaper: https://arxiv.org/abs/1912.11370\nCode & Models: TBA\n\nAbstract:\nTransfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.\n\nAuthors: Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "293": "The dirty little secret of Batch Normalization is its intrinsic dependence on the training batch size. Group Normalization attempts to achieve the benefits of normalization without batch statistics and, most importantly, without sacrificing performance compared to Batch Normalization.\n\nhttps://arxiv.org/abs/1803.08494\n\nAbstract:\nBatch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.\n\nAuthors: Yuxin Wu, Kaiming He\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "294": "It's common for neural networks to include data normalization such as BatchNorm or GroupNorm. This paper extends the normalization to also include the weights of the network. This surprisingly simple change leads to a boost in performance and - combined with GroupNorm - new state-of-the-art results.\n\nhttps://arxiv.org/abs/1903.10520\n\nAbstract:\nIn this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: this https URL.\n\nAuthors: Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "295": "Object detection in images is a notoriously hard task! Objects can be of a wide variety of classes, can be numerous or absent, they can occlude each other or be out of frame. All of this makes it even more surprising that the architecture in this paper is so simple. Thanks to a clever loss function, a single Transformer stacked on a CNN is enough to handle the entire task!\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:50 - Problem Formulation\n2:30 - Architecture Overview\n6:20 - Bipartite Match Loss Function\n15:55 - Architecture in Detail\n25:00 - Object Queries\n31:00 - Transformer Properties\n35:40 - Results\n\nERRATA:\nWhen I introduce bounding boxes, I say they consist of x and y, but you also need the width and height.\n\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\n\nPaper: https://arxiv.org/abs/2005.12872\nBlog: https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/\nCode: https://github.com/facebookresearch/detr\n\nAbstract:\nWe present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.\n\nAuthors: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "296": "Do we really need dot-product attention? The attention mechanism is a central part of modern Transformers, mainly due to the dot-product attention mechanism. This paper changes the mechanism to remove the quadratic interaction terms and comes up with a new model, the Synthesizer. As it turns out, you can do pretty well like that!\n\nOUTLINE:\n0:00 - Intro & High Level Overview\n1:00 - Abstract\n2:30 - Attention Mechanism as Information Routing\n5:45 - Dot Product Attention\n8:05 - Dense Synthetic Attention\n15:00 - Random Synthetic Attention\n17:15 - Comparison to Feed-Forward Layers\n22:00 - Factorization & Mixtures\n23:10 - Number of Parameters\n25:35 - Machine Translation & Language Modeling Experiments\n36:15 - Summarization & Dialogue Generation Experiments\n37:15 - GLUE & SuperGLUE Experiments\n42:00 - Weight Sizes & Number of Head Ablations\n47:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2005.00743\nMy Video on Transformers (Attention Is All You Need): https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nThe dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \\textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).\n\nAuthors: Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "297": "How do you learn labels without labels? How do you classify images when you don't know what to classify them into? This paper investigates a new combination of representation learning, clustering, and self-labeling in order to group visually similar images together - and achieves surprisingly high accuracy on benchmark datasets.\n\nOUTLINE:\n0:00 - Intro & High-level Overview\n2:15 - Problem Statement\n4:50 - Why naive Clustering does not work\n9:25 - Representation Learning\n13:40 - Nearest-neighbor-based Clustering\n28:00 - Self-Labeling\n32:10 - Experiments\n38:20 - ImageNet Experiments\n41:00 - Overclustering\n\nPaper: https://arxiv.org/abs/2005.12320\nCode: https://github.com/wvangansbeke/Unsupervised-Classification\n\nAbstract:\nIs it possible to automatically classify images without the use of ground-truth annotations? Or when even the classes themselves, are not a priori known? These remain important, and open questions in computer vision. Several approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by huge margins, in particular +26.9% on CIFAR10, +21.5% on CIFAR100-20 and +11.7% on STL10 in terms of classification accuracy. Furthermore, results on ImageNet show that our approach is the first to scale well up to 200 randomly selected classes, obtaining 69.3% top-1 and 85.5% top-5 accuracy, and marking a difference of less than 7.5% with fully-supervised methods. Finally, we applied our approach to all 1000 classes on ImageNet, and found the results to be very encouraging. The code will be made publicly available.\n\nAuthors: Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "298": "Neural Architecture Search is usually prohibitively expensive in both time and resources to be useful. A search strategy has to keep evaluating new models, training them to convergence in an inner loop to find out if they are any good. This paper proposes to abstract the problem and extract the essential part of the architecture to be optimized into a smaller version and evaluates that version on specifically custom learned data points to predict its performance, which is much faster and cheaper than running the full model.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:00 - Neural Architecture Search\n4:30 - Predicting performance via architecture encoding\n7:50 - Synthetic Petri Dish\n12:50 - Motivating MNIST example\n18:15 - Entire Algorithm\n23:00 - Producing the synthetic data\n26:00 - Combination with architecture search\n27:30 - PTB RNN-Cell Experiment\n29:20 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2005.13092\nCode: https://github.com/uber-research/Synthetic-Petri-Dish\n\nAbstract:\nNeural Architecture Search (NAS) explores a large space of architectural motifs -- a compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands of domain-specific data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available. Our hope is that this work can inspire a new research direction in studying the performance of extracted components of models in an alternative controlled setting.\n\nAuthors: Aditya Rawal, Joel Lehman, Felipe Petroski Such, Jeff Clune, Kenneth O. Stanley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "299": "Text-to-speech engines are usually multi-stage pipelines that transform the signal into many intermediate representations and require supervision at each step. When trying to train TTS end-to-end, the alignment problem arises: Which text corresponds to which piece of sound? This paper uses an alignment module to tackle this problem and produces astonishingly good sound.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems with Text-to-Speech\n3:55 - Adversarial Training\n5:20 - End-to-End Training\n7:20 - Discriminator Architecture\n10:40 - Generator Architecture\n12:20 - The Alignment Problem\n14:40 - Aligner Architecture\n24:00 - Spectrogram Prediction Loss\n32:30 - Dynamic Time Warping\n38:30 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.03575\nWebsite: https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech\n\nAbstract:\nModern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable monotonic interpolation scheme to predict the duration of each input token. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.\n\nAuthors: Jeff Donahue, Sander Dieleman, Miko\u0142aj Bi\u0144kowski, Erich Elsen, Karen Simonyan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "300": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "301": "Pre-training a CNN backbone for visual transfer learning has recently seen a big push into the direction of incorporating more data, at the cost of less supervision. This paper investigates the opposite: Visual transfer learning by pre-training from very few, but very high-quality samples on an image captioning task.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pre-Training for Visual Tasks\n3:40 - Quality-Quantity Tradeoff\n5:50 - Image Captioning\n8:35 - VirTex Method\n14:30 - Linear Classification\n20:30 - Ablations\n22:05 - Fine-Tuning\n25:45 - Attention Visualization\n27:30 - Conclusion & Remarks\n\nPaper: https://arxiv.org/abs/2006.06666\nCode: https://github.com/kdexd/virtex\n\nAbstract:\nThe de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.\n\nAuthors: Karan Desai, Justin Johnson\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "302": "The Lottery Ticket Hypothesis has shown that it's theoretically possible to prune a neural network at the beginning of training and still achieve good performance, if we only knew which weights to prune away. This paper does not only explain where other attempts at pruning fail, but provides an algorithm that provably reaches maximum compression capacity, all without looking at any data!\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pruning Neural Networks\n3:40 - Lottery Ticket Hypothesis\n6:00 - Paper Story Overview\n9:45 - Layer Collapse\n18:15 - Synaptic Saliency Conservation\n23:25 - Connecting Layer Collapse & Saliency Conservation\n28:30 - Iterative Pruning avoids Layer Collapse\n33:20 - The SynFlow Algorithm\n40:45 - Experiments\n43:35 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05467\nCode: https://github.com/ganguli-lab/Synaptic-Flow\nMy Video on the Lottery Ticket Hypothesis: https://youtu.be/ZVVnvZdUMUk\nStreet Talk about LTH: https://youtu.be/SfjJoevBbjU\n\nAbstract:\nPruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.\n\nAuthors: Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "303": "Even though LSTMs and GRUs solve the vanishing and exploding gradient problems, they have trouble learning to remember things over very long time spans. Inspired from bistability, a property of biological neurons, this paper constructs a recurrent cell with an inherent memory property, with only minimal modification to existing architectures.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Recurrent Neural Networks\n6:00 - Gated Recurrent Unit\n14:40 - Neuronal Bistability\n22:50 - Bistable Recurrent Cell\n31:00 - Neuromodulation\n32:50 - Copy First Benchmark\n37:35 - Denoising Benchmark\n48:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05252\nCode: https://github.com/nvecoven/BRC\n\nAbstract:\nRecurrent neural networks (RNNs) provide state-of-the-art performances in a wide variety of tasks that require memory. These performances can often be achieved thanks to gated recurrent cells such as gated recurrent units (GRU) and long short-term memory (LSTM). Standard gated cells share a layer internal state to store information at the network level, and long term memory is shaped by network-wide recurrent connection weights. Biological neurons on the other hand are capable of holding information at the cellular level for an arbitrary long amount of time through a process called bistability. Through bistability, cells can stabilize to different stable states depending on their own past state and inputs, which permits the durable storing of past information in neuron state. In this work, we take inspiration from biological neuron bistability to embed RNNs with long-lasting memory at the cellular level. This leads to the introduction of a new bistable biologically-inspired recurrent cell that is shown to strongly improves RNN performance on time-series which require very long memory, despite using only cellular connections (all recurrent connections are from neurons to themselves, i.e. a neuron state is not influenced by the state of other neurons). Furthermore, equipping this cell with recurrent neuromodulation permits to link them to standard GRU cells, taking a step towards the biological plausibility of GRU.\n\nAuthors: Nicolas Vecoven, Damien Ernst, Guillaume Drion\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "304": "Image-to-Image translation usually requires corresponding samples or at least domain labels of the dataset. This paper removes that restriction and allows for fully unsupervised image translation of a source image to the style of one or many reference images. This is achieved by jointly training a guiding network that provides style information and pseudo-labels.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Unsupervised Image-to-Image Translation\n7:05 - Architecture Overview\n14:15 - Pseudo-Label Loss\n19:30 - Encoder Style Contrastive Loss\n25:30 - Adversarial Loss\n31:20 - Generator Style Contrastive Loss\n35:15 - Image Reconstruction Loss\n36:55 - Architecture Recap\n39:55 - Full Loss\n42:05 - Experiments\n\nPaper: https://arxiv.org/abs/2006.06500\nCode: https://github.com/clovaai/tunit\n\nAbstract:\nEvery recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, our model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at this https URL\n\nAuthors: Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "305": "Implicit neural representations are created when a neural network is used to represent a signal as a function. SIRENs are a particular type of INR that can be applied to a variety of signals, such as images, sound, or 3D shapes. This is an interesting departure from regular machine learning and required me to think differently.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:15 - Implicit Neural Representations\n9:40 - Representing Images\n14:30 - SIRENs\n18:05 - Initialization\n20:15 - Derivatives of SIRENs\n23:05 - Poisson Image Reconstruction\n28:20 - Poisson Image Editing\n31:35 - Shapes with Signed Distance Functions\n45:55 - Paper Website\n48:55 - Other Applications\n50:45 - Hypernetworks over SIRENs\n54:30 - Broader Impact\n\nPaper: https://arxiv.org/abs/2006.09661\nWebsite: https://vsitzmann.github.io/siren/\n\nAbstract:\nImplicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.\n\nAuthors: Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "306": "Counting repeated actions in a video is one of the easiest tasks for humans, yet remains incredibly hard for machines. RepNet achieves state-of-the-art by creating an information bottleneck in the form of a temporal self-similarity matrix, relating video frames to each other in a way that forces the model to surface the information relevant for counting. Along with that, the authors produce a new dataset for evaluating counting models.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Problem Statement\n5:15 - Output & Loss\n6:25 - Per-Frame Embeddings\n11:20 - Temporal Self-Similarity Matrix\n19:00 - Periodicity Predictor\n25:50 - Architecture Recap\n27:00 - Synthetic Dataset\n30:15 - Countix Dataset\n31:10 - Experiments\n33:35 - Applications\n35:30 - Conclusion & Comments\n\nPaper Website: https://sites.google.com/view/repnet\nColab: https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\n\nAbstract:\nWe present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called RepNet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix (~90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos.\n\nAuthors: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "307": "Neural networks are very good at predicting systems' numerical outputs, but not very good at deriving the discrete symbolic equations that govern many physical systems. This paper combines Graph Networks with symbolic regression and shows that the strong inductive biases of these models can be used to derive accurate symbolic equations from observation data.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:10 - Problem Statement\n4:25 - Symbolic Regression\n6:40 - Graph Neural Networks\n12:05 - Inductive Biases for Physics\n15:15 - How Graph Networks compute outputs\n23:10 - Loss Backpropagation\n24:30 - Graph Network Recap\n26:10 - Analogies of GN to Newtonian Mechanics\n28:40 - From Graph Network to Equation\n33:50 - L1 Regularization of Edge Messages\n40:10 - Newtonian Dynamics Example\n43:10 - Cosmology Example\n44:45 - Conclusions & Appendix\n\nPaper: https://arxiv.org/abs/2006.11287\nCode: https://github.com/MilesCranmer/symbolic_deep_learning\n\nAbstract:\nWe develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.\n\nAuthors: Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "308": "Backpropagation is one of the central components of modern deep learning. However, it's not biologically plausible, which limits the applicability of deep learning to understand how the human brain works. Direct Feedback Alignment is a biologically plausible alternative and this paper shows that, contrary to previous research, it can be successfully applied to modern deep architectures and solve challenging tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Problem with Backpropagation\n10:25 - Direct Feedback Alignment\n21:00 - My Intuition why DFA works\n31:20 - Experiments\n\nPaper: https://arxiv.org/abs/2006.12878\nCode: https://github.com/lightonai/dfa-scales-to-modern-deep-learning\nReferenced Paper by Arild N\u00f8kland: https://arxiv.org/abs/1609.01596\n\nAbstract:\nDespite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.\n\nAuthors: Julien Launay, Iacopo Poli, Fran\u00e7ois Boniface, Florent Krzakala\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "309": "Object detection often does not occur in a vacuum. Static cameras, such as wildlife traps, collect lots of irregularly sampled data over a large time frame and often capture repeating or similar events. This model learns to dynamically incorporate other frames taken by the same camera into its object detection pipeline.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Problem Formulation\n2:10 - Static Camera Data\n6:45 - Architecture Overview\n10:00 - Short-Term Memory\n15:40 - Long-Term Memory\n20:10 - Quantitative Results\n22:30 - Qualitative Results\n30:10 - False Positives\n32:50 - Appendix & Conclusion\n\nPaper: https://arxiv.org/abs/1912.03538\n\nMy Video On Attention Is All You Need: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nIn static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame.\nWe apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.\n\nAuthors: Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "310": "We've become very good at making generative models for images and classes of images, but not yet of sets of images, especially when the number of sets is unknown and can contain sets that have never been encountered during training. This paper builds a probabilistic framework and a practical implementation of a generative model for sets of images based on variational methods.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n8:05 - Architecture Overview\n20:05 - Probabilistic Model\n33:50 - Likelihood Function\n40:30 - Model Architectures\n44:20 - Loss Function & Optimization\n47:30 - Results\n58:45 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.10705\n\nAbstract:\nImages with shared characteristics naturally form sets. For example, in a face verification benchmark, images of the same identity form sets. For generative models, the standard way of dealing with sets is to represent each as a one hot vector, and learn a conditional generative model p(x|y). This representation assumes that the number of sets is limited and known, such that the distribution over sets reduces to a simple multinomial distribution. In contrast, we study a more generic problem where the number of sets is large and unknown. We introduce Set Distribution Networks (SDNs), a novel framework that learns to autoencode and freely generate sets. We achieve this by jointly learning a set encoder, set discriminator, set generator, and set prior. We show that SDNs are able to reconstruct image sets that preserve salient attributes of the inputs in our benchmark datasets, and are also able to generate novel objects/identities. We examine the sets generated by SDN with a pre-trained 3D reconstruction network and a face verification network, respectively, as a novel way to evaluate the quality of generated sets of images.\n\nAuthors: Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Carlos Guestrin, Josh M. Susskind\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "311": "Visual scenes are often comprised of sets of independent objects. Yet, current vision models make no assumptions about the nature of the pictures they look at. By imposing an objectness prior, this paper a module that is able to recognize permutation-invariant sets of objects from pixels in both supervised and unsupervised settings. It does so by introducing a slot attention module that combines an attention mechanism with dynamic routing.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Problem Formulation\n4:30 - Slot Attention Architecture\n13:30 - Slot Attention Algorithm\n21:30 - Iterative Routing Visualization\n29:15 - Experiments\n36:20 - Inference Time Flexibility\n38:35 - Broader Impact Statement\n42:05 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15055\n\nMy Video on Facebook's DETR: https://youtu.be/T35ba_VXkMY\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on Capsules: https://youtu.be/nXGHJTtFYRU\n\nAbstract:\nLearning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.\n\nAuthors: Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "312": "Google builds a 600 billion parameter transformer to do massively multilingual, massive machine translation. Interestingly, the larger model scale does not come from increasing depth of the transformer, but from increasing width in the feedforward layers, combined with a hard routing to parallelize computations on up to 2048 TPUs. A very detailed engineering paper!\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Main Results\n5:10 - Mixture-of-Experts\n16:00 - Difference to Scaling Classic Transformers\n18:50 - Backpropagation in Mixture-of-Experts\n20:05 - MoE Routing Algorithm in GShard\n38:20 - GShard Einsum Examples\n47:40 - Massively Multilingual Translation\n56:00 - Results\n1:11:30 - Conclusion & Comments\n\nERRATA:\nI said the computation of MoE scales linearly, but actually, it's sub(!)-linear.\n\nPaper: https://arxiv.org/abs/2006.16668\n\nAbstract:\nNeural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.\n\nAuthors:\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "313": "#machinelearning #ai #google\n\nThe high-level architecture of CNNs has not really changed over the years. We tend to build high-resolution low-dimensional layers first, followed by ever more coarse, but deep layers. This paper challenges this decades-old heuristic and uses neural architecture search to find an alternative, called SpineNet that employs multiple rounds of re-scaling and long-range skip connections.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Problem Statement\n2:30 - The Problem with Current Architectures\n8:20 - Scale-Permuted Networks\n11:40 - Neural Architecture Search\n14:00 - Up- and Downsampling\n19:10 - From ResNet to SpineNet\n24:20 - Ablations\n27:00 - My Idea: Attention Routing for CNNs\n29:55 - More Experiments\n34:45 - Conclusion & Comments\n\nPapers: https://arxiv.org/abs/1912.05027\nCode: https://github.com/tensorflow/tpu/tree/master/models/official/detection\n\nAbstract:\nConvolutional neural networks typically encode an input image into a series of intermediate features with decreasing resolutions. While this structure is suited to classification tasks, it does not perform well for tasks requiring simultaneous recognition and localization (e.g., object detection). The encoder-decoder architectures are proposed to resolve this by applying a decoder network onto a backbone model designed for classification tasks. In this paper, we argue encoder-decoder architecture is ineffective in generating strong multi-scale features because of the scale-decreased backbone. We propose SpineNet, a backbone with scale-permuted intermediate features and cross-scale connections that is learned on an object detection task by Neural Architecture Search. Using similar building blocks, SpineNet models outperform ResNet-FPN models by ~3% AP at various scales while using 10-20% fewer FLOPs. In particular, SpineNet-190 achieves 52.5% AP with a MaskR-CNN detector and achieves 52.1% AP with a RetinaNet detector on COCO for a single model without test-time augmentation, significantly outperforms prior art of detectors. SpineNet can transfer to classification tasks, achieving 5% top-1 accuracy improvement on a challenging iNaturalist fine-grained dataset. Code is at: this https URL.\n\nAuthors: Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, Xiaodan Song\n\nThumbnail art by Lucas Ferreira\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "314": "#ai #attention #transformer #deeplearning\n\nTransformers are famous for two things: Their superior performance and their insane requirements of compute and memory. This paper reformulates the attention mechanism in terms of kernel functions and obtains a linear formulation, which reduces these requirements. Surprisingly, this formulation also surfaces an interesting connection between autoregressive transformers and RNNs.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Softmax Attention & Transformers\n8:40 - Quadratic Complexity of Softmax Attention\n9:40 - Generalized Attention Mechanism\n13:45 - Kernels\n20:40 - Linear Attention\n25:20 - Experiments\n28:30 - Intuition on Linear Attention\n33:55 - Connecting Autoregressive Transformers and RNNs\n41:30 - Caveats with the RNN connection\n46:00 - More Results & Conclusion\n\nPaper: https://arxiv.org/abs/2006.16236\nWebsite: https://linear-transformers.com/\nCode: https://github.com/idiap/fast-transformers\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nTransformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \ue23b(N2) to \ue23b(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\nAuthors: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "315": "Supermasks are binary masks of a randomly initialized neural network that result in the masked network performing well on a particular task. This paper considers the problem of (sequential) Lifelong Learning and trains one Supermask per Task, while keeping the randomly initialized base network constant. By minimizing the output entropy, the system can automatically derive the Task ID of a data point at inference time and distinguish up to 2500 tasks automatically.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Catastrophic Forgetting\n5:20 - Supermasks\n9:35 - Lifelong Learning using Supermasks\n11:15 - Inference Time Task Discrimination by Entropy\n15:05 - Mask Superpositions\n24:20 - Proof-of-Concept, Task Given at Inference\n30:15 - Binary Maximum Entropy Search\n32:00 - Task Not Given at Inference\n37:15 - Task Not Given at Training\n41:35 - Ablations\n45:05 - Superfluous Neurons\n51:10 - Task Selection by Detecting Outliers\n57:40 - Encoding Masks in Hopfield Networks\n59:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.14769\nCode: https://github.com/RAIVNLab/supsup\n\nMy Video about Lottery Tickets: https://youtu.be/ZVVnvZdUMUk\nMy Video about Supermasks: https://youtu.be/jhCInVFE2sc\n\nAbstract:\nWe present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.\n\nAuthors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "316": "I take a closer look at \"Supermasks in Superposition\" after I've already done a video on it. Specifically, I look at: 1. The intuition and theoretical justification behind the G objective, 2. Whether Supermasks and Superposition can be viewed as two distinct ideas and 3. The Paper's Broader Impact Statement.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - SupSup Recap\n4:00 - In-Depth Analysis of the G Objective\n20:30 - Superposition without Supermasks\n25:40 - Broader Impact Statement\n36:40 - Conclusion\n37:20 - Live Coding\n\nPart 1 on SupSup: https://youtu.be/3jT1qJ8ETzk\nMy Code: https://colab.research.google.com/drive/1bEcppdN6qZRpEFplIiv41ZI3vDwDjcvC?usp=sharing\nPaper: https://arxiv.org/abs/2006.14769\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "317": "VAEs have been traditionally hard to train at high resolutions and unstable when going deep with many layers. In addition, VAE samples are often more blurry and less crisp than those from GANs. This paper details all the engineering choices necessary to successfully train a deep hierarchical VAE that exhibits global consistency and astounding sharpness at high resolutions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Variational Autoencoders\n8:25 - Hierarchical VAE Decoder\n12:45 - Output Samples\n15:00 - Hierarchical VAE Encoder\n17:20 - Engineering Decisions\n22:10 - KL from Deltas\n26:40 - Experimental Results\n28:40 - Appendix\n33:00 - Conclusion\n\nPaper: https://arxiv.org/abs/2007.03898\n\nAbstract:\nNormalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\u00d7256 pixels.\n\nAuthors: Arash Vahdat, Jan Kautz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "318": "Neural networks for implicit representations, such as SIRENs, have been very successful at modeling natural signals. However, in the classical approach, each data point requires its own neural network to be fit. This paper extends implicit representations to an entire dataset by introducing latent vectors of data points to SIRENs. Interestingly, the paper shows that such latent vectors can be obtained without the need for an explicit encoder, by simply looking at the negative gradient of the zero-vector through the representation function.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:10 - Implicit Generative Models\n5:30 - Implicitly Represent a Dataset\n11:00 - Gradient Origin Networks\n23:55 - Relation to Gradient Descent\n28:05 - Messing with their Code\n37:40 - Implicit Encoders\n38:50 - Using GONs as classifiers\n40:55 - Experiments & Conclusion\n\nPaper: https://arxiv.org/abs/2007.02798\nCode: https://github.com/cwkx/GON\nProject Page: https://cwkx.github.io/data/GON/\n\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\n\nAbstract:\nThis paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.\n\nAuthors: Sam Bond-Taylor, Chris G. Willcocks\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "319": "#ai #research #alexnet\n\nAlexNet was the start of the deep learning revolution. Up until 2012, the best computer vision systems relied on hand-crafted features and highly specialized algorithms to perform object classification. This paper was the first to successfully train a deep convolutional neural network on not one, but two GPUs and managed to outperform the competition on ImageNet by an order of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - The necessity of larger models\n6:20 - Why CNNs?\n11:05 - ImageNet\n12:05 - Model Architecture Overview\n14:35 - ReLU Nonlinearities\n18:45 - Multi-GPU training\n21:30 - Classification Results\n24:30 - Local Response Normalization\n28:05 - Overlapping Pooling\n32:25 - Data Augmentation\n38:30 - Dropout\n40:30 - More Results\n43:50 - Conclusion\n\nPaper: http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\n\nAbstract:\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n\nAuthors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "320": "#ai #deeplearning #gan\n\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:50 - Motivation\n8:40 - Minimax Loss Function\n13:20 - Intuition Behind the Loss\n19:30 - GAN Algorithm\n22:05 - Theoretical Analysis\n27:00 - Experiments\n33:10 - Advantages & Disadvantages\n35:00 - Conclusion\n\nPaper: https://arxiv.org/abs/1406.2661\n\nAbstract:\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "321": "#ai #research #resnet\n\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper's invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:45 - The Problem with Depth\n3:15 - VGG-Style Networks\n6:00 - Overfitting is Not the Problem\n7:25 - Motivation for Residual Connections\n10:25 - Residual Blocks\n12:10 - From VGG to ResNet\n18:50 - Experimental Results\n23:30 - Bottleneck Blocks\n24:40 - Deeper ResNets\n28:15 - More Results\n29:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1512.03385\n\nAbstract:\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "322": "#ai #research #machinelearning\n\nNeural Architecture Search is typically very slow and resource-intensive. A meta-controller has to train many hundreds or thousands of different models to find a suitable building plan. This paper proposes to use statistics of the Jacobian around data points to estimate the performance of proposed architectures at initialization. This method does not require training and speeds up NAS by orders of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n0:50 - Neural Architecture Search\n4:15 - Controller-based NAS\n7:35 - Architecture Search Without Training\n9:30 - Linearization Around Datapoints\n14:10 - Linearization Statistics\n19:00 - NAS-201 Benchmark\n20:15 - Experiments\n34:15 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04647\nCode: https://github.com/BayesWatch/nas-without-training\n\nAbstract:\nThe time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the NAS-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU. Code to reproduce our experiments is available at this https URL.\n\nAuthors: Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "323": "#ai #nlp #attention\n\nThe quadratic resource requirements of the attention mechanism are the main roadblock in scaling up transformers to long sequences. This paper replaces the full quadratic attention mechanism by a combination of random attention, window attention, and global attention. Not only does this allow the processing of longer sequences, translating to state-of-the-art experimental results, but also the paper shows that BigBird comes with theoretical guarantees of universal approximation and turing completeness.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:50 - Quadratic Memory in Full Attention\n4:55 - Architecture Overview\n6:35 - Random Attention\n10:10 - Window Attention\n13:45 - Global Attention\n15:40 - Architecture Summary\n17:10 - Theoretical Result\n22:00 - Experimental Parameters\n25:35 - Structured Block Computations\n29:30 - Recap\n31:50 - Experimental Results\n34:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2007.14062\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on Longformer: https://youtu.be/_8KNb5iqblE\n... and its memory requirements: https://youtu.be/gJR28onlqzs\n\nAbstract:\nTransformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n\nAuthors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "324": "#ai #transformer #attention\n\nHopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Binary Hopfield Networks\n5:55 - Continuous Hopfield Networks\n8:15 - Update Rules & Energy Functions\n13:30 - Connection to Transformers\n14:35 - Hopfield Attention Layers\n26:45 - Theoretical Analysis\n48:10 - Investigating BERT\n1:02:30 - Immune Repertoire Classification\n\nPaper: https://arxiv.org/abs/2008.02217\nCode: https://github.com/ml-jku/hopfield-layers\nImmune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nWe show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL\n\nAuthors: Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "325": "#ai #machinelearning #attention\n\nConvolutional Neural Networks have dominated image processing for the last decade, but transformers are quickly replacing traditional models. This paper proposes a fully attentional model for images by combining learned Positional Embeddings with Axial Attention. This new model can compete with CNNs on image classification and achieve state-of-the-art in various image segmentation tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - This Paper's Contributions\n6:20 - From Convolution to Self-Attention for Images\n16:30 - Learned Positional Embeddings\n24:20 - Propagating Positional Embeddings through Layers\n27:00 - Traditional vs Position-Augmented Attention\n31:10 - Axial Attention\n44:25 - Replacing Convolutions in ResNet\n46:10 - Experimental Results & Examples\n\nPaper: https://arxiv.org/abs/2003.07853\nCode: https://github.com/csrhddlam/axial-deeplab\n\nMy Video on BigBird: https://youtu.be/WVPE62Gk3EM\nMy Video on ResNet: https://youtu.be/GWt6Fu05voI\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nConvolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.\n\nAuthors: Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "326": "#ai #research #transformers\n\nTransformers are Ruining Convolutions. This paper, under review at ICLR, shows that given enough data, a standard Transformer can outperform Convolutional Neural Networks in image recognition tasks, which are classically tasks where CNNs excel. In this Video, I explain the architecture of the Vision Transformer (ViT), the reason why it works better and rant about why double-bline peer review is broken.\n\nOUTLINE:\n0:00 - Introduction\n0:30 - Double-Blind Review is Broken\n5:20 - Overview\n6:55 - Transformers for Images\n10:40 - Vision Transformer Architecture\n16:30 - Experimental Results\n18:45 - What does the Model Learn?\n21:00 - Why Transformers are Ruining Everything\n27:45 - Inductive Biases in Transformers\n29:05 - Conclusion & Comments\n\nPaper (Under Review): https://openreview.net/forum?id=YicbFdNTTy\nArxiv version: https://arxiv.org/abs/2010.11929\n\nBiT Paper: https://arxiv.org/pdf/1912.11370.pdf\nImageNet-ReaL Paper: https://arxiv.org/abs/2006.07159\n\nMy Video on BiT (Big Transfer): https://youtu.be/k1GOF2jmX7c\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on ResNets: https://youtu.be/GWt6Fu05voI\n\n\nAbstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to sequences of image patches. When pre-trained on large amounts of data and transferred to multiple recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc), Vision Transformer attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\nAuthors: Anonymous / Under Review\n\nErrata:\n- Patches are not flattened, but vectorized\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "327": "#ai #research #attention\n\nTransformers, having already captured NLP, have recently started to take over the field of Computer Vision. So far, the size of images as input has been challenging, as the Transformers' Attention Mechanism's memory requirements grows quadratic in its input size. LambdaNetworks offer a way around this requirement and capture long-range interactions without the need to build expensive attention maps. They reach a new state-of-the-art in ImageNet and compare favorably to both Transformers and CNNs in terms of efficiency.\n\nOUTLINE:\n0:00 - Introduction & Overview\n6:25 - Attention Mechanism Memory Requirements\n9:30 - Lambda Layers vs Attention Layers\n17:10 - How Lambda Layers Work\n31:50 - Attention Re-Appears in Lambda Layers\n40:20 - Positional Encodings\n51:30 - Extensions and Experimental Comparisons\n58:00 - Code\n\nPaper: https://openreview.net/forum?id=xTJEN-ggl1b\nLucidrains' Code: https://github.com/lucidrains/lambda-networks\n\nAbstract:\nWe present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda layer, captures such interactions by transforming available contexts into linear functions,  termed lambdas,  and applying these linear functions to each input separately.  Lambda layers are versatile and may be implemented to model content and position-based interactions in global, local or masked contexts.  As they bypass the need for expensive attention maps, lambda layers can routinely be applied to inputs of length in the thousands, en-abling their applications to long sequences or high-resolution images. The resulting neural network architectures, LambdaNetworks, are computationally efficient and simple to implement using direct calls to operations available in modern neural network libraries.  Experiments on ImageNet classification and COCO object detection  and  instance  segmentation  demonstrate  that  LambdaNetworks  significantly  outperform  their  convolutional  and  attentional  counterparts  while  being more computationally efficient. Finally, we introduce LambdaResNets, a family of LambdaNetworks, that considerably improve the speed-accuracy tradeoff of image classification models. LambdaResNets reach state-of-the-art accuracies on ImageNet while being \u223c4.5x faster than the popular EfficientNets on modern machine learning accelerators.\n\nAuthors: Anonymous\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "328": "#ai #research #attention\n\nTransformers have huge memory and compute requirements because they construct an Attention matrix, which grows quadratically in the size of the input. The Performer is a model that uses random positive orthogonal features to construct an unbiased estimator to the Attention matrix and obtains an arbitrarily good approximation in linear time! The method generalizes beyond attention and opens the door to the next generation of deep learning architectures.\n\nOUTLINE:\n0:00 - Intro & Outline\n6:15 - Quadratic Bottleneck in Attention Mechanisms\n10:00 - Decomposing the Attention Matrix\n15:30 - Approximating the Softmax Kernel\n24:45 - Different Choices, Different Kernels\n28:00 - Why the Naive Approach does not work!\n31:30 - Better Approximation via Positive Features\n36:55 - Positive Features are Infinitely Better\n40:10 - Orthogonal Features are Even Better\n43:25 - Experiments\n49:20 - Broader Impact Statement\n50:00 - Causal Attention via Prefix Sums\n52:10 - Code\n53:50 - Final Remarks & Conclusion\n\nPaper: https://arxiv.org/abs/2009.14794\nCode: https://github.com/google-research/google-research/tree/master/performer\nBlog: https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html\n\nKernels on ML Street Talk: https://www.youtube.com/watch?v=y_RjsDHl5Y4\nMy Video on Linformer: https://www.youtube.com/watch?v=-_2AF9Lhweo\nMy Video on Reformer: https://www.youtube.com/watch?v=i4H0kjxrias\nMy Video on Attention: https://www.youtube.com/watch?v=iDulhoQ2pro\n\nAbstract:\nWe introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\nAuthors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "329": "#ai #research #engineering\n\nNumerical solvers for Partial Differential Equations are notoriously slow. They need to evolve their state by tiny steps in order to stay accurate, and they need to repeat this for each new problem. Neural Fourier Operators, the architecture proposed in this paper, can evolve a PDE in time by a single forward pass, and do so for an entire family of PDEs, as long as the training set covers them well. By performing crucial operations only in Fourier Space, this new architecture is also independent of the discretization or sampling of the underlying signal and has the potential to speed up many scientific applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:15 - Navier Stokes Problem Statement\n11:00 - Formal Problem Definition\n15:00 - Neural Operator\n31:30 - Fourier Neural Operator\n48:15 - Experimental Examples\n50:35 - Code Walkthrough\n1:01:00 - Summary & Conclusion\n\nPaper: https://arxiv.org/abs/2010.08895\nBlog: https://zongyi-li.github.io/blog/2020/fourier-pde/\nCode: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_3d.py\nMIT Technology Review: https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/\n\nAbstract:\nThe classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.\n\nAuthors: Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "330": "#ai #technology #switchtransformer\n\nScale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model's parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:30 - Performance Gains from Scale\n8:30 - Switch Transformer Architecture\n17:00 - Model-, Data- and Expert-Parallelism\n25:30 - Experimental Results\n29:00 - Stabilizing Training\n32:20 - Distillation into Dense Models\n33:30 - Final Comments\n\nPaper: https://arxiv.org/abs/2101.03961\nCodebase T5: https://github.com/google-research/text-to-text-transfer-transformer\n\nAbstract:\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n\nAuthors: William Fedus, Barret Zoph, Noam Shazeer\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "331": "#ai #science #transformers\n\nAutoregressive Transformers have taken over the world of Language Modeling (GPT-3). However, in order to train them, people use causal masking and sample parallelism, which means computation only happens in a feedforward manner. This results in higher layer information, which would be available, to not be used in the lower layers of subsequent tokens, and leads to a loss in the computational capabilities of the overall model. Feedback Transformers trade-off training speed for access to these representations and demonstrate remarkable improvements in complex reasoning and long-range dependency tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems of Autoregressive Processing\n3:30 - Information Flow in Recurrent Neural Networks\n7:15 - Information Flow in Transformers\n9:10 - Solving Complex Computations with Neural Networks\n16:45 - Causal Masking in Transformers\n19:00 - Missing Higher Layer Information Flow\n26:10 - Feedback Transformer Architecture\n30:00 - Connection to Attention-RNNs\n36:00 - Formal Definition\n37:05 - Experimental Results\n43:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2002.09402\n\nMy video on Attention: https://youtu.be/iDulhoQ2pro\n\nERRATA: Sometimes I say \"Switch Transformer\" instead of \"Feedback Transformer\". Forgive me :)\n\nAbstract:\nTransformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\n\nAuthors: Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "332": "#nfnets #deepmind #machinelearning\n\nBatch Normalization is a core component of modern deep learning. It enables training at higher batch sizes, prevents mean shift, provides implicit regularization, and allows networks to reach higher performance than without. However, BatchNorm also has disadvantages, such as its dependence on batch size and its computational overhead, especially in distributed settings. Normalizer-Free Networks, developed at Google DeepMind, are a class of CNNs that achieve state-of-the-art classification accuracy on ImageNet without batch normalization. This is achieved by using adaptive gradient clipping (AGC), combined with a number of improvements in general network architecture. The resulting networks train faster, are more accurate, and provide better transfer learning performance. Code is provided in Jax.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:40 - What's the problem with BatchNorm?\n11:00 - Paper contribution Overview\n13:30 - Beneficial properties of BatchNorm\n15:30 - Previous work: NF-ResNets\n18:15 - Adaptive Gradient Clipping\n21:40 - AGC and large batch size\n23:30 - AGC induces implicit dependence between training samples\n28:30 - Are BatchNorm's problems solved?\n30:00 - Network architecture improvements\n31:10 - Comparison to EfficientNet\n33:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.06171\nCode: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nMy Video on BatchNorm: https://www.youtube.com/watch?v=OioFONrSETc\nMy Video on ResNets: https://www.youtube.com/watch?v=GWt6Fu05voI\n\nERRATA (from Lucas Beyer): \"I believe you missed the main concern with \"batch cheating\". It's for losses that act on the full batch, as opposed to on each sample individually.\nFor example, triplet in FaceNet or n-pairs in CLIP. BN allows for \"shortcut\" solution to loss. See also BatchReNorm paper.\"\n\nAbstract:\nBatch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. Our code is available at this https URL deepmind-research/tree/master/nfnets\n\nAuthors: Andrew Brock, Soham De, Samuel L. Smith, Karen Simonyan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "333": "#transformer #gan #machinelearning\n\nGenerative Adversarial Networks (GANs) hold the state-of-the-art when it comes to image generation. However, while the rest of computer vision is slowly taken over by transformers or other attention-based architectures, all working GANs to date contain some form of convolutional layers. This paper changes that and builds TransGAN, the first GAN where both the generator and the discriminator are transformers. The discriminator is taken over from ViT (an image is worth 16x16 words), and the generator uses pixelshuffle to successfully up-sample the generated resolution. Three tricks make training work: Data augmentations using DiffAug, an auxiliary superresolution task, and a localized initialization of self-attention. Their largest model reaches competitive performance with the best convolutional GANs on CIFAR10, STL-10, and CelebA.\n\nOUTLINE:\n0:00 - Introduction & Overview\n3:05 - Discriminator Architecture\n5:25 - Generator Architecture\n11:20 - Upsampling with PixelShuffle\n15:05 - Architecture Recap\n16:00 - Vanilla TransGAN Results\n16:40 - Trick 1: Data Augmentation with DiffAugment\n19:10 - Trick 2: Super-Resolution Co-Training\n22:20 - Trick 3: Locality-Aware Initialization for Self-Attention\n27:30 - Scaling Up & Experimental Results\n28:45 - Recap & Conclusion\n\nPaper: https://arxiv.org/abs/2102.07074\nCode: https://github.com/VITA-Group/TransGAN\nMy Video on ViT: https://youtu.be/TrdevFK_am4\n\nAbstract:\nThe recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \\textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \\textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \\textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA 64\u00d764, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at \\url{this https URL}.\n\nAuthors: Yifan Jiang, Shiyu Chang, Zhangyang Wang\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "334": "#fastweights #deeplearning #transformers\n\nTransformers are dominating Deep Learning, but their quadratic memory and compute requirements make them expensive to train and hard to use. Many papers have attempted to linearize the core module: the attention mechanism, using kernels - for example, the Performer. However, such methods are either not satisfactory or have other downsides, such as a reliance on random features. This paper establishes an intrinsic connection between linearized (kernel) attention and the much older Fast Weight Memory Systems, in part popularized by J\u00fcrgen Schmidhuber in the 90s. It shows the fundamental limitations of these algorithms and suggests new update rules and new kernels in order to fix these problems. The resulting model compares favorably to Performers on key synthetic experiments and real-world tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Fast Weight Systems\n7:00 - Distributed Storage of Symbolic Values\n12:30 - Autoregressive Attention Mechanisms\n18:50 - Connecting Fast Weights to Attention Mechanism\n22:00 - Softmax as a Kernel Method (Performer)\n25:45 - Linear Attention as Fast Weights\n27:50 - Capacity Limitations of Linear Attention\n29:45 - Synthetic Data Experimental Setup\n31:50 - Improving the Update Rule\n37:30 - Deterministic Parameter-Free Projection (DPFP) Kernel\n46:15 - Experimental Results\n50:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.11174\nCode: https://github.com/ischlag/fast-weight-transformers\nMachine Learning Street Talk on Kernels: https://youtu.be/y_RjsDHl5Y4\n\nAbstract:\nWe show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early '90s. From this observation we infer a memory capacity limitation of recent linearised softmax attention variants. With finite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.\n\nAuthors: Imanol Schlag, Kazuki Irie, J\u00fcrgen Schmidhuber\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "335": "#glom #hinton #capsules\n\nGeoffrey Hinton describes GLOM, a Computer Vision model that combines transformers, neural fields, contrastive learning, capsule networks, denoising autoencoders and RNNs. GLOM decomposes an image into a parse tree of objects and their parts. However, unlike previous systems, the parse tree is constructed dynamically and differently for each input, without changing the underlying neural network. This is done by a multi-step consensus algorithm that runs over different levels of abstraction at each location of an image simultaneously. GLOM is just an idea for now but suggests a radically new approach to AI visual scene understanding.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Object Recognition as Parse Trees\n5:40 - Capsule Networks\n8:00 - GLOM Architecture Overview\n13:10 - Top-Down and Bottom-Up communication\n18:30 - Emergence of Islands\n22:00 - Cross-Column Attention Mechanism\n27:10 - My Improvements for the Attention Mechanism\n35:25 - Some Design Decisions\n43:25 - Training GLOM as a Denoising Autoencoder & Contrastive Learning\n52:20 - Coordinate Transformations & Representing Uncertainty\n57:05 - How GLOM handles Video\n1:01:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.12627\n\nAbstract:\nThis paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language\n\nAuthors: Geoffrey Hinton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "336": "#perceiver #deepmind #transformer\n\nInspired by the fact that biological creatures attend to multiple modalities at the same time, DeepMind releases its new Perceiver model. Based on the Transformer architecture, the Perceiver makes no assumptions on the modality of the input data and also solves the long-standing quadratic bottleneck problem. This is achieved by having a latent low-dimensional Transformer, where the input data is fed multiple times via cross-attention. The Perceiver's weights can also be shared across layers, making it very similar to an RNN. Perceivers achieve competitive performance on ImageNet and state-of-the-art on other modalities, all while making no architectural adjustments to input data.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:20 - Built-In assumptions of Computer Vision Models\n5:10 -  The Quadratic Bottleneck of Transformers\n8:00 - Cross-Attention in Transformers\n10:45 - The Perceiver Model Architecture & Learned Queries\n20:05 - Positional Encodings via Fourier Features\n23:25 - Experimental Results & Attention Maps\n29:05 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2103.03206\n\nMy Video on Transformers (Attention is All You Need): https://youtu.be/iDulhoQ2pro\n\nAbstract:\nBiological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.\n\nAuthors: Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "337": "#mixer #google #imagenet\n\nConvolutional Neural Networks have dominated computer vision for nearly 10 years, and that might finally come to an end. First, Vision Transformers (ViT) have shown remarkable performance, and now even simple MLP-based models reach competitive accuracy, as long as sufficient data is used for pre-training. This paper presents MLP-Mixer, using MLPs in a particular weight-sharing arrangement to achieve a competitive, high-throughput model and it raises some interesting questions about the nature of learning and inductive biases and their interaction with scale for future research.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:20 - MLP-Mixer Architecture\n13:20 - Experimental Results\n17:30 - Effects of Scale\n24:30 - Learned Weights Visualization\n27:25 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2105.01601\n\nAbstract:\nConvolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. \"mixing\" the per-location features), and one with MLPs applied across patches (i.e. \"mixing\" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.\n\nAuthors: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy\n\nERRATA: Here is their definition of what the 5-shot classifier is: \"we report the few-shot accuracies obtained by solving the L2-regularized linear regression problem between the frozen learned representations of images and the labels\"\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "338": "#involution #computervision #attention\n\nConvolutional Neural Networks (CNNs) have dominated computer vision for almost a decade by applying two fundamental principles: Spatial agnosticism and channel-specific computations. Involution aims to invert these principles and presents a spatial-specific computation, which is also channel-agnostic. The resulting Involution Operator and RedNet architecture are a compromise between classic Convolutions and the newer Local Self-Attention architectures and perform favorably in terms of computation accuracy tradeoff when compared to either.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Principles of Convolution\n10:50 - Towards spatial-specific computations\n17:00 - The Involution Operator\n20:00 - Comparison to Self-Attention\n25:15 - Experimental Results\n30:30 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2103.06255\nCode: https://github.com/d-li14/involution\n\nAbstract:\nConvolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL.\n\nAuthors: Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "339": "#expirespan #nlp #facebookai\n\nFacebook AI (FAIR) researchers present Expire-Span, a variant of Transformer XL that dynamically assigns expiration dates to previously encountered signals. Because of this, Expire-Span can handle sequences of many thousand tokens, while keeping the memory and compute requirements at a manageable level. It severely matches or outperforms baseline systems, while consuming much less resources. We discuss its architecture, advantages, and shortcomings.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Remembering the past in sequence models\n5:45 - Learning to expire past memories\n8:30 - Difference to local attention\n10:00 - Architecture overview\n13:45 - Comparison to Transformer XL\n18:50 - Predicting expiration masks\n32:30 - Experimental Results\n40:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2105.06548\nCode: https://github.com/facebookresearch/transformer-sequential\n\nADDENDUM: I mention several times that the gradient signal of the e quantity only occurs inside the R ramp. By that, I mean the gradient stemming from the model loss. The regularization loss acts also outside the R ramp.\n\nAbstract:\nAttention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.\n\nAuthors: Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, Angela Fan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "340": "#attention #transformer #fastformer\n\nTransformers have become the dominant model class in the last few years for large data, but their quadratic complexity in terms of sequence length has plagued them until now. Fastformer claims to be the fastest and most performant linear attention variant, able to consume long contexts at once. This is achieved by a combination of additive attention and elementwise products. While initial results look promising, I have my reservations...\n\nOUTLINE:\n0:00 - Intro & Outline\n2:15 - Fastformer description\n5:20 - Baseline: Classic Attention\n10:00 - Fastformer architecture\n12:50 - Additive Attention\n18:05 - Query-Key element-wise multiplication\n21:35 - Redundant modules in Fastformer\n25:00 - Problems with the architecture\n27:30 - Is this even attention?\n32:20 - Experimental Results\n34:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2108.09084\n\nAbstract:\nTransformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.\n\nAuthors: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "341": "#machinelearning #ardm #generativemodels\n\nDiffusion models have made large advances in recent months as a new type of generative models. This paper introduces Autoregressive Diffusion Models (ARDMs), which are a mix between autoregressive generative models and diffusion models. ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time. This paper applies ARDMs to both text and image data, and as an extension, the models can also be used to perform lossless compression.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Decoding Order in Autoregressive Models\n6:15 - Autoregressive Diffusion Models\n8:35 - Dependent and Independent Sampling\n14:25 - Application to Character-Level Language Models\n18:15 - How Sampling & Training Works\n26:05 - Extension 1: Parallel Sampling\n29:20 - Extension 2: Depth Upscaling\n33:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2110.02037\n\nAbstract:\nWe introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.\n\nAuthors: Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "342": "#scalingtransformers #terraformer #sparsity\n\nTransformers keep pushing the state of the art in language and other domains, mainly due to their ability to scale to ever more parameters. However, this scaling has made it prohibitively expensive to run a lot of inference requests against a Transformer, both in terms of compute and memory requirements. Scaling Transformers are a new kind of architecture that leverage sparsity in the Transformer blocks to massively speed up inference, and by including additional ideas from other architectures, they create the Terraformer, which is both fast, accurate, and consumes very little memory.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Recap: Transformer stack\n6:55 - Sparse Feedforward layer\n19:20 - Sparse QKV Layer\n43:55 - Terraformer architecture\n55:05 - Experimental Results & Conclusion\n\nPaper: https://arxiv.org/abs/2111.12763\nCode: https://github.com/google/trax/blob/master/trax/examples/Terraformer_from_scratch.ipynb\n\nAbstract:\nLarge Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.\n\nAuthors: Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, \u0141ukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, Jonni Kanerva\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "343": "#nuwa #microsoft #generative\n\nN\u00dcWA is a unifying architecture that can ingest text, images, and videos and brings all of them into a quantized latent representation to support a multitude of visual generation tasks, such as text-to-image, text-guided video manipulation, or sketch-to-video. This paper details how the encoders for the different modalities are constructed, and how the latent representation is transformed using their novel 3D nearby self-attention layers. Experiments are shown on 8 different visual generation tasks that the model supports.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:20 - Sponsor: ClearML\n3:35 - Tasks & Naming\n5:10 - The problem with recurrent image generation\n7:35 - Creating a shared latent space w/ Vector Quantization\n23:20 - Transforming the latent representation\n26:25 - Recap: Self- and Cross-Attention\n28:50 - 3D Nearby Self-Attention\n41:20 - Pre-Training Objective\n46:05 - Experimental Results\n50:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2111.12417\nGithub: https://github.com/microsoft/NUWA\n\nSponsor: ClearML\nhttps://clear.ml\n\nAbstract:\nThis paper presents a unified multimodal pre-trained model called N\u00dcWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate N\u00dcWA on 8 downstream tasks. Compared to several strong baselines, N\u00dcWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.\n\nAuthors: Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "344": "#deeplearning #noether #symmetries\n\nThis video includes an interview with first author Ferran Alet!\nEncoding inductive biases has been a long established methods to provide deep networks with the ability to learn from less data. Especially useful are encodings of symmetry properties of the data, such as the convolution's translation invariance. But such symmetries are often hard to program explicitly, and can only be encoded exactly when done in a direct fashion. Noether Networks use Noether's theorem connecting symmetries to conserved quantities and are able to dynamically and approximately enforce symmetry properties upon deep neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n18:10 - Interview Start\n21:20 - Symmetry priors vs conserved quantities\n23:25 - Example: Pendulum\n27:45 - Noether Network Model Overview\n35:35 - Optimizing the Noether Loss\n41:00 - Is the computation graph stable?\n46:30 - Increasing the inference time computation\n48:45 - Why dynamically modify the model?\n55:30 - Experimental Results & Discussion\n\nPaper: https://arxiv.org/abs/2112.03321\nWebsite: https://dylandoblar.github.io/noether-networks/\nCode: https://github.com/dylandoblar/noether-networks\n\nAbstract:\nProgress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.\n\nAuthors: Ferran Alet, Dylan Doblar, Allan Zhou, Joshua Tenenbaum, Kenji Kawaguchi, Chelsea Finn\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "345": "#deeplearning #neuralinterpreter #ai\n\nThis video includes an interview with the paper's authors!\nWhat if we treated deep networks like modular programs? Neural Interpreters divide computation into small modules and route data to them via a dynamic type inference system. The resulting model combines recurrent elements, weight sharing, attention, and more to tackle both abstract reasoning, as well as computer vision tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Model Overview\n7:00 - Interpreter weights and function code\n9:40 - Routing data to functions via neural type inference\n14:55 - ModLin layers\n18:25 - Experiments\n21:35 - Interview Start\n24:50 - General Model Structure\n30:10 - Function code and signature\n40:30 - Explaining Modulated Layers\n49:50 - A closer look at weight sharing\n58:30 - Experimental Results\n\nPaper: https://arxiv.org/abs/2110.06399\n\nGuests:\nNasim Rahaman: https://twitter.com/nasim_rahaman\nFrancesco Locatello: https://twitter.com/FrancescoLocat8\nWaleed Gondal: https://twitter.com/Wallii_gondal\n\nAbstract:\nModern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \\emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization\n\nAuthors: Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, Bernhard Sch\u00f6lkopf\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "346": "#hypertransformer #metalearning #deeplearning\n\nThis video contains a paper explanation and an interview with author Andrey Zhmoginov!\nFew-shot learning is an interesting sub-field in meta-learning, with wide applications, such as creating personalized models based on just a handful of data points. Traditionally, approaches have followed the BERT approach where a large model is pre-trained and then fine-tuned. However, this couples the size of the final model to the size of the model that has been pre-trained. Similar problems exist with \"true\" meta-learners, such as MaML. HyperTransformer fundamentally decouples the meta-learner from the size of the final model by directly predicting the weights of the final model. The HyperTransformer takes the few-shot dataset as a whole into its context and predicts either one or multiple layers of a (small) ConvNet, meaning its output are the weights of the convolution filters. Interestingly, and with the correct engineering care, this actually appears to deliver promising results and can be extended in many ways.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:05 - Weight-generation vs Fine-tuning for few-shot learning\n10:10 - HyperTransformer model architecture overview\n22:30 - Why the self-attention mechanism is useful here\n34:45 - Start of Interview\n39:45 - Can neural networks even produce weights of other networks?\n47:00 - How complex does the computational graph get?\n49:45 - Why are transformers particularly good here?\n58:30 - What can the attention maps tell us about the algorithm?\n1:07:00 - How could we produce larger weights?\n1:09:30 - Diving into experimental results\n1:14:30 - What questions remain open?\n\nPaper: https://arxiv.org/abs/2201.04182\n\nERRATA: I introduce Max Vladymyrov as Mark Vladymyrov\n\nAbstract:\nIn this work we propose a HyperTransformer, a transformer-based model for few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable. Finally, we extend our approach to a semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance.\n\nAuthors: Andrey Zhmoginov, Mark Sandler, Max Vladymyrov\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "347": "#cm3 #languagemodel #transformer\n\nThis video contains a paper explanation and an incredibly informative interview with first author Armen Aghajanyan.\nAutoregressive Transformers have come to dominate many fields in Machine Learning, from text generation to image creation and many more. However, there are two problems. First, the collected data is usually scraped from the web and uni- or bi-modal and throws away a lot of structure of the original websites, and second, language modelling losses are uni-directional. CM3 addresses both problems: It directly operates on HTML and includes text, hyperlinks, and even images (via VQGAN tokenization) and can therefore be used in plenty of ways: Text generation, captioning, image creation, entity linking, and much more. It also introduces a new training strategy called Causally Masked Language Modelling, which brings a level of bi-directionality into autoregressive language modelling. In the interview after the paper explanation, Armen and I go deep into the how and why of these giant models, we go over the stunning results and we make sense of what they mean for the future of universal models.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:30 - Directly learning the structure of HTML\n12:30 - Causally Masked Language Modelling\n18:50 - A short look at how to use this model\n23:20 - Start of interview\n25:30 - Feeding language models with HTML\n29:45 - How to get bi-directionality into decoder-only Transformers?\n37:00 - Images are just tokens\n41:15 - How does one train such giant models?\n45:40 - CM3 results are amazing\n58:20 - Large-scale dataset collection and content filtering\n1:04:40 - More experimental results\n1:12:15 - Why don't we use raw HTML?\n1:18:20 - Does this paper contain too many things?\n\nPaper: https://arxiv.org/abs/2201.07520\n\nAbstract:\nWe introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.\n\nAuthors: Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "348": "#blip #review #ai\n\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\n\nSponsor: Zeta Alpha\nhttps://zeta-alpha.com\nUse code YANNIC for 20% off!\n\nOUTLINE:\n0:00 - Intro\n0:50 - Sponsor: Zeta Alpha\n3:40 - Paper Overview\n6:40 - Vision-Language Pre-Training\n11:15 - Contributions of the paper\n14:30 - Model architecture: many parts for many tasks\n19:50 - How data flows in the model\n26:50 - Parameter sharing between the modules\n29:45 - Captioning & Filtering bootstrapping\n41:10 - Fine-tuning the model for downstream tasks\n\nPaper: https://arxiv.org/abs/2201.12086\nCode: https://github.com/salesforce/BLIP\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\n\nAbstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\n\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "349": "Twitter: https://twitter.com/zaptiee\nSecond Channel: https://www.youtube.com/channel/UCCAfRoTJrKPbSrh_Eg3i4vg\nLast Video: https://www.youtube.com/watch?v=_7rYU4K4TFA\n\nPeople Who Helped\n\nResearch:\nBernie: https://www.youtube.com/channel/UCU4VFB2dYNUTf-8tnuM16GA\n\nAdditional Contributions:\nJust Jargon: https://www.youtube.com/channel/UCfNZgLc7Cxc2GxlfpEj-AEA\nJoon The King: https://www.youtube.com/channel/UCYMbbNRzlXOhQHBB6EoudZg\n\nEdited Intro by: \nBiodegradable: https://twitter.com/BiodeEditable\n\nNarration audio edited by:\nZwick The Editor: https://twitter.com/dude2o\n\nCharacter art by:\nFapping Flamingo: https://twitter.com/FappingFlamingo\nNicky Kitsune:  https://twitter.com/InsaNicky\n\nTrack Listing:\nhttps://pastebin.com/2Lnv2KGi", "350": "#blip #interview #salesforce\n\nPaper Review Video: https://youtu.be/X2k7n4FuI7c\nSponsor: Assembly AI\nhttps://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic2\n\nThis is an interview with Junnan Li and Dongxu Li, authors of BLIP and members of Salesforce research.\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Assembly AI\n1:30 - Start of Interview\n2:30 - What's the pitch?\n4:40 - How did data bootstrapping come into the project?\n7:10 - How big of a problem is data quality?\n11:10 - Are the captioning & filtering models biased towards COCO data?\n14:40 - Could the data bootstrapping be done multiple times?\n16:20 - What was the evolution of the BLIP architecture?\n21:15 - Are there additional benefits to adding language modelling?\n23:50 - Can we imagine a modular future for pre-training?\n29:45 - Diving into the experimental results\n42:40 - What did and did not work out during the research?\n45:00 - How is research life at Salesforce?\n46:45 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2201.12086\nCode: https://github.com/salesforce/BLIP\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\n\nAbstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\n\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "351": "#nlp #sparsity #transformers\n\nThis video is an interview with Barret Zoph and William Fedus of Google Brain about Sparse Expert Models.\nSparse Expert models have been hugely successful at distributing parts of models, mostly Transformers, across large array of machines and use a routing function to effectively route signals between them. This means that even though these models have a huge number of parameters, the computational load for a given signal does not increase because the model is only sparsely activated. Sparse expert models, such as Switch Transformers and GLAM can scale up to trillions of parameters and bring a number of desirable properties. We discuss everything from the fundamentals, history, strengths and weaknesses, up to the current state of the art of these models.\n\nOUTLINE:\n0:00 - Intro\n0:30 - What are sparse expert models?\n4:25 - Start of Interview\n5:55 - What do you mean by sparse experts?\n8:10 - How does routing work in these models?\n12:10 - What is the history of sparse experts?\n14:45 - What does an individual expert learn?\n19:25 - When are these models appropriate?\n22:30 - How comparable are sparse to dense models?\n26:30 - How does the pathways system connect to this?\n28:45 - What improvements did GLAM make?\n31:30 - The \"designing sparse experts\" paper\n37:45 - Can experts be frozen during training?\n41:20 - Can the routing function be improved?\n47:15 - Can experts be distributed beyond data centers?\n50:20 - Are there sparse experts for other domains than NLP?\n52:15 - Are sparse and dense models in competition?\n53:35 - Where do we go from here?\n56:30 - How can people get started with this?\n\nPapers:\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (https://arxiv.org/abs/2101.03961)\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts (https://arxiv.org/abs/2112.06905)\nDesigning Effective Sparse Expert Models (https://arxiv.org/abs/2202.08906)\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "352": "#gpt4 #rwkv #transformer \n\nWe take a look at RWKV, a highly scalable architecture between Transformers and RNNs.\n\nFully Connected (June 7th in SF) Promo Link: https://www.fullyconnected.com/?promo=ynnc\n\nOUTLINE:\n0:00 - Introduction\n1:50 - Fully Connected In-Person Conference in SF June 7th\n3:00 - Transformers vs RNNs\n8:00 - RWKV: Best of both worlds\n12:30 - LSTMs\n17:15 - Evolution of RWKV's Linear Attention\n30:40 - RWKV's Layer Structure\n49:15 - Time-Parallel vs Sequence Mode\n53:55 - Experimental Results & Limitations\n58:00 - Visualizations\n1:01:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.13048\nCode: https://github.com/BlinkDL/RWKV-LM\n\nAbstract:\nTransformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.\n\nAuthors: Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "353": "Here's a lazy way to separate computation and subsequent analysis in a notebook without the overhead of manually saving local variables.\n\nWARNING: Don't do this in a serious project.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "354": "Huggingface released its newest library called NLP, which gives you easy access to almost any NLP dataset and metric in one convenient interface. We will combine this with a BERT model from Huggingface's Transformers library to build a sentiment classifier for IMDB.\n\nOUTLINE:\n0:00 - Intro\n1:30 - Boilerplate\n3:20 - PyTorch Lightning Module\n9:50 - Load Dataset\n12:15 - Tokenization\n20:50 - Torch Tensors\n25:50 - Data Loader\n28:00 - Create BERT Model\n32:00 - Implement Validation and Train Step\n47:00 - Run & Recap\n50:20 - Epilogue\n\nMy Code: https://github.com/yk/huggingface-nlp-demo\nNLP Library: https://github.com/huggingface/nlp\nTutorial Colab: https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb\nTransformers Library: https://github.com/huggingface/transformers\nPytorch Lightning: https://github.com/PyTorchLightning/pytorch-lightning\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "355": "Has the world overfitted to ImageNet? What if we collect another dataset in exactly the same fashion? This paper gives a surprising answer!\n\nPaper: https://arxiv.org/abs/1902.10811\nData: https://github.com/modestyachts/ImageNetV2\n\nAbstract:\nWe build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets.\n\nAuthors: Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "356": "This generative model for music can make entire songs with remarkable quality and consistency. It can be conditioned on genre, artist, and even lyrics.\n\nBlog: https://openai.com/blog/jukebox/\nPaper: https://cdn.openai.com/papers/jukebox.pdf\nCode: https://github.com/openai/jukebox/\n\nAbstract:\nWe introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.\n\nAuthors: Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "357": "Answering complex questions about tabular information is hard. No two tables are alike and sometimes the answer you're looking for is not even in the table and needs to be computed from a subset of the cells. Surprisingly, this model can figure it all out by itself through some clever input encoding and loss engineering.\n\nPaper: https://arxiv.org/abs/2004.02349\nCode: https://github.com/google-research/tapas\n\nAbstract:\nAnswering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.\n\nAuthors: Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Martin Eisenschlos\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "358": "This ONE SIMPLE TRICK can take a vanilla RL algorithm to achieve state-of-the-art. What is it? Simply augment your training data before feeding it to the learner! This can be dropped into any RL pipeline and promises big improvements across the board.\n\nPaper: https://arxiv.org/abs/2004.14990\nCode: https://www.github.com/MishaLaskin/rad\n\nAbstract:\nLearning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at this https URL.\n\nAuthors: Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "359": "When AI makes a plan it usually does so step by step, forward in time. But often it is beneficial to define intermediate goals to divide a large problem into easier sub-problems. This paper proposes a generalization of MCTS that searches not for the best next actions to take, but for the best way to sub-divide the problem recursively into problems so tiny that they can each be solved in a single step.\n\nPaper: https://arxiv.org/abs/2004.11410\nSite: https://sites.google.com/view/dc-mcts/home\n\nAbstract:\nStandard planners for sequential decision making (including Monte Carlo planning, tree search, dynamic programming, etc.) are constrained by an implicit sequential planning assumption: The order in which a plan is constructed is the same in which it is executed. We consider alternatives to this assumption for the class of goal-directed Reinforcement Learning (RL) problems. Instead of an environment transition model, we assume an imperfect, goal-directed policy. This low-level policy can be improved by a plan, consisting of an appropriate sequence of sub-goals that guide it from the start to the goal state. We propose a planning algorithm, Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS), for approximating the optimal plan by means of proposing intermediate sub-goals which hierarchically partition the initial tasks into simpler ones that are then solved independently and recursively. The algorithm critically makes use of a learned sub-goal proposal for finding appropriate partitions trees of new tasks based on prior experience. Different strategies for learning sub-goal proposals give rise to different planning strategies that strictly generalize sequential planning. We show that this algorithmic flexibility over planning order leads to improved results in navigation tasks in grid-worlds as well as in challenging continuous control environments.\n\nAuthors: Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B. Hamrick, Nicolas Heess, Alexander Neitz, Theophane Weber\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "360": "One CNN to rule them all! BiT is a pre-trained ResNet that can be used as a starting point for any visual task. This paper explains what it takes to pre-train such a large model and details how fine-tuning on downstream tasks is done best.\n\nPaper: https://arxiv.org/abs/1912.11370\nCode & Models: TBA\n\nAbstract:\nTransfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.\n\nAuthors: Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "361": "This is a hard paper! Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities.\n\nPaper: https://arxiv.org/abs/1811.02486\nBlog: https://openai.com/blog/learning-concepts-with-energy-functions/\nVideos: https://sites.google.com/site/energyconceptmodels/\n\nAbstract:\nMany hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at this http URL\n\nAuthors: Igor Mordatch\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "362": "The dirty little secret of Batch Normalization is its intrinsic dependence on the training batch size. Group Normalization attempts to achieve the benefits of normalization without batch statistics and, most importantly, without sacrificing performance compared to Batch Normalization.\n\nhttps://arxiv.org/abs/1803.08494\n\nAbstract:\nBatch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.\n\nAuthors: Yuxin Wu, Kaiming He\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "363": "CPUs are often bottlenecks in Machine Learning pipelines. Data fetching, loading, preprocessing and augmentation can be slow to a point where the GPUs are mostly idle. Data Echoing is a technique to re-use data that is already in the pipeline to reclaim this idle time and keep the GPUs busy at all times.\n\nhttps://arxiv.org/abs/1907.05550\n\nAbstract:\nIn the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce \"data echoing,\" which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or \"echoes\") intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.\n\nAuthors: Dami Choi, Alexandre Passos, Christopher J. Shallue, George E. Dahl\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "364": "It's common for neural networks to include data normalization such as BatchNorm or GroupNorm. This paper extends the normalization to also include the weights of the network. This surprisingly simple change leads to a boost in performance and - combined with GroupNorm - new state-of-the-art results.\n\nhttps://arxiv.org/abs/1903.10520\n\nAbstract:\nIn this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: this https URL.\n\nAuthors: Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "365": "Does self-supervision really need a lot of data? How low can you go? This paper shows that a single image is enough to learn the lower layers of a deep neural network. Interestingly, more data does not appear to help as long as enough data augmentation is applied.\n\nOUTLINE:\n0:00 - Overview\n1:40 - What is self-supervision\n4:20 - What does this paper do\n7:00 - Linear probes\n11:15 - Linear probe results\n17:10 - Results\n22:25 - Learned Features\n\nhttps://arxiv.org/abs/1904.13132\n\nAbstract:\nWe look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.\n\nAuthors: Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi\n\nThumbnail Image: https://commons.wikimedia.org/wiki/File:Golden_Gate_Bridge_during_blue_hour_(16_x_10).jpg\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "366": "Gradient-based Meta-Learning requires full backpropagation through the inner optimization procedure, which is a computational nightmare. This paper is able to circumvent this and implicitly compute meta-gradients by the clever introduction of a quadratic regularizer.\n\nOUTLINE:\n0:00 - Intro\n0:15 - What is Meta-Learning?\n9:05 - MAML vs iMAML\n16:35 - Problem Formulation\n19:15 - Proximal Regularization\n26:10 - Derivation of the Implicit Gradient\n40:55 - Intuition why this works\n43:20 - Full Algorithm\n47:40 - Experiments\n\nPaper: https://arxiv.org/abs/1909.04630\nBlog Post: https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/\n\nAbstract:\nA core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.\n\nAuthors: Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "367": "Neural Networks often draw hard boundaries in high-dimensional space, which makes them very brittle. Mixup is a technique that linearly interpolates between data and labels at training time and achieves much smoother and more regular class boundaries.\n\nOUTLINE:\n0:00 - Intro\n0:30 - The problem with ERM\n2:50 - Mixup\n6:40 - Code\n9:35 - Results\n\nhttps://arxiv.org/abs/1710.09412\n\nAbstract:\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\n\nAuthors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "368": "What can an agent do without any reward? Explore the world! While many formulations of intrinsic rewards exist (Curiosity, Novelty, etc.), they all look back in time to learn. Plan2Explore is the first model that uses planning in a learned imaginary latent world model to seek out states where it is uncertain about what will happen.\n\nOUTLINE:\n0:00 - Intro & Problem Statement\n3:30 - Model\n5:10 - Intrinsic Motivation\n9:05 - Planning in Latent Space\n11:15 - Latent Disagreement\n16:30 - Maximizing Information Gain\n21:00 - More problems with the model\n26:45 - Experiments\n32:10 - Final Comments\n\nPaper: https://arxiv.org/abs/2005.05960\nWebsite: https://ramanans1.github.io/plan2explore/\nCode: https://github.com/ramanans1/plan2explore\n\nAbstract:\nReinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at this https URL\n\nAuthors: Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "369": "Can you peek into people's brains? Reading human thoughts is a long-standing dream of the AI field. This paper reads fMRI signals from a person and then reconstructs what that person's eyes currently see. This is achieved by translating the fMRI signal to features of a Deep Neural Network and then iteratively optimizing the input of the network to match those features. The results are impressive.\n\nOUTLINE:\n0:00 - Overview\n1:35 - Pipeline\n4:00 - Training\n5:20 - Image Reconstruction\n7:00 - Deep Generator Network\n8:15 - Results\n\nPaper: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633\nMy Video on OpenAI Microscope (what I called Atlas): https://youtu.be/Ok44otx90D4\n\nAbstract:\nThe mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.\n\nAuthors: Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "370": "Why are humans so good at video games? Maybe it's because a lot of games are designed with humans in mind. What happens if we change that? This paper removes the influence of human priors from a game and ends up with a pretty fun experience.\n\nPaper: https://arxiv.org/abs/1802.10217\nWebsite: https://rach0012.github.io/humanRL_website/\nCode: https://github.com/rach0012/humanRL_prior_games\n\nAbstract:\nWhat makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at this https URL\n\nAuthors: Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, Alexei A. Efros\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "371": "BERT is a giant model. Turns out you can prune away many of its components and it still works. This paper analyzes BERT pruning in light of the Lottery Ticket Hypothesis and finds that even the \"bad\" lottery tickets can be fine-tuned to good accuracy.\n\nOUTLINE:\n0:00 - Overview\n1:20 - BERT\n3:20 - Lottery Ticket Hypothesis\n13:00 - Paper Abstract\n18:00 - Pruning BERT\n23:00 - Experiments\n50:00 - Conclusion\n\nhttps://arxiv.org/abs/2005.00561\n\nML Street Talk Channel: https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ\n\nAbstract:\nMuch of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the \"bad\" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the \"good\" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the \"good\" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.\n\nAuthors: Sai Prasanna, Anna Rogers, Anna Rumshisky\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "372": "Can you plan with a learned model of the world? Yes, but there's a catch: The better your planning algorithm is, the more the errors of your world model will hurt you! This paper solves this problem by regularizing the planning algorithm to stay in high probability regions, given its experience.\n\nhttps://arxiv.org/abs/1903.11981\n\nInterview w/ Harri: https://youtu.be/HnZDmxYnpg4\n\nAbstract:\nTrajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.\n\nAuthors: Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "373": "Object detection in images is a notoriously hard task! Objects can be of a wide variety of classes, can be numerous or absent, they can occlude each other or be out of frame. All of this makes it even more surprising that the architecture in this paper is so simple. Thanks to a clever loss function, a single Transformer stacked on a CNN is enough to handle the entire task!\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:50 - Problem Formulation\n2:30 - Architecture Overview\n6:20 - Bipartite Match Loss Function\n15:55 - Architecture in Detail\n25:00 - Object Queries\n31:00 - Transformer Properties\n35:40 - Results\n\nERRATA:\nWhen I introduce bounding boxes, I say they consist of x and y, but you also need the width and height.\n\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\n\nPaper: https://arxiv.org/abs/2005.12872\nBlog: https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/\nCode: https://github.com/facebookresearch/detr\n\nAbstract:\nWe present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.\n\nAuthors: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "374": "#gpt3 #openai #gpt-3\n\nHow far can you go with ONLY language modeling? Can a large enough language model perform NLP task out of the box? OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before and the results are astounding.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Language Models\n2:45 - Language Modeling Datasets\n3:20 - Model Size\n5:35 - Transformer Models\n7:25 - Fine Tuning\n10:15 - In-Context Learning\n17:15 - Start of Experimental Results\n19:10 - Question Answering\n23:10 - What I think is happening\n28:50 - Translation\n31:30 - Winograd Schemes\n33:00 - Commonsense Reasoning\n37:00 - Reading Comprehension\n37:30 - SuperGLUE\n40:40 - NLI\n41:40 - Arithmetic Expressions\n48:30 - Word Unscrambling\n50:30 - SAT Analogies\n52:10 - News Article Generation\n58:10 - Made-up Words\n1:01:10 - Training Set Contamination\n1:03:10 - Task Examples\n\nhttps://arxiv.org/abs/2005.14165\nhttps://github.com/openai/gpt-3\n\nAbstract:\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\nAuthors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "375": "Do we really need dot-product attention? The attention mechanism is a central part of modern Transformers, mainly due to the dot-product attention mechanism. This paper changes the mechanism to remove the quadratic interaction terms and comes up with a new model, the Synthesizer. As it turns out, you can do pretty well like that!\n\nOUTLINE:\n0:00 - Intro & High Level Overview\n1:00 - Abstract\n2:30 - Attention Mechanism as Information Routing\n5:45 - Dot Product Attention\n8:05 - Dense Synthetic Attention\n15:00 - Random Synthetic Attention\n17:15 - Comparison to Feed-Forward Layers\n22:00 - Factorization & Mixtures\n23:10 - Number of Parameters\n25:35 - Machine Translation & Language Modeling Experiments\n36:15 - Summarization & Dialogue Generation Experiments\n37:15 - GLUE & SuperGLUE Experiments\n42:00 - Weight Sizes & Number of Head Ablations\n47:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2005.00743\nMy Video on Transformers (Attention Is All You Need): https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nThe dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \\textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).\n\nAuthors: Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "376": "This RL framework can discover low-level skills all by itself without any reward. Even better, at test time it can compose its learned skills and reach a specified goal without any additional learning! Warning: Math-heavy!\n\nOUTLINE:\n0:00 - Motivation\n2:15 - High-Level Overview\n3:20 - Model-Based vs Model-Free Reinforcement Learning\n9:00 - Skills\n12:10 - Mutual Information Objective\n18:40 - Decomposition of the Objective\n27:10 - Unsupervised Skill Discovery Algorithm\n42:20 - Planning in Skill Space\n48:10 - Conclusion\n\nPaper: https://arxiv.org/abs/1907.01657\nWebsite: https://sites.google.com/view/dads-skill\nCode: https://github.com/google-research/dads\n\nAbstract:\nConventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.\n\nAuthors: Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "377": "How does one measure the Intelligence of an AI? Is AlphaGo intelligent? How about GPT-3? In this landmark paper, Chollet proposes a solid measure of intelligence for AI that revolves around generalization, rather than skill.\n\nOUTLINE:\n0:00 - Intro\n1:15 - The need for a measure of intelligence\n3:35 - Intelligence as generalization ability\n5:45 - Nature vs nurture\n11:45 - Skill-based evaluation\n18:30 - Generalization based evaluation\n30:25 - Inspiration from psychometrics\n36:30 - Conclusion\n\nhttps://arxiv.org/abs/1911.01547\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nThumbnail: Photo by mohamed hassan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "378": "How do you learn labels without labels? How do you classify images when you don't know what to classify them into? This paper investigates a new combination of representation learning, clustering, and self-labeling in order to group visually similar images together - and achieves surprisingly high accuracy on benchmark datasets.\n\nOUTLINE:\n0:00 - Intro & High-level Overview\n2:15 - Problem Statement\n4:50 - Why naive Clustering does not work\n9:25 - Representation Learning\n13:40 - Nearest-neighbor-based Clustering\n28:00 - Self-Labeling\n32:10 - Experiments\n38:20 - ImageNet Experiments\n41:00 - Overclustering\n\nPaper: https://arxiv.org/abs/2005.12320\nCode: https://github.com/wvangansbeke/Unsupervised-Classification\n\nAbstract:\nIs it possible to automatically classify images without the use of ground-truth annotations? Or when even the classes themselves, are not a priori known? These remain important, and open questions in computer vision. Several approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by huge margins, in particular +26.9% on CIFAR10, +21.5% on CIFAR100-20 and +11.7% on STL10 in terms of classification accuracy. Furthermore, results on ImageNet show that our approach is the first to scale well up to 200 randomly selected classes, obtaining 69.3% top-1 and 85.5% top-5 accuracy, and marking a difference of less than 7.5% with fully-supervised methods. Finally, we applied our approach to all 1000 classes on ImageNet, and found the results to be very encouraging. The code will be made publicly available.\n\nAuthors: Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "379": "Many object detectors focus on locating the center of the object they want to find. However, this leaves them with the secondary problem of determining the specifications of the bounding box, leading to undesirable solutions like anchor boxes. This paper directly detects the top left and the bottom right corners of objects independently, along with descriptors that allows to match the two later and form a complete bounding box. For this, a new pooling method, called corner pooling, is introduced.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:40 - Object Detection\n2:40 - Pipeline I - Hourglass\n4:00 - Heatmap & Embedding Outputs\n8:40 - Heatmap Loss\n10:55 - Embedding Loss\n14:35 - Corner Pooling\n20:40 - Experiments\n\nPaper: https://arxiv.org/abs/1808.01244\nCode: https://github.com/princeton-vl/CornerNet\n\nAbstract:\nWe propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.\n\nAuthors: Hei Law, Jia Deng\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "380": "Neural Architecture Search is usually prohibitively expensive in both time and resources to be useful. A search strategy has to keep evaluating new models, training them to convergence in an inner loop to find out if they are any good. This paper proposes to abstract the problem and extract the essential part of the architecture to be optimized into a smaller version and evaluates that version on specifically custom learned data points to predict its performance, which is much faster and cheaper than running the full model.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:00 - Neural Architecture Search\n4:30 - Predicting performance via architecture encoding\n7:50 - Synthetic Petri Dish\n12:50 - Motivating MNIST example\n18:15 - Entire Algorithm\n23:00 - Producing the synthetic data\n26:00 - Combination with architecture search\n27:30 - PTB RNN-Cell Experiment\n29:20 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2005.13092\nCode: https://github.com/uber-research/Synthetic-Petri-Dish\n\nAbstract:\nNeural Architecture Search (NAS) explores a large space of architectural motifs -- a compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands of domain-specific data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available. Our hope is that this work can inspire a new research direction in studying the performance of extracted components of models in an alternative controlled setting.\n\nAuthors: Aditya Rawal, Joel Lehman, Felipe Petroski Such, Jeff Clune, Kenneth O. Stanley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "381": "Proper evaluation of text generation models, such as machine translation systems, requires expensive and slow human assessment. As these models have gotten better in previous years, proxy-scores, like BLEU, are becoming less and less useful. This paper proposes to learn a proxy score and demonstrates that it correlates well with human raters, even as the data distribution shifts.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n1:00 - The Problem with Evaluating Machine Translation\n5:10 - Task Evaluation as a Learning Problem\n10:45 - Naive Fine-Tuning BERT\n13:25 - Pre-Training on Synthetic Data\n16:50 - Generating the Synthetic Data\n18:30 - Priming via Auxiliary Tasks\n23:35 - Experiments & Distribution Shifts\n27:00 - Concerns & Conclusion\n\nPaper: https://arxiv.org/abs/2004.04696\nCode: https://github.com/google-research/bleurt\n\nAbstract:\nText generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.\n\nAbstract: Thibault Sellam, Dipanjan Das, Ankur P. Parikh\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "382": "Code migration between languages is an expensive and laborious task. To translate from one language to the other, one needs to be an expert at both. Current automatic tools often produce illegible and complicated code. This paper applies unsupervised neural machine translation to source code of Python, C++, and Java and is able to translate between them, without ever being trained in a supervised fashion.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - The Transcompiling Problem\n5:55 - Neural Machine Translation\n8:45 - Unsupervised NMT\n12:55 - Shared Embeddings via Token Overlap\n20:45 - MLM Objective\n25:30 - Denoising Objective\n30:10 - Back-Translation Objective\n33:00 - Evaluation Dataset\n37:25 - Results\n41:45 - Tokenization\n42:40 - Shared Embeddings\n43:30 - Human-Aware Translation\n47:25 - Failure Cases\n48:05 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.03511\n\nAbstract:\nA transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.\n\nAuthors: Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "383": "Text-to-speech engines are usually multi-stage pipelines that transform the signal into many intermediate representations and require supervision at each step. When trying to train TTS end-to-end, the alignment problem arises: Which text corresponds to which piece of sound? This paper uses an alignment module to tackle this problem and produces astonishingly good sound.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems with Text-to-Speech\n3:55 - Adversarial Training\n5:20 - End-to-End Training\n7:20 - Discriminator Architecture\n10:40 - Generator Architecture\n12:20 - The Alignment Problem\n14:40 - Aligner Architecture\n24:00 - Spectrogram Prediction Loss\n32:30 - Dynamic Time Warping\n38:30 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.03575\nWebsite: https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech\n\nAbstract:\nModern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable monotonic interpolation scheme to predict the duration of each input token. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.\n\nAuthors: Jeff Donahue, Sander Dieleman, Miko\u0142aj Bi\u0144kowski, Erich Elsen, Karen Simonyan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "384": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Complexity of Self-Attention\n4:50 - Embedding Dimension & Multiple Heads\n8:45 - Formal Attention\n10:30 - Empirical Investigation into RoBERTa\n20:00 - Theorem: Self-Attention is Low Rank\n28:10 - Linear Self-Attention Method\n36:15 - Theorem: Linear Self-Attention\n44:10 - Language Modeling\n46:40 - NLP Benchmarks\n47:50 - Compute Time & Memory Gains\n48:20 - Broader Impact Statement\n49:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.04768\n\nAbstract:\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "385": "Pre-training a CNN backbone for visual transfer learning has recently seen a big push into the direction of incorporating more data, at the cost of less supervision. This paper investigates the opposite: Visual transfer learning by pre-training from very few, but very high-quality samples on an image captioning task.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pre-Training for Visual Tasks\n3:40 - Quality-Quantity Tradeoff\n5:50 - Image Captioning\n8:35 - VirTex Method\n14:30 - Linear Classification\n20:30 - Ablations\n22:05 - Fine-Tuning\n25:45 - Attention Visualization\n27:30 - Conclusion & Remarks\n\nPaper: https://arxiv.org/abs/2006.06666\nCode: https://github.com/kdexd/virtex\n\nAbstract:\nThe de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.\n\nAuthors: Karan Desai, Justin Johnson\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "386": "In this part, we go much more in-depth into the relationship between intelligence, generality, skill, experience, and prior knowledge and take a close look at what priors are built into humans. This will form the basis for comparing the intelligence of humans and AI systems.\n\nOUTLINE:\n0:00 - Intro & Recap\n3:00 - Optimize for Generality\n5:45 - Buying Skill with Data and Priors\n12:40 - The Human Scope\n17:30 - Human Priors\n24:05 - Core Knowledge\n28:50 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/1911.01547\nTim Scarfe's Video: https://youtu.be/GpWLZUbPhr0\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "387": "In this part, we look at the ARC challenge as a proposed test of machine intelligence. The dataset features 1000 tasks that test rapid generalization based on human core knowledge priors, such as object-ness, symmetry, and navigation.\n\nOUTLINE:\n0:00 - Intro\n0:55 - What is ARC?\n6:30 - The Goals of ARC\n10:40 - Assumed Priors & Examples\n21:50 - An Imagined Solution\n28:15 - Consequences of a Solution\n31:00 - Weaknesses\n31:25 - My Comments & Ideas\n\nPaper: https://arxiv.org/abs/1911.01547\nARC: https://github.com/fchollet/ARC\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "388": "In this part, we go over the formal definition of the measure of intelligence. In order to do this, we have to frame and quantify the notions of generalization difficulty, priors, and experience in terms of algorithmic complexity.\n\nOUTLINE:\n0:00 - Intro & Recap\n2:50 - Concept Schema\n10:00 - Algorithmic Complexity\n13:00 - Definitions\n15:25 - Generalization Difficulty\n18:55 - Developer Aware Generalization Difficulty\n22:40 - Priors\n25:10 - Experience\n30:50 - The Measure Of Intelligence\n38:00 - An Ideal Intelligence Benchmark\n42:30 - Conclusion\n\nPaper: https://arxiv.org/abs/1911.01547\n\nPart 1: https://youtu.be/3_qGrmD6iQY\nPart 2: https://youtu.be/THcuTJbeD34\n\nAbstract:\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \"buy\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\n\nAuthors: Fran\u00e7ois Chollet\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "389": "#ai #research #gaming\n\nDeep RL is usually used to solve games, but this paper turns the process on its head and applies RL to game level creation. Compared to traditional approaches, it frames level design as a sequential decision making progress and ends up with a fast and diverse level generator.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:30 - Level Design via Reinforcement Learning\n3:00 - Reinforcement Learning\n4:45 - Observation Space\n5:40 - Action Space\n15:40 - Change Percentage Limit\n20:50 - Quantitative Results\n22:10 - Conclusion & Outlook\n\nPaper: https://arxiv.org/abs/2001.09212\nCode: https://github.com/amidos2006/gym-pcgrl\n\nAbstract:\nWe investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.\n\nAuthors: Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius\n\nERRATA:\n- The reward is given after each step.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "390": "Determining the stability properties of differential systems is a challenging task that involves very advanced symbolic and numeric mathematical manipulations. This paper shows that given enough training data, a simple language model with no underlying knowledge of mathematics can learn to solve these problems with remarkably high accuracy.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Differential System Tasks\n11:30 - Datasets & Models\n15:15 - Experiments\n21:00 - Discussion & My Comments\n\nPaper: https://arxiv.org/abs/2006.06462\nMy Video on Deep Learning for Symbolic Mathematics: https://youtu.be/p3sAF3gVMMA\n\nAbstract:\nCan advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.\n\nAuthors: Fran\u00e7ois Charton, Amaury Hayat, Guillaume Lample\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "391": "The Lottery Ticket Hypothesis has shown that it's theoretically possible to prune a neural network at the beginning of training and still achieve good performance, if we only knew which weights to prune away. This paper does not only explain where other attempts at pruning fail, but provides an algorithm that provably reaches maximum compression capacity, all without looking at any data!\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Pruning Neural Networks\n3:40 - Lottery Ticket Hypothesis\n6:00 - Paper Story Overview\n9:45 - Layer Collapse\n18:15 - Synaptic Saliency Conservation\n23:25 - Connecting Layer Collapse & Saliency Conservation\n28:30 - Iterative Pruning avoids Layer Collapse\n33:20 - The SynFlow Algorithm\n40:45 - Experiments\n43:35 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05467\nCode: https://github.com/ganguli-lab/Synaptic-Flow\nMy Video on the Lottery Ticket Hypothesis: https://youtu.be/ZVVnvZdUMUk\nStreet Talk about LTH: https://youtu.be/SfjJoevBbjU\n\nAbstract:\nPruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.\n\nAuthors: Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "392": "Even though LSTMs and GRUs solve the vanishing and exploding gradient problems, they have trouble learning to remember things over very long time spans. Inspired from bistability, a property of biological neurons, this paper constructs a recurrent cell with an inherent memory property, with only minimal modification to existing architectures.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Recurrent Neural Networks\n6:00 - Gated Recurrent Unit\n14:40 - Neuronal Bistability\n22:50 - Bistable Recurrent Cell\n31:00 - Neuromodulation\n32:50 - Copy First Benchmark\n37:35 - Denoising Benchmark\n48:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05252\nCode: https://github.com/nvecoven/BRC\n\nAbstract:\nRecurrent neural networks (RNNs) provide state-of-the-art performances in a wide variety of tasks that require memory. These performances can often be achieved thanks to gated recurrent cells such as gated recurrent units (GRU) and long short-term memory (LSTM). Standard gated cells share a layer internal state to store information at the network level, and long term memory is shaped by network-wide recurrent connection weights. Biological neurons on the other hand are capable of holding information at the cellular level for an arbitrary long amount of time through a process called bistability. Through bistability, cells can stabilize to different stable states depending on their own past state and inputs, which permits the durable storing of past information in neuron state. In this work, we take inspiration from biological neuron bistability to embed RNNs with long-lasting memory at the cellular level. This leads to the introduction of a new bistable biologically-inspired recurrent cell that is shown to strongly improves RNN performance on time-series which require very long memory, despite using only cellular connections (all recurrent connections are from neurons to themselves, i.e. a neuron state is not influenced by the state of other neurons). Furthermore, equipping this cell with recurrent neuromodulation permits to link them to standard GRU cells, taking a step towards the biological plausibility of GRU.\n\nAuthors: Nicolas Vecoven, Damien Ernst, Guillaume Drion\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "393": "Image-to-Image translation usually requires corresponding samples or at least domain labels of the dataset. This paper removes that restriction and allows for fully unsupervised image translation of a source image to the style of one or many reference images. This is achieved by jointly training a guiding network that provides style information and pseudo-labels.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Unsupervised Image-to-Image Translation\n7:05 - Architecture Overview\n14:15 - Pseudo-Label Loss\n19:30 - Encoder Style Contrastive Loss\n25:30 - Adversarial Loss\n31:20 - Generator Style Contrastive Loss\n35:15 - Image Reconstruction Loss\n36:55 - Architecture Recap\n39:55 - Full Loss\n42:05 - Experiments\n\nPaper: https://arxiv.org/abs/2006.06500\nCode: https://github.com/clovaai/tunit\n\nAbstract:\nEvery recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, our model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at this https URL\n\nAuthors: Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "394": "Self-supervised representation learning relies on negative samples to keep the encoder from collapsing to trivial solutions. However, this paper shows that negative samples, which are a nuisance to implement, are not necessary for learning good representation, and their algorithm BYOL is able to outperform other baselines using just positive samples.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Image Representation Learning\n3:55 - Self-Supervised Learning\n5:35 - Negative Samples\n10:50 - BYOL\n23:20 - Experiments\n30:10 - Conclusion & Broader Impact\n\nPaper: https://arxiv.org/abs/2006.07733\n\nAbstract:\nWe introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.\n\nAuthors: Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "395": "BERT and GPT-2/3 have shown the enormous power of using generative models as pre-training for classification tasks. However, for images, pre-training is usually done with supervised or self-supervised objectives. This paper investigates how far you can get when applying the principles from the world of NLP to the world of images.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Generative Models for Pretraining\n4:50 - Pretraining for Visual Tasks\n7:40 - Model Architecture\n15:15 - Linear Probe Experiments\n24:15 - Fine-Tuning Experiments\n30:25 - Conclusion & Comments\n\nPaper:\nhttps://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf\nBlog: https://openai.com/blog/image-gpt/\nCode: https://github.com/openai/image-gpt\n\nAbstract:\nInspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.\n\nAuthors: Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "396": "This paper proposes SimCLRv2 and shows that semi-supervised learning benefits a lot from self-supervised pre-training. And stunningly, that effect gets larger the fewer labels are available and the more parameters the model has.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Semi-Supervised Learning\n3:50 - Pre-Training via Self-Supervision\n5:45 - Contrastive Loss\n10:50 - Retaining Projection Heads\n13:10 - Supervised Fine-Tuning\n13:45 - Unsupervised Distillation & Self-Training\n18:45 - Architecture Recap\n22:25 - Experiments\n34:15 - Broader Impact\n\nPaper: https://arxiv.org/abs/2006.10029\nCode: https://github.com/google-research/simclr\n\nAbstract:\nOne paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\\% ImageNet top-1 accuracy with just 1\\% of the labels (\u226413 labeled images per class) using ResNet-50, a 10\u00d7 improvement in label efficiency over the previous state-of-the-art. With 10\\% of labels, ResNet-50 trained with our method achieves 77.5\\% top-1 accuracy, outperforming standard supervised training with all of the labels.\n\nAuthors: Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "397": "Implicit neural representations are created when a neural network is used to represent a signal as a function. SIRENs are a particular type of INR that can be applied to a variety of signals, such as images, sound, or 3D shapes. This is an interesting departure from regular machine learning and required me to think differently.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:15 - Implicit Neural Representations\n9:40 - Representing Images\n14:30 - SIRENs\n18:05 - Initialization\n20:15 - Derivatives of SIRENs\n23:05 - Poisson Image Reconstruction\n28:20 - Poisson Image Editing\n31:35 - Shapes with Signed Distance Functions\n45:55 - Paper Website\n48:55 - Other Applications\n50:45 - Hypernetworks over SIRENs\n54:30 - Broader Impact\n\nPaper: https://arxiv.org/abs/2006.09661\nWebsite: https://vsitzmann.github.io/siren/\n\nAbstract:\nImplicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.\n\nAuthors: Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "398": "Counting repeated actions in a video is one of the easiest tasks for humans, yet remains incredibly hard for machines. RepNet achieves state-of-the-art by creating an information bottleneck in the form of a temporal self-similarity matrix, relating video frames to each other in a way that forces the model to surface the information relevant for counting. Along with that, the authors produce a new dataset for evaluating counting models.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Problem Statement\n5:15 - Output & Loss\n6:25 - Per-Frame Embeddings\n11:20 - Temporal Self-Similarity Matrix\n19:00 - Periodicity Predictor\n25:50 - Architecture Recap\n27:00 - Synthetic Dataset\n30:15 - Countix Dataset\n31:10 - Experiments\n33:35 - Applications\n35:30 - Conclusion & Comments\n\nPaper Website: https://sites.google.com/view/repnet\nColab: https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\n\nAbstract:\nWe present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called RepNet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix (~90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos.\n\nAuthors: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "399": "I retrace my first reading of Facebook AI's DETR paper and explain my process of understanding it.\n\nOUTLINE:\n0:00 - Introduction\n1:25 - Title\n4:10 - Authors\n5:55 - Affiliation\n7:40 - Abstract\n13:50 - Pictures\n20:30 - Introduction\n22:00 - Related Work\n24:00 - Model\n30:00 - Experiments\n41:50 - Conclusions & Abstract\n42:40 - Final Remarks\n\nOriginal Video about DETR: https://youtu.be/T35ba_VXkMY\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "400": "Neural networks are very good at predicting systems' numerical outputs, but not very good at deriving the discrete symbolic equations that govern many physical systems. This paper combines Graph Networks with symbolic regression and shows that the strong inductive biases of these models can be used to derive accurate symbolic equations from observation data.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:10 - Problem Statement\n4:25 - Symbolic Regression\n6:40 - Graph Neural Networks\n12:05 - Inductive Biases for Physics\n15:15 - How Graph Networks compute outputs\n23:10 - Loss Backpropagation\n24:30 - Graph Network Recap\n26:10 - Analogies of GN to Newtonian Mechanics\n28:40 - From Graph Network to Equation\n33:50 - L1 Regularization of Edge Messages\n40:10 - Newtonian Dynamics Example\n43:10 - Cosmology Example\n44:45 - Conclusions & Appendix\n\nPaper: https://arxiv.org/abs/2006.11287\nCode: https://github.com/MilesCranmer/symbolic_deep_learning\n\nAbstract:\nWe develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.\n\nAuthors: Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "401": "Backpropagation is one of the central components of modern deep learning. However, it's not biologically plausible, which limits the applicability of deep learning to understand how the human brain works. Direct Feedback Alignment is a biologically plausible alternative and this paper shows that, contrary to previous research, it can be successfully applied to modern deep architectures and solve challenging tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Problem with Backpropagation\n10:25 - Direct Feedback Alignment\n21:00 - My Intuition why DFA works\n31:20 - Experiments\n\nPaper: https://arxiv.org/abs/2006.12878\nCode: https://github.com/lightonai/dfa-scales-to-modern-deep-learning\nReferenced Paper by Arild N\u00f8kland: https://arxiv.org/abs/1609.01596\n\nAbstract:\nDespite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.\n\nAuthors: Julien Launay, Iacopo Poli, Fran\u00e7ois Boniface, Florent Krzakala\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "402": "Object detection often does not occur in a vacuum. Static cameras, such as wildlife traps, collect lots of irregularly sampled data over a large time frame and often capture repeating or similar events. This model learns to dynamically incorporate other frames taken by the same camera into its object detection pipeline.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Problem Formulation\n2:10 - Static Camera Data\n6:45 - Architecture Overview\n10:00 - Short-Term Memory\n15:40 - Long-Term Memory\n20:10 - Quantitative Results\n22:30 - Qualitative Results\n30:10 - False Positives\n32:50 - Appendix & Conclusion\n\nPaper: https://arxiv.org/abs/1912.03538\n\nMy Video On Attention Is All You Need: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nIn static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame.\nWe apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.\n\nAuthors: Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "403": "We've become very good at making generative models for images and classes of images, but not yet of sets of images, especially when the number of sets is unknown and can contain sets that have never been encountered during training. This paper builds a probabilistic framework and a practical implementation of a generative model for sets of images based on variational methods.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n8:05 - Architecture Overview\n20:05 - Probabilistic Model\n33:50 - Likelihood Function\n40:30 - Model Architectures\n44:20 - Loss Function & Optimization\n47:30 - Results\n58:45 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.10705\n\nAbstract:\nImages with shared characteristics naturally form sets. For example, in a face verification benchmark, images of the same identity form sets. For generative models, the standard way of dealing with sets is to represent each as a one hot vector, and learn a conditional generative model p(x|y). This representation assumes that the number of sets is limited and known, such that the distribution over sets reduces to a simple multinomial distribution. In contrast, we study a more generic problem where the number of sets is large and unknown. We introduce Set Distribution Networks (SDNs), a novel framework that learns to autoencode and freely generate sets. We achieve this by jointly learning a set encoder, set discriminator, set generator, and set prior. We show that SDNs are able to reconstruct image sets that preserve salient attributes of the inputs in our benchmark datasets, and are also able to generate novel objects/identities. We examine the sets generated by SDN with a pre-trained 3D reconstruction network and a face verification network, respectively, as a novel way to evaluate the quality of generated sets of images.\n\nAuthors: Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Carlos Guestrin, Josh M. Susskind\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "404": "The abundance of data on the internet is vast. Especially unlabeled images are plentiful and can be collected with ease. This model investigates a new method for incorporating unlabeled data into a supervised learning pipeline. First, a teacher model is trained in a supervised fashion. Then, that teacher is used to label the unlabeled data. Next, a larger student model is trained on the combination of all data and achieves better performance than the teacher by itself.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:05 - Semi-Supervised & Transfer Learning\n5:45 - Self-Training & Knowledge Distillation\n10:00 - Noisy Student Algorithm Overview\n20:20 - Noise Methods\n22:30 - Dataset Balancing\n25:20 - Results\n30:15 - Perturbation Robustness\n34:35 - Ablation Studies\n39:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1911.04252\nCode: https://github.com/google-research/noisystudent\nModels: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\n\nAbstract:\nWe present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.\nNoisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.\n\nAuthors: Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "405": "Deep neural networks are large models and pruning has become an important part of ML product pipelines, making models small while keeping their performance high. However, the classic pruning method, Magnitude Pruning, is suboptimal in models that are obtained by transfer learning. This paper proposes a solution, called Movement Pruning and shows its superior performance.\n\nOUTLINE:\n0:00 - Intro & High-Level Overview\n0:55 - Magnitude Pruning\n4:25 - Transfer Learning\n7:25 - The Problem with Magnitude Pruning in Transfer Learning\n9:20 - Movement Pruning\n22:20 - Experiments\n24:20 - Improvements via Distillation\n26:40 - Analysis of the Learned Weights\n\nPaper: https://arxiv.org/abs/2005.07683\nCode: https://github.com/huggingface/transformers/tree/master/examples/movement-pruning\n\nAbstract:\nMagnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.\n\nAuthors: Victor Sanh, Thomas Wolf, Alexander M. Rush\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "406": "Visual scenes are often comprised of sets of independent objects. Yet, current vision models make no assumptions about the nature of the pictures they look at. By imposing an objectness prior, this paper a module that is able to recognize permutation-invariant sets of objects from pixels in both supervised and unsupervised settings. It does so by introducing a slot attention module that combines an attention mechanism with dynamic routing.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Problem Formulation\n4:30 - Slot Attention Architecture\n13:30 - Slot Attention Algorithm\n21:30 - Iterative Routing Visualization\n29:15 - Experiments\n36:20 - Inference Time Flexibility\n38:35 - Broader Impact Statement\n42:05 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15055\n\nMy Video on Facebook's DETR: https://youtu.be/T35ba_VXkMY\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on Capsules: https://youtu.be/nXGHJTtFYRU\n\nAbstract:\nLearning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.\n\nAuthors: Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "407": "Google builds a 600 billion parameter transformer to do massively multilingual, massive machine translation. Interestingly, the larger model scale does not come from increasing depth of the transformer, but from increasing width in the feedforward layers, combined with a hard routing to parallelize computations on up to 2048 TPUs. A very detailed engineering paper!\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Main Results\n5:10 - Mixture-of-Experts\n16:00 - Difference to Scaling Classic Transformers\n18:50 - Backpropagation in Mixture-of-Experts\n20:05 - MoE Routing Algorithm in GShard\n38:20 - GShard Einsum Examples\n47:40 - Massively Multilingual Translation\n56:00 - Results\n1:11:30 - Conclusion & Comments\n\nERRATA:\nI said the computation of MoE scales linearly, but actually, it's sub(!)-linear.\n\nPaper: https://arxiv.org/abs/2006.16668\n\nAbstract:\nNeural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.\n\nAuthors:\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "408": "Proteins are the workhorses of almost all cellular functions and a core component of life. But despite their versatility, all proteins are built as sequences of the same 20 amino acids. These sequences can be analyzed with tools from NLP. This paper investigates the attention mechanism of a BERT model that has been trained on protein sequence data and discovers that the language model has implicitly learned non-trivial higher-order biological properties of proteins.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - From DNA to Proteins\n5:20 - BERT for Amino Acid Sequences\n8:50 - The Structure of Proteins\n12:40 - Investigating Biological Properties by Inspecting BERT\n17:45 - Amino Acid Substitution\n24:55 - Contact Maps\n30:15 - Binding Sites\n33:45 - Linear Probes\n35:25 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.15222\nCode: https://github.com/salesforce/provis\n\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nTransformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at this https URL.\n\nAuthors: Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "409": "#machinelearning #ai #google\n\nThe high-level architecture of CNNs has not really changed over the years. We tend to build high-resolution low-dimensional layers first, followed by ever more coarse, but deep layers. This paper challenges this decades-old heuristic and uses neural architecture search to find an alternative, called SpineNet that employs multiple rounds of re-scaling and long-range skip connections.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Problem Statement\n2:30 - The Problem with Current Architectures\n8:20 - Scale-Permuted Networks\n11:40 - Neural Architecture Search\n14:00 - Up- and Downsampling\n19:10 - From ResNet to SpineNet\n24:20 - Ablations\n27:00 - My Idea: Attention Routing for CNNs\n29:55 - More Experiments\n34:45 - Conclusion & Comments\n\nPapers: https://arxiv.org/abs/1912.05027\nCode: https://github.com/tensorflow/tpu/tree/master/models/official/detection\n\nAbstract:\nConvolutional neural networks typically encode an input image into a series of intermediate features with decreasing resolutions. While this structure is suited to classification tasks, it does not perform well for tasks requiring simultaneous recognition and localization (e.g., object detection). The encoder-decoder architectures are proposed to resolve this by applying a decoder network onto a backbone model designed for classification tasks. In this paper, we argue encoder-decoder architecture is ineffective in generating strong multi-scale features because of the scale-decreased backbone. We propose SpineNet, a backbone with scale-permuted intermediate features and cross-scale connections that is learned on an object detection task by Neural Architecture Search. Using similar building blocks, SpineNet models outperform ResNet-FPN models by ~3% AP at various scales while using 10-20% fewer FLOPs. In particular, SpineNet-190 achieves 52.5% AP with a MaskR-CNN detector and achieves 52.1% AP with a RetinaNet detector on COCO for a single model without test-time augmentation, significantly outperforms prior art of detectors. SpineNet can transfer to classification tasks, achieving 5% top-1 accuracy improvement on a challenging iNaturalist fine-grained dataset. Code is at: this https URL.\n\nAuthors: Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, Xiaodan Song\n\nThumbnail art by Lucas Ferreira\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "410": "#ai #attention #transformer #deeplearning\n\nTransformers are famous for two things: Their superior performance and their insane requirements of compute and memory. This paper reformulates the attention mechanism in terms of kernel functions and obtains a linear formulation, which reduces these requirements. Surprisingly, this formulation also surfaces an interesting connection between autoregressive transformers and RNNs.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Softmax Attention & Transformers\n8:40 - Quadratic Complexity of Softmax Attention\n9:40 - Generalized Attention Mechanism\n13:45 - Kernels\n20:40 - Linear Attention\n25:20 - Experiments\n28:30 - Intuition on Linear Attention\n33:55 - Connecting Autoregressive Transformers and RNNs\n41:30 - Caveats with the RNN connection\n46:00 - More Results & Conclusion\n\nPaper: https://arxiv.org/abs/2006.16236\nWebsite: https://linear-transformers.com/\nCode: https://github.com/idiap/fast-transformers\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nTransformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \ue23b(N2) to \ue23b(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\nAuthors: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "411": "Supermasks are binary masks of a randomly initialized neural network that result in the masked network performing well on a particular task. This paper considers the problem of (sequential) Lifelong Learning and trains one Supermask per Task, while keeping the randomly initialized base network constant. By minimizing the output entropy, the system can automatically derive the Task ID of a data point at inference time and distinguish up to 2500 tasks automatically.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Catastrophic Forgetting\n5:20 - Supermasks\n9:35 - Lifelong Learning using Supermasks\n11:15 - Inference Time Task Discrimination by Entropy\n15:05 - Mask Superpositions\n24:20 - Proof-of-Concept, Task Given at Inference\n30:15 - Binary Maximum Entropy Search\n32:00 - Task Not Given at Inference\n37:15 - Task Not Given at Training\n41:35 - Ablations\n45:05 - Superfluous Neurons\n51:10 - Task Selection by Detecting Outliers\n57:40 - Encoding Masks in Hopfield Networks\n59:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2006.14769\nCode: https://github.com/RAIVNLab/supsup\n\nMy Video about Lottery Tickets: https://youtu.be/ZVVnvZdUMUk\nMy Video about Supermasks: https://youtu.be/jhCInVFE2sc\n\nAbstract:\nWe present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.\n\nAuthors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "412": "I take a closer look at \"Supermasks in Superposition\" after I've already done a video on it. Specifically, I look at: 1. The intuition and theoretical justification behind the G objective, 2. Whether Supermasks and Superposition can be viewed as two distinct ideas and 3. The Paper's Broader Impact Statement.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - SupSup Recap\n4:00 - In-Depth Analysis of the G Objective\n20:30 - Superposition without Supermasks\n25:40 - Broader Impact Statement\n36:40 - Conclusion\n37:20 - Live Coding\n\nPart 1 on SupSup: https://youtu.be/3jT1qJ8ETzk\nMy Code: https://colab.research.google.com/drive/1bEcppdN6qZRpEFplIiv41ZI3vDwDjcvC?usp=sharing\nPaper: https://arxiv.org/abs/2006.14769\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "413": "VAEs have been traditionally hard to train at high resolutions and unstable when going deep with many layers. In addition, VAE samples are often more blurry and less crisp than those from GANs. This paper details all the engineering choices necessary to successfully train a deep hierarchical VAE that exhibits global consistency and astounding sharpness at high resolutions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Variational Autoencoders\n8:25 - Hierarchical VAE Decoder\n12:45 - Output Samples\n15:00 - Hierarchical VAE Encoder\n17:20 - Engineering Decisions\n22:10 - KL from Deltas\n26:40 - Experimental Results\n28:40 - Appendix\n33:00 - Conclusion\n\nPaper: https://arxiv.org/abs/2007.03898\n\nAbstract:\nNormalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\u00d7256 pixels.\n\nAuthors: Arash Vahdat, Jan Kautz\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "414": "Neural networks for implicit representations, such as SIRENs, have been very successful at modeling natural signals. However, in the classical approach, each data point requires its own neural network to be fit. This paper extends implicit representations to an entire dataset by introducing latent vectors of data points to SIRENs. Interestingly, the paper shows that such latent vectors can be obtained without the need for an explicit encoder, by simply looking at the negative gradient of the zero-vector through the representation function.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:10 - Implicit Generative Models\n5:30 - Implicitly Represent a Dataset\n11:00 - Gradient Origin Networks\n23:55 - Relation to Gradient Descent\n28:05 - Messing with their Code\n37:40 - Implicit Encoders\n38:50 - Using GONs as classifiers\n40:55 - Experiments & Conclusion\n\nPaper: https://arxiv.org/abs/2007.02798\nCode: https://github.com/cwkx/GON\nProject Page: https://cwkx.github.io/data/GON/\n\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\n\nAbstract:\nThis paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.\n\nAuthors: Sam Bond-Taylor, Chris G. Willcocks\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher", "415": "#ai #research #optimization\n\nDeep Ensembles work surprisingly well for improving the generalization capabilities of deep neural networks. Surprisingly, they outperform Bayesian Networks, which are - in theory - doing the same thing. This paper investigates how Deep Ensembles are especially suited to capturing the non-convex loss landscape of neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Deep Ensembles\n4:15 - The Solution Space of Deep Networks\n7:30 - Bayesian Models\n9:00 - The Ensemble Effect\n10:25 - Experiment Setup\n11:30 - Solution Equality While Training\n19:40 - Tracking Multiple Trajectories\n21:20 - Similarity of Independent Solutions\n24:10 - Comparison to Baselines\n30:10 - Weight Space Cross-Sections\n35:55 - Diversity vs Accuracy\n41:00 - Comparing Ensembling Methods\n44:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1912.02757\n\nAbstract:\nDeep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.\n\nAuthors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", "416": "#ai #dqn #deepmind\n\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Arcade Learning Environment\n4:25 - Deep Reinforcement Learning\n9:20 - Deep Q-Learning\n26:30 - Experience Replay\n32:25 - Network Architecture\n33:50 - Experiments\n37:45 - Conclusion\n\nPaper: https://arxiv.org/abs/1312.5602\n\nAbstract:\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "417": "#ai #research #alexnet\n\nAlexNet was the start of the deep learning revolution. Up until 2012, the best computer vision systems relied on hand-crafted features and highly specialized algorithms to perform object classification. This paper was the first to successfully train a deep convolutional neural network on not one, but two GPUs and managed to outperform the competition on ImageNet by an order of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - The necessity of larger models\n6:20 - Why CNNs?\n11:05 - ImageNet\n12:05 - Model Architecture Overview\n14:35 - ReLU Nonlinearities\n18:45 - Multi-GPU training\n21:30 - Classification Results\n24:30 - Local Response Normalization\n28:05 - Overlapping Pooling\n32:25 - Data Augmentation\n38:30 - Dropout\n40:30 - More Results\n43:50 - Conclusion\n\nPaper: http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\n\nAbstract:\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n\nAuthors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "418": "#ai #deeplearning #gan\n\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:50 - Motivation\n8:40 - Minimax Loss Function\n13:20 - Intuition Behind the Loss\n19:30 - GAN Algorithm\n22:05 - Theoretical Analysis\n27:00 - Experiments\n33:10 - Advantages & Disadvantages\n35:00 - Conclusion\n\nPaper: https://arxiv.org/abs/1406.2661\n\nAbstract:\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "419": "#ai #research #word2vec\n\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:50 - Distributed Word Representations\n5:40 - Skip-Gram Model\n12:00 - Hierarchical Softmax\n14:55 - Negative Sampling\n22:30 - Mysterious 3/4 Power\n25:50 - Frequent Words Subsampling\n28:15 - Empirical Results\n29:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1310.4546\nCode: https://code.google.com/archive/p/word2vec/\n\nAbstract:\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\nAuthors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "420": "#ai #research #resnet\n\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper's invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:45 - The Problem with Depth\n3:15 - VGG-Style Networks\n6:00 - Overfitting is Not the Problem\n7:25 - Motivation for Residual Connections\n10:25 - Residual Blocks\n12:10 - From VGG to ResNet\n18:50 - Experimental Results\n23:30 - Bottleneck Blocks\n24:40 - Deeper ResNets\n28:15 - More Results\n29:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1512.03385\n\nAbstract:\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "421": "#ai #research #machinelearning\n\nNeural Architecture Search is typically very slow and resource-intensive. A meta-controller has to train many hundreds or thousands of different models to find a suitable building plan. This paper proposes to use statistics of the Jacobian around data points to estimate the performance of proposed architectures at initialization. This method does not require training and speeds up NAS by orders of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n0:50 - Neural Architecture Search\n4:15 - Controller-based NAS\n7:35 - Architecture Search Without Training\n9:30 - Linearization Around Datapoints\n14:10 - Linearization Statistics\n19:00 - NAS-201 Benchmark\n20:15 - Experiments\n34:15 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04647\nCode: https://github.com/BayesWatch/nas-without-training\n\nAbstract:\nThe time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the NAS-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU. Code to reproduce our experiments is available at this https URL.\n\nAuthors: Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "422": "#ai #research #machinelearning\n\nOnline Reinforcement Learning is a flourishing field with countless methods for practitioners to choose from. However, each of those methods comes with a plethora of hyperparameter choices. This paper builds a unified framework for five continuous control tasks and investigates in a large-scale study the effects of these choices. As a result, they come up with a set of recommendations for future research and applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:55 - Parameterized Agents\n7:00 - Unified Online RL and Parameter Choices\n14:10 - Policy Loss\n16:40 - Network Architecture\n20:25 - Initial Policy\n24:20 - Normalization & Clipping\n26:30 - Advantage Estimation\n28:55 - Training Setup\n33:05 - Timestep Handling\n34:10 - Optimizers\n35:05 - Regularization\n36:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.05990\n\nAbstract:\nIn recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress (Engstrom'20). As a step towards filling that gap, we implement over 50 such \"choices\" in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.\n\nAuthors: Marcin Andrychowicz, Anton Raichuk, Piotr Sta\u0144czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "423": "#ai #transformer #attention\n\nHopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:35 - Binary Hopfield Networks\n5:55 - Continuous Hopfield Networks\n8:15 - Update Rules & Energy Functions\n13:30 - Connection to Transformers\n14:35 - Hopfield Attention Layers\n26:45 - Theoretical Analysis\n48:10 - Investigating BERT\n1:02:30 - Immune Repertoire Classification\n\nPaper: https://arxiv.org/abs/2008.02217\nCode: https://github.com/ml-jku/hopfield-layers\nImmune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505\n\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\n\nAbstract:\nWe show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL\n\nAuthors: Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "424": "#ai #neuroscience #rl\n\nReinforcement Learning is a powerful tool, but it lacks biological plausibility because it learns a fixed policy network. Animals use neuroplasticity to reconfigure their policies on the fly and quickly adapt to new situations. This paper uses Hebbian Learning, a biologically inspired technique, to have agents adapt random networks to high-performing solutions as an episode is progressing, leading to agents that can reconfigure themselves in response to new observations.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Reinforcement Learning vs Hebbian Plasticity\n9:00 - Episodes in Hebbian Learning\n10:00 - Hebbian Plasticity Rules\n18:10 - Quadruped Experiment Results\n21:20 - Evolutionary Learning of Hebbian Plasticity\n29:10 - More Experimental Results\n34:50 - Conclusions\n35:30 - Broader Impact Statement\n\nVideos: https://twitter.com/risi1979/status/1280544779630186499\nPaper: https://arxiv.org/abs/2007.02686\n\nAbstract:\nLifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to different morphological damage in the absence of any explicit reward or error signal.\n\nAuthors: Elias Najarro, Sebastian Risi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "425": "#ai #tech #science\n\nOpen Domain Question Answering is one of the most challenging tasks in NLP. When answering a question, the model is able to retrieve arbitrary documents from an indexed corpus to gather more information. REALM shows how Masked Language Modeling (MLM) pretraining can be used to train a retriever for relevant documents in an end-to-end fashion and improves over state-of-the-art by a significant margin.\n\nOUTLINE:\n0:00 - Introduction & Overview\n4:30 - World Knowledge in Language Models\n8:15 - Masked Language Modeling for Latent Document Retrieval\n14:50 - Problem Formulation\n17:30 - Knowledge Retriever Model using MIPS\n23:50 - Question Answering Model\n27:50 - Architecture Recap\n29:55 - Analysis of the Loss Gradient\n34:15 - Initialization using the Inverse Cloze Task\n41:40 - Prohibiting Trivial Retrievals\n44:05 - Null Document\n45:00 - Salient Span Masking\n50:15 - My Idea on Salient Span Masking\n51:50 - Experimental Results and Ablations\n57:30 - Concrete Example from the Model\n\nPaper: https://arxiv.org/abs/2002.08909\nCode: https://github.com/google-research/language/tree/master/language/realm\n\nMy Video on GPT-3: https://www.youtube.com/watch?v=SY5PvZrJhLE\nMy Video on BERT: https://www.youtube.com/watch?v=-9evrZnBorM\nMy Video on Word2Vec: https://www.youtube.com/watch?v=yexR53My2O4\n\nAbstract:\nLanguage model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\n\nAuthors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "426": "#ai #research #reinforcementlearning\n\nReinforcement Learning is a powerful tool, but it is also incredibly data-hungry. Given a new task, an RL agent has to learn a good policy entirely from scratch. This paper proposes a new framework that allows an agent to carry over knowledge from previous tasks into solving new tasks, even deriving zero-shot policies that perform well on completely new reward functions.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n6:25 - Q-Learning Primer\n11:40 - Multiple Rewards, Multiple Policies\n14:25 - Example Environment\n17:35 - Tasks as Linear Mixtures of Features\n24:15 - Successor Features\n28:00 - Zero-Shot Policy for New Tasks\n35:30 - Results on New Task W3\n37:00 - Inferring the Task via Regression\n39:20 - The Influence of the Given Policies\n48:40 - Learning the Feature Functions\n50:30 - More Complicated Tasks\n51:40 - Life-Long Learning, Comments & Conclusion\n\nPaper: https://www.pnas.org/content/early/2020/08/13/1907370117\n\nMy Video on Successor Features: https://youtu.be/KXEEqcwXn8w\n\nAbstract:\nThe combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.\n\nAuthors:\nAndr\u00e9 Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "427": "#ai #research #privacy\n\nData is the modern gold. Neural classifiers can improve their performance by training on more data, but given a trained classifier, it's difficult to tell what data it was trained on. This is especially relevant if you have proprietary or personal data and you want to make sure that other people don't use it to train their models. This paper introduces a method to mark a dataset with a hidden \"radioactive\" tag, such that any resulting classifier will clearly exhibit this tag, which can be detected.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - How Neural Classifiers Work\n5:45 - Radioactive Marking via Adding Features\n13:55 - Random Vectors in High-Dimensional Spaces\n18:05 - Backpropagation of the Fake Features\n21:00 - Re-Aligning Feature Spaces\n25:00 - Experimental Results\n28:55 - Black-Box Test\n32:00 - Conclusion & My Thoughts\n\nPaper: https://arxiv.org/abs/2002.00937\n\nAbstract:\nWe want to detect whether a particular image dataset has been used to train a model. We propose a new technique, \\emph{radioactive data}, that makes imperceptible changes to this dataset such that any model trained on it will bear an identifiable mark. The mark is robust to strong variations such as different architectures or optimization methods. Given a trained model, our technique detects the use of radioactive data and provides a level of confidence (p-value). Our experiments on large-scale benchmarks (Imagenet), using standard architectures (Resnet-18, VGG-16, Densenet-121) and training procedures, show that we can detect usage of radioactive data with high confidence (p &lt; 10^-4) even when only 1% of the data used to trained our model is radioactive. Our method is robust to data augmentation and the stochasticity of deep network optimization. As a result, it offers a much higher signal-to-noise ratio than data poisoning and backdoor methods.\n\nAuthors: Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\u00e9 J\u00e9gou\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "428": "#ai #machinelearning #attention\n\nConvolutional Neural Networks have dominated image processing for the last decade, but transformers are quickly replacing traditional models. This paper proposes a fully attentional model for images by combining learned Positional Embeddings with Axial Attention. This new model can compete with CNNs on image classification and achieve state-of-the-art in various image segmentation tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - This Paper's Contributions\n6:20 - From Convolution to Self-Attention for Images\n16:30 - Learned Positional Embeddings\n24:20 - Propagating Positional Embeddings through Layers\n27:00 - Traditional vs Position-Augmented Attention\n31:10 - Axial Attention\n44:25 - Replacing Convolutions in ResNet\n46:10 - Experimental Results & Examples\n\nPaper: https://arxiv.org/abs/2003.07853\nCode: https://github.com/csrhddlam/axial-deeplab\n\nMy Video on BigBird: https://youtu.be/WVPE62Gk3EM\nMy Video on ResNet: https://youtu.be/GWt6Fu05voI\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\n\nAbstract:\nConvolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.\n\nAuthors: Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh Chen\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "429": "#ai #biology #machinelearning\n\nNeural Cellular Automata are models for how living creatures can use local message passing to reach global consensus without a central authority. This paper teaches pixels of an image to communicate with each other and figure out as a group which digit they represent. On the way, the authors have to deal with pesky side-effects that come from applying the Cross-Entropy Loss in combination with a Softmax layer, but ultimately achieve a self-sustaining, stable and continuous algorithm that models living systems.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Neural Cellular Automata\n7:30 - Global Agreement via Message-Passing\n11:05 - Neural CAs as Recurrent Convolutions\n14:30 - Training Continuously Alive Systems\n17:30 - Problems with Cross-Entropy\n26:10 - Out-of-Distribution Robustness\n27:10 - Chimeric Digits\n27:45 - Visualizing Latent State Dimensions\n29:05 - Conclusion & Comments\n\nPaper: https://distill.pub/2020/selforg/mnist/\n\nMy Video on Neural CAs: https://youtu.be/9Kec_7WFyp0\n\nAbstract:\nGrowing Neural Cellular Automata [1] demonstrated how simple cellular automata (CAs) can learn to self-organise into complex shapes while being resistant to perturbations. Such a computational model approximates a solution to an open question in biology, namely, how do cells cooperate to create a complex multicellular anatomy and work to regenerate it upon damage? The model parameterizing the cells\u2019 rules is parameter-efficient, end-to-end differentiable, and illustrates a new approach to modeling the regulation of anatomical homeostasis. In this work, we use a version of this model to show how CAs can be applied to a common task in machine learning: classification. We pose the question: can CAs use local message passing to achieve global agreement on what digit they compose?\n\nAuthors: Ettore Randazzo, Alexander Mordvintsev, Eyvind Niklasson, Michael Levin, Sam Greydanus\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "430": "#summarization #gpt3 #openai\n\nText Summarization is a hard task, both in training and evaluation. Training is usually done maximizing the log-likelihood of a human-generated reference summary, while evaluation is performed using overlap-based metrics like ROUGE. Both significantly undervalue the breadth and intricacies of language and the nature of the information contained in text summaries. This paper by OpenAI includes direct human feedback both in evaluation and - via reward model proxies - in training. The final model even outperforms single humans when judged by other humans and is an interesting application of using reinforcement learning together with humans in the loop.\n\nOUTLINE:\n0:00 - Intro & Overview\n5:35 - Summarization as a Task\n7:30 - Problems with the ROUGE Metric\n10:10 - Training Supervised Models\n12:30 - Main Results\n16:40 - Including Human Feedback with Reward Models & RL\n26:05 - The Unknown Effect of Better Data\n28:30 - KL Constraint & Connection to Adversarial Examples\n37:15 - More Results\n39:30 - Understanding the Reward Model\n41:50 - Limitations & Broader Impact\n\nPaper: https://arxiv.org/abs/2009.01325\nBlog: https://openai.com/blog/learning-to-summarize-with-human-feedback/\nCode: https://github.com/openai/summarize-from-feedback\nSamples: https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/\n\nMy Video on GPT-3: https://youtu.be/SY5PvZrJhLE\nMy Video on GPT-2: https://youtu.be/u1_qMdb0kYU\n\nAbstract:\nAs language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\n\nAuthors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "431": "#ai #research #hardware\n\nWe like to think that ideas in research succeed because of their merit, but this story is likely incomplete. The term \"hardware lottery\" describes the fact that certain algorithmic ideas are successful because they happen to be suited well to the prevalent hardware, whereas other ideas, which would be equally viable, are left behind because no accelerators for them exists. This paper is part history, part opinion and gives lots of inputs to think about.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - The Hardware Lottery\n8:30 - Sections Overview\n11:30 - Why ML researchers are disconnected from hardware\n16:50 - Historic Examples of Hardware Lotteries\n29:05 - Are we in a Hardware Lottery right now?\n39:55 - GPT-3 as an Example\n43:40 - Comparing Scaling Neural Networks to Human Brains\n46:00 - The Way Forward\n49:25 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2009.06489\nWebsite: https://hardwarelottery.github.io/\n\nAbstract:\nHardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which makes it increasingly costly to stray off of the beaten path of research ideas.\n\nAuthors: Sara Hooker\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "432": "#ai #research #optimization\n\nOptimization is still the domain of hand-crafted, simple algorithms. An ML engineer not only has to pick a suitable one for their problem but also often do grid-search over various hyper-parameters. This paper proposes to learn a single, unified optimization algorithm, given not by an equation, but by an LSTM-based neural network, to act as an optimizer for any deep learning problem, and ultimately to optimize itself.\n\nOUTLINE:\n0:00 - Intro & Outline\n2:20 - From Hand-Crafted to Learned Features\n4:25 - Current Optimization Algorithm\n9:40 - Learned Optimization\n15:50 - Optimizer Architecture\n22:50 - Optimizing the Optimizer using Evolution Strategies\n30:30 - Task Dataset\n34:00 - Main Results\n36:50 - Implicit Regularization in the Learned Optimizer\n41:05 - Generalization across Tasks\n41:40 - Scaling Up\n45:30 - The Learned Optimizer Trains Itself\n47:20 - Pseudocode\n49:45 - Broader Impact Statement\n52:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2009.11243\n\nAbstract:\nMuch as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.\n\nAuthors: Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, Jascha Sohl-Dickstein\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "433": "#ai #research #transformers\n\nTransformers are Ruining Convolutions. This paper, under review at ICLR, shows that given enough data, a standard Transformer can outperform Convolutional Neural Networks in image recognition tasks, which are classically tasks where CNNs excel. In this Video, I explain the architecture of the Vision Transformer (ViT), the reason why it works better and rant about why double-bline peer review is broken.\n\nOUTLINE:\n0:00 - Introduction\n0:30 - Double-Blind Review is Broken\n5:20 - Overview\n6:55 - Transformers for Images\n10:40 - Vision Transformer Architecture\n16:30 - Experimental Results\n18:45 - What does the Model Learn?\n21:00 - Why Transformers are Ruining Everything\n27:45 - Inductive Biases in Transformers\n29:05 - Conclusion & Comments\n\nPaper (Under Review): https://openreview.net/forum?id=YicbFdNTTy\nArxiv version: https://arxiv.org/abs/2010.11929\n\nBiT Paper: https://arxiv.org/pdf/1912.11370.pdf\nImageNet-ReaL Paper: https://arxiv.org/abs/2006.07159\n\nMy Video on BiT (Big Transfer): https://youtu.be/k1GOF2jmX7c\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\nMy Video on BERT: https://youtu.be/-9evrZnBorM\nMy Video on ResNets: https://youtu.be/GWt6Fu05voI\n\n\nAbstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to sequences of image patches. When pre-trained on large amounts of data and transferred to multiple recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc), Vision Transformer attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\nAuthors: Anonymous / Under Review\n\nErrata:\n- Patches are not flattened, but vectorized\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "434": "#ai #research #optimization\n\nDeep Learning famously gives rise to very complex, non-linear optimization problems that cannot be solved analytically. Therefore, the choice of a suitable optimization algorithm can often make or break the training of a Deep Neural Network. Yet, the literature is full with hundreds of different algorithms, each claiming to be superior and selecting one of them is mostly done based on popular opinion or anecdotes. This paper investigates 14 of the most popular optimizers in a standardized benchmark and even though there is no clear winner, it can give some recommendations as a result.\n\nOUTLINE:\n0:00 - Introduction & Overview\n2:15 - The Overwhelming Amount of Optimizers\n5:50 - Compared Optimizers\n6:50 - Default Parameters & Tuning Distribution\n13:10 - Deep Learning Problems Considered\n16:45 - Tuning on Single Seeds\n23:15 - Results & Interpretation\n34:00 - Learning Rate Schedules & Noise\n36:10 - Conclusions & Comments\n\nPaper: https://arxiv.org/abs/2007.01547\nRaw Results: https://github.com/SirRob1997/Crowded-Valley---Results\n\nAbstract:\nChoosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.\n\nAuthors: Robin M. Schmidt, Frank Schneider, Philipp Hennig\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "435": "#ai #research #attention\n\nTransformers, having already captured NLP, have recently started to take over the field of Computer Vision. So far, the size of images as input has been challenging, as the Transformers' Attention Mechanism's memory requirements grows quadratic in its input size. LambdaNetworks offer a way around this requirement and capture long-range interactions without the need to build expensive attention maps. They reach a new state-of-the-art in ImageNet and compare favorably to both Transformers and CNNs in terms of efficiency.\n\nOUTLINE:\n0:00 - Introduction & Overview\n6:25 - Attention Mechanism Memory Requirements\n9:30 - Lambda Layers vs Attention Layers\n17:10 - How Lambda Layers Work\n31:50 - Attention Re-Appears in Lambda Layers\n40:20 - Positional Encodings\n51:30 - Extensions and Experimental Comparisons\n58:00 - Code\n\nPaper: https://openreview.net/forum?id=xTJEN-ggl1b\nLucidrains' Code: https://github.com/lucidrains/lambda-networks\n\nAbstract:\nWe present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda layer, captures such interactions by transforming available contexts into linear functions,  termed lambdas,  and applying these linear functions to each input separately.  Lambda layers are versatile and may be implemented to model content and position-based interactions in global, local or masked contexts.  As they bypass the need for expensive attention maps, lambda layers can routinely be applied to inputs of length in the thousands, en-abling their applications to long sequences or high-resolution images. The resulting neural network architectures, LambdaNetworks, are computationally efficient and simple to implement using direct calls to operations available in modern neural network libraries.  Experiments on ImageNet classification and COCO object detection  and  instance  segmentation  demonstrate  that  LambdaNetworks  significantly  outperform  their  convolutional  and  attentional  counterparts  while  being more computationally efficient. Finally, we introduce LambdaResNets, a family of LambdaNetworks, that considerably improve the speed-accuracy tradeoff of image classification models. LambdaResNets reach state-of-the-art accuracies on ImageNet while being \u223c4.5x faster than the popular EfficientNets on modern machine learning accelerators.\n\nAuthors: Anonymous\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "436": "#ai #research #attention\n\nTransformers have huge memory and compute requirements because they construct an Attention matrix, which grows quadratically in the size of the input. The Performer is a model that uses random positive orthogonal features to construct an unbiased estimator to the Attention matrix and obtains an arbitrarily good approximation in linear time! The method generalizes beyond attention and opens the door to the next generation of deep learning architectures.\n\nOUTLINE:\n0:00 - Intro & Outline\n6:15 - Quadratic Bottleneck in Attention Mechanisms\n10:00 - Decomposing the Attention Matrix\n15:30 - Approximating the Softmax Kernel\n24:45 - Different Choices, Different Kernels\n28:00 - Why the Naive Approach does not work!\n31:30 - Better Approximation via Positive Features\n36:55 - Positive Features are Infinitely Better\n40:10 - Orthogonal Features are Even Better\n43:25 - Experiments\n49:20 - Broader Impact Statement\n50:00 - Causal Attention via Prefix Sums\n52:10 - Code\n53:50 - Final Remarks & Conclusion\n\nPaper: https://arxiv.org/abs/2009.14794\nCode: https://github.com/google-research/google-research/tree/master/performer\nBlog: https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html\n\nKernels on ML Street Talk: https://www.youtube.com/watch?v=y_RjsDHl5Y4\nMy Video on Linformer: https://www.youtube.com/watch?v=-_2AF9Lhweo\nMy Video on Reformer: https://www.youtube.com/watch?v=i4H0kjxrias\nMy Video on Attention: https://www.youtube.com/watch?v=iDulhoQ2pro\n\nAbstract:\nWe introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\nAuthors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "437": "#ai #research #nlp\n\nKnowledge Graphs are structured databases that capture real-world entities and their relations to each other. KGs are usually built by human experts, which costs considerable amounts of time and money. This paper hypothesizes that language models, which have increased their performance dramatically in the last few years, contain enough knowledge to use them to construct a knowledge graph from a given corpus, without any fine-tuning of the language model itself. The resulting system can uncover new, unknown relations and outperforms all baselines in automated KG construction, even trained ones!\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - TabNine Promotion\n4:20 - Title Misnomer\n6:45 - From Corpus To Knowledge Graph\n13:40 - Paper Contributions\n15:50 - Candidate Fact Finding Algorithm\n25:50 - Causal Attention Confusion\n31:25 - More Constraints\n35:00 - Mapping Facts To Schemas\n38:40 - Example Constructed Knowledge Graph\n40:10 - Experimental Results\n47:25 - Example Discovered Facts\n50:40 - Conclusion & My Comments\n\nPaper: https://arxiv.org/abs/2010.11967\n\nAbstract:\nThis paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.\n\nAuthors: Chenguang Wang, Xiao Liu, Dawn Song\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "438": "#ai #research #machinelearning\n\nDeep Learning models are often overparameterized and have many degrees of freedom, which leads to many local minima that all perform equally well on the test set. But it turns out that even though they all generalize in-distribution, the performance of these models can be drastically different when tested out-of-distribution. Notably, in many cases, a good model can actually be found among all these candidates, but it seems impossible to select it. This paper describes this problem, which it calls underspecification, and gives several theoretical and practical examples.\n\nOUTLINE:\n0:00 - Into & Overview\n2:00 - Underspecification of ML Pipelines\n11:15 - Stress Tests\n12:40 - Epidemiological Example\n20:45 - Theoretical Model\n26:55 - Example from Medical Genomics\n34:00 - ImageNet-C Example\n36:50 - BERT Models\n56:55 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2011.03395\n\nAbstract:\nML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.\n\nAuthors: Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "439": "#ai #research #engineering\n\nNumerical solvers for Partial Differential Equations are notoriously slow. They need to evolve their state by tiny steps in order to stay accurate, and they need to repeat this for each new problem. Neural Fourier Operators, the architecture proposed in this paper, can evolve a PDE in time by a single forward pass, and do so for an entire family of PDEs, as long as the training set covers them well. By performing crucial operations only in Fourier Space, this new architecture is also independent of the discretization or sampling of the underlying signal and has the potential to speed up many scientific applications.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:15 - Navier Stokes Problem Statement\n11:00 - Formal Problem Definition\n15:00 - Neural Operator\n31:30 - Fourier Neural Operator\n48:15 - Experimental Examples\n50:35 - Code Walkthrough\n1:01:00 - Summary & Conclusion\n\nPaper: https://arxiv.org/abs/2010.08895\nBlog: https://zongyi-li.github.io/blog/2020/fourier-pde/\nCode: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_3d.py\nMIT Technology Review: https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/\n\nAbstract:\nThe classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.\n\nAuthors: Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "440": "#ai #biology #neuroscience\n\nBackpropagation is the workhorse of modern deep learning and a core component of most frameworks, but it has long been known that it is not biologically plausible, driving a divide between neuroscience and machine learning. This paper shows that Predictive Coding, a much more biologically plausible algorithm, can approximate Backpropagation for any computation graph, which they verify experimentally by building and training CNNs and LSTMs using Predictive Coding. This suggests that the brain and deep neural networks could be much more similar than previously believed.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Backpropagation & Biology\n7:40 - Experimental Results\n8:40 - Predictive Coding\n29:00 - Pseudocode\n32:10 - Predictive Coding approximates Backprop\n35:00 - Hebbian Updates\n36:35 - Code Walkthrough\n46:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.04182\nCode: https://github.com/BerenMillidge/PredictiveCodingBackprop\n\nAbstract:\nBackpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.\n\nAuthors: Beren Millidge, Alexander Tschantz, Christopher L. Buckley\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "441": "#deepmind #biology #ai\n\nThis is Biology's AlexNet moment! DeepMind solves a 50-year old problem in Protein Folding Prediction. AlphaFold 2 improves over DeepMind's 2018 AlphaFold system with a new architecture and massively outperforms all competition. In this Video, we take a look at how AlphaFold 1 works and what we can gather about AlphaFold 2 from the little information that's out there.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Proteins & Protein Folding\n14:20 - AlphaFold 1 Overview\n18:20 - Optimizing a differentiable geometric model at inference\n25:40 - Learning the Spatial Graph Distance Matrix\n31:20 - Multiple Sequence Alignment of Evolutionarily Similar Sequences\n39:40 - Distance Matrix Output Results\n43:45 - Guessing AlphaFold 2 (it's Transformers)\n53:30 - Conclusion & Comments\n\nAlphaFold 2 Blog: https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\nAlphaFold 1 Blog: https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery\nAlphaFold 1 Paper: https://www.nature.com/articles/s41586-019-1923-7\nMSA Reference: https://arxiv.org/abs/1211.1281\nCASP14 Challenge: https://predictioncenter.org/casp14/index.cgi\nCASP14 Result Bar Chart: https://www.predictioncenter.org/casp14/zscores_final.cgi\n\nPaper Title: High Accuracy Protein Structure Prediction Using Deep Learning\n\nAbstract:\nProteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and what a protein does largely depends on its unique 3D structure. Figuring out what shapes proteins fold into is known as the \u201cprotein folding problem\u201d, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.\n\nAuthors: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \u017d\u00eddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "442": "#ai #technology #poker\n\nThis paper does for Poker what AlphaZero has done for Chess & Go. The combination of Self-Play Reinforcement Learning and Tree Search has had tremendous success in perfect-information games, but transferring such techniques to imperfect information games is a hard problem. Not only does ReBeL solve this problem, but it provably converges to a Nash Equilibrium and delivers a superhuman Heads Up No-Limit Hold'em bot with very little domain knowledge.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:20 - Rock, Paper, and Double Scissor\n10:00 - AlphaZero Tree Search\n18:30 - Notation Setup: Infostates & Nash Equilibria\n31:45 - One Card Poker: Introducing Belief Representations\n45:00 - Solving Games in Belief Representation\n55:20 - The ReBeL Algorithm\n1:04:00 - Theory & Experiment Results\n1:07:00 - Broader Impact\n1:10:20 - High-Level Summary\n\nPaper: https://arxiv.org/abs/2007.13544\nCode: https://github.com/facebookresearch/rebel\nBlog: https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/\n\nERRATA: As someone last video pointed out: This is not the best Poker algorithm, but the best one that uses very little expert knowledge.\n\nAbstract:\nThe combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.\n\nAuthors: Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "443": "#ai #privacy #tech\n\nThis paper demonstrates a method to extract verbatim pieces of the training data from a trained language model. Moreover, some of the extracted pieces only appear a handful of times in the dataset. This points to serious security and privacy implications for models like GPT-3. The authors discuss the risks and propose mitigation strategies.\n\nOUTLINE:\n0:00 - Intro & Overview\n9:15 - Personal Data Example\n12:30 - Eidetic Memorization & Language Models\n19:50 - Adversary's Objective & Outlier Data\n24:45 - Ethical Hedging\n26:55 - Two-Step Method Overview\n28:20 - Perplexity Baseline\n30:30 - Improvement via Perplexity Ratios\n37:25 - Weights for Patterns & Weights for Memorization\n43:40 - Analysis of Main Results\n1:00:30 - Mitigation Strategies\n1:01:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2012.07805\n\nAbstract:\nIt has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\n\nAuthors: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "444": "#openai #science #gpt3\n\nOpenAI's newest model, DALL\u00b7E, shows absolutely amazing abilities in generating high-quality images from arbitrary text descriptions. Like GPT-3, the range of applications and the diversity of outputs is astonishing, given that this is a single model, trained on a purely autoregressive task. This model is a significant step towards the combination of text and images in future AI applications.\n\nOUTLINE:\n0:00 - Introduction\n2:45 - Overview\n4:20 - Dataset\n5:35 - Comparison to GPT-3\n7:00 - Model Architecture\n13:20 - VQ-VAE\n21:00 - Combining VQ-VAE with GPT-3\n27:30 - Pre-Training with Relaxation\n32:15 - Experimental Results\n33:00 - My Hypothesis about DALL\u00b7E's inner workings\n36:15 - Sparse Attention Patterns\n38:00 - DALL\u00b7E can't count\n39:35 - DALL\u00b7E can't global order\n40:10 - DALL\u00b7E renders different views\n41:10 - DALL\u00b7E is very good at texture\n41:40 - DALL\u00b7E can complete a bust\n43:30 - DALL\u00b7E can do some reflections, but not others\n44:15 - DALL\u00b7E can do cross-sections of some objects\n45:50 - DALL\u00b7E is amazing at style\n46:30 - DALL\u00b7E can generate logos\n47:40 - DALL\u00b7E can generate bedrooms\n48:35 - DALL\u00b7E can combine unusual concepts\n49:25 - DALL\u00b7E can generate illustrations\n50:15 - DALL\u00b7E sometimes understands complicated prompts\n50:55 - DALL\u00b7E can pass part of an IQ test\n51:40 - DALL\u00b7E probably does not have geographical / temporal knowledge\n53:10 - Reranking dramatically improves quality\n53:50 - Conclusions & Comments\n\nBlog: https://openai.com/blog/dall-e/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "445": "#ai #openai #technology\n\nPaper Title: Learning Transferable Visual Models From Natural Language Supervision\nCLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new image & text tasks.\n\nOUTLINE:\n0:00 - Introduction\n3:15 - Overview\n4:40 - Connecting Images & Text\n9:00 - Building Zero-Shot Classifiers\n14:40 - CLIP Contrastive Training Objective\n22:25 - Encoder Choices\n25:00 - Zero-Shot CLIP vs Linear ResNet-50\n31:50 - Zero-Shot vs Few-Shot\n35:35 - Scaling Properties\n36:35 - Comparison on different tasks\n37:40 - Robustness to Data Shift\n44:20 - Broader Impact Section\n47:00 - Conclusion & Comments\n\nPaper: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf\nBlog: https://openai.com/blog/clip/\nCode: https://github.com/openai/CLIP\n\nAbstract:\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "446": "#ai #technology #switchtransformer\n\nScale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model's parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:30 - Performance Gains from Scale\n8:30 - Switch Transformer Architecture\n17:00 - Model-, Data- and Expert-Parallelism\n25:30 - Experimental Results\n29:00 - Stabilizing Training\n32:20 - Distillation into Dense Models\n33:30 - Final Comments\n\nPaper: https://arxiv.org/abs/2101.03961\nCodebase T5: https://github.com/google-research/text-to-text-transfer-transformer\n\nAbstract:\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n\nAuthors: William Fedus, Barret Zoph, Noam Shazeer\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "447": "#ai #research #blockchain\n\nBig Tech is currently dominating the pursuit of ever more capable AI. This happens behind closed doors and results in a monopoly of power. SingularityNET is an open, decentralized network where anyone can offer and consume AI services, and where AI agents can interlink with each other to provide ever more sophisticated AI, with the goal to create a singularity that's beneficial for humanity. This video takes a look at the basics behind SingularityNET and some of its core components.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:55 - Document Summarization Example Workflow\n5:50 - Why AI needs a Marketplace?\n9:20 - A network of APIs\n12:30 - AI Evaluators & Matchmakers\n15:00 - My criticisms of the Marketplace\n17:45 - What is on the Blockchain?\n20:45 - AI Marketplace Demo\n22:00 - The AGI Token & Inflation\n26:30 - Reputation System & other features\n30:00 - Democratic Governance\n33:00 - Benefit Tasks\n36:15 - My general thoughts on the application examples\n38:05 - Measuring Intelligence on SingularityNET\n45:15 - OfferNet Economy\n50:00 - Summary & Comments\n\nWhitepaper: https://public.singularitynet.io/whitepaper.pdf\nWebsite: https://singularitynet.io/\nAI Marketplace: https://beta.singularitynet.io/aimarketplace\n\nReferences:\nhttps://www.hansonrobotics.com/wp-content/uploads/2018/12/Using-Tononi-Phi-to-Measure-Consciousness-of-a-Cognitive-System-While-Reading-and-Conversing.pdf\nhttps://arxiv.org/pdf/1601.02626.pdf\nhttps://blog.singularitynet.io/singularitynet-the-past-the-present-and-the-future-7bacb2b8e7f0\nhttps://blog.singularitynet.io/singularitynet-supervisory-council-e7c513fd3ea6\nhttps://blog.singularitynet.io/singularitynet-phase-two-massive-token-utilization-toward-decentralized-beneficial-agi-6e3ac5a5b44a\n\nADDENDUM:\nI forgot to mention one important example for the utility of dynamic matchmaking: If I have a German text to summarize, and there is a German summarizer, but there is also a better English one, a clever AI could figure out for me whether to use the German one or whether to use a translator to English, then the English summarizer, then a backtranslator. And it could even do so depending on the input text.\n\nAbstract:\n[...] Most AI research today is controlled by a handful of corporations\u2014those with\nthe resources to fund development. Independent developers of AI tools have no\nreadily available way to monetize their creations. Usually, their most lucrative\noption is to sell their tool to one of the big tech companies, leading to control of\nthe technology becoming even more concentrated. SingularityNET\u2019s open-source\nprotocol and collection of smart contracts are designed to address these problems.\nDevelopers can launch their AI tools on the network, where they can interoperate\nwith other AIs and with paying users.\nNot only does the SingularityNET platform give developers a commercial\nlaunchpad (much like app stores give mobile app developers an easy path to\nmarket), it also allows the AIs to interoperate, creating a more synergistic, broadly\ncapable intelligence. For example, if a text-to-speech AI and an Italian-to-English\ntranslation AI were both on the network, then the network as a whole would be\ncapable of using Italian text to produce English speech.\nWithin this framework, AI transforms from a corporate asset to a global\ncommons; anyone can access AI tech or become a stakeholder in its development.\nAlso, anyone can add an AI/machine learning service to SingularityNET for use\nby the network and receive network payment tokens in exchange. [...]\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "448": "#ai #science #transformers\n\nAutoregressive Transformers have taken over the world of Language Modeling (GPT-3). However, in order to train them, people use causal masking and sample parallelism, which means computation only happens in a feedforward manner. This results in higher layer information, which would be available, to not be used in the lower layers of subsequent tokens, and leads to a loss in the computational capabilities of the overall model. Feedback Transformers trade-off training speed for access to these representations and demonstrate remarkable improvements in complex reasoning and long-range dependency tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:55 - Problems of Autoregressive Processing\n3:30 - Information Flow in Recurrent Neural Networks\n7:15 - Information Flow in Transformers\n9:10 - Solving Complex Computations with Neural Networks\n16:45 - Causal Masking in Transformers\n19:00 - Missing Higher Layer Information Flow\n26:10 - Feedback Transformer Architecture\n30:00 - Connection to Attention-RNNs\n36:00 - Formal Definition\n37:05 - Experimental Results\n43:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2002.09402\n\nMy video on Attention: https://youtu.be/iDulhoQ2pro\n\nERRATA: Sometimes I say \"Switch Transformer\" instead of \"Feedback Transformer\". Forgive me :)\n\nAbstract:\nTransformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\n\nAuthors: Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "449": "#deeplearning #kernels #neuralnetworks\n\nFull Title: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine\n\nDeep Neural Networks are often said to discover useful representations of the data. However, this paper challenges this prevailing view and suggest that rather than representing the data, deep neural networks store superpositions of the training data in their weights and act as kernel machines at inference time. This is a theoretical paper with a main theorem and an understandable proof and the result leads to many interesting implications for the field.\n\nOUTLINE:\n0:00 - Intro & Outline\n4:50 - What is a Kernel Machine?\n10:25 - Kernel Machines vs Gradient Descent\n12:40 - Tangent Kernels\n22:45 - Path Kernels\n25:00 - Main Theorem\n28:50 - Proof of the Main Theorem\n39:10 - Implications & My Comments\n\nPaper: https://arxiv.org/abs/2012.00152\nStreet Talk about Kernels: https://youtu.be/y_RjsDHl5Y4\n\nERRATA: I simplify a bit too much when I pit kernel methods against gradient descent. Of course, you can even learn kernel machines using GD, they're not mutually exclusive. And it's also not true that you \"don't need a model\" in kernel machines, as it usually still contains learned parameters.\n\nAbstract:\nDeep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.\n\nAuthors: Pedro Domingos\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "450": "#transformer #nystromer #nystromformer\n\nThe Nystr\u00f6mformer (or Nystromformer, Nystr\u00f6mer, Nystromer), is a new drop-in replacement for approximating the Self-Attention matrix in Transformers with linear memory and time requirements. Most importantly, it uses the Nystrom-Method to subselect (or segment mean) queries and keys as so-called landmarks and uses those to reconstruct the inherently low-rank attention matrix. This is relevant for many areas of Machine Learning, especially Natural Language processing, where it enables longer sequences of text to be processed at once.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - The Quadratic Memory Bottleneck in Self-Attention\n7:20 - The Softmax Operation in Attention\n11:15 - Nystr\u00f6m-Approximation\n14:00 - Getting Around the Softmax Problem\n18:05 - Intuition for Landmark Method\n28:05 - Full Algorithm\n30:20 - Theoretical Guarantees\n35:55 - Avoiding the Large Attention Matrix\n36:55 - Subsampling Keys vs Negative Sampling\n43:15 - Experimental Results\n47:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.03902\nCode: https://github.com/mlpen/Nystromformer\nAppendix: https://github.com/mlpen/Nystromformer/blob/main/doc/Nystromformer_Supplement.pdf\nLRA Results: https://twitter.com/tanmingxing/status/1359301186734620675\nTwitter lucidrains w/ author: https://twitter.com/lucidrains/status/1359597104075661312\nTwitter lucidrains w/ _clashluke: https://twitter.com/_clashluke/status/1359483460851802115\n\nAbstract:\nTransformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -- a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard Transformer. Our code is at this https URL.\n\nAuthors: Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "451": "#nfnets #deepmind #machinelearning\n\nBatch Normalization is a core component of modern deep learning. It enables training at higher batch sizes, prevents mean shift, provides implicit regularization, and allows networks to reach higher performance than without. However, BatchNorm also has disadvantages, such as its dependence on batch size and its computational overhead, especially in distributed settings. Normalizer-Free Networks, developed at Google DeepMind, are a class of CNNs that achieve state-of-the-art classification accuracy on ImageNet without batch normalization. This is achieved by using adaptive gradient clipping (AGC), combined with a number of improvements in general network architecture. The resulting networks train faster, are more accurate, and provide better transfer learning performance. Code is provided in Jax.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:40 - What's the problem with BatchNorm?\n11:00 - Paper contribution Overview\n13:30 - Beneficial properties of BatchNorm\n15:30 - Previous work: NF-ResNets\n18:15 - Adaptive Gradient Clipping\n21:40 - AGC and large batch size\n23:30 - AGC induces implicit dependence between training samples\n28:30 - Are BatchNorm's problems solved?\n30:00 - Network architecture improvements\n31:10 - Comparison to EfficientNet\n33:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.06171\nCode: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nMy Video on BatchNorm: https://www.youtube.com/watch?v=OioFONrSETc\nMy Video on ResNets: https://www.youtube.com/watch?v=GWt6Fu05voI\n\nERRATA (from Lucas Beyer): \"I believe you missed the main concern with \"batch cheating\". It's for losses that act on the full batch, as opposed to on each sample individually.\nFor example, triplet in FaceNet or n-pairs in CLIP. BN allows for \"shortcut\" solution to loss. See also BatchReNorm paper.\"\n\nAbstract:\nBatch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. Our code is available at this https URL deepmind-research/tree/master/nfnets\n\nAuthors: Andrew Brock, Soham De, Samuel L. Smith, Karen Simonyan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "452": "#transformer #gan #machinelearning\n\nGenerative Adversarial Networks (GANs) hold the state-of-the-art when it comes to image generation. However, while the rest of computer vision is slowly taken over by transformers or other attention-based architectures, all working GANs to date contain some form of convolutional layers. This paper changes that and builds TransGAN, the first GAN where both the generator and the discriminator are transformers. The discriminator is taken over from ViT (an image is worth 16x16 words), and the generator uses pixelshuffle to successfully up-sample the generated resolution. Three tricks make training work: Data augmentations using DiffAug, an auxiliary superresolution task, and a localized initialization of self-attention. Their largest model reaches competitive performance with the best convolutional GANs on CIFAR10, STL-10, and CelebA.\n\nOUTLINE:\n0:00 - Introduction & Overview\n3:05 - Discriminator Architecture\n5:25 - Generator Architecture\n11:20 - Upsampling with PixelShuffle\n15:05 - Architecture Recap\n16:00 - Vanilla TransGAN Results\n16:40 - Trick 1: Data Augmentation with DiffAugment\n19:10 - Trick 2: Super-Resolution Co-Training\n22:20 - Trick 3: Locality-Aware Initialization for Self-Attention\n27:30 - Scaling Up & Experimental Results\n28:45 - Recap & Conclusion\n\nPaper: https://arxiv.org/abs/2102.07074\nCode: https://github.com/VITA-Group/TransGAN\nMy Video on ViT: https://youtu.be/TrdevFK_am4\n\nAbstract:\nThe recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \\textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \\textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \\textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA 64\u00d764, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at \\url{this https URL}.\n\nAuthors: Yifan Jiang, Shiyu Chang, Zhangyang Wang\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "453": "#dreamer #deeprl #reinforcementlearning\n\nModel-Based Reinforcement Learning has been lagging behind Model-Free RL on Atari, especially among single-GPU algorithms. This collaboration between Google AI, DeepMind, and the University of Toronto (UofT) pushes world models to the next level. The main contribution is a learned latent state consisting of one discrete part and one stochastic part, whereby the stochastic part is a set of 32 categorical variables, each with 32 possible values. The world model can freely decide how it wants to use these variables to represent the input, but is tasked with the prediction of future observations and rewards. This procedure gives rise to an informative latent representation and in a second step, reinforcement learning (A2C Actor-Critic) can be done purely - and very efficiently - on the basis of the world-model's latent states. No observations needed! This paper combines this with straight-through estimators, KL balancing, and many other tricks to achieve state-of-the-art single-GPU performance in Atari.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:50 - Short Recap of Reinforcement Learning\n6:05 - Problems with Model-Free Reinforcement Learning\n10:40 - How World Models Help\n12:05 - World Model Learner Architecture\n16:50 - Deterministic & Stochastic Hidden States\n18:50 - Latent Categorical Variables\n22:00 - Categorical Variables and Multi-Modality\n23:20 - Sampling & Stochastic State Prediction\n30:55 - Actor-Critic Learning in Dream Space\n32:05 - The Incompleteness of Learned World Models\n34:15 - How General is this Algorithm?\n37:25 - World Model Loss Function\n39:20 - KL Balancing\n40:35 - Actor-Critic Loss Function\n41:45 - Straight-Through Estimators for Sampling Backpropagation\n46:25 - Experimental Results\n52:00 - Where Does It Fail?\n54:25 - Conclusion\n\nPaper: https://arxiv.org/abs/2010.02193\nCode: https://github.com/danijar/dreamerv2\nAuthor Blog: https://danijar.com/project/dreamerv2/\nGoogle AI Blog: https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html\n\nERRATA (from the authors): \n- KL balancing (prior vs posterior within the KL) is different from beta VAEs (reconstruction vs KL)\n- The vectors of categoricals can in theory represent 32^32 different images so their capacity is quite large\n\nAbstract:\nIntelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.\n\nAuthors: Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "454": "#deberta #bert #huggingface\n\nDeBERTa by Microsoft is the next iteration of BERT-style Self-Attention Transformer models, surpassing RoBERTa in State-of-the-art in multiple NLP tasks. DeBERTa brings two key improvements: First, they treat content and position information separately in a new form of disentangled attention mechanism. Second, they resort to relative positional encodings throughout the base of the transformer, and provide absolute positional encodings only at the very end. The resulting model is both more accurate on downstream tasks and needs less pretraining steps to reach good accuracy. Models are also available in Huggingface and on Github.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:15 - Position Encodings in Transformer's Attention Mechanism\n9:55 - Disentangling Content & Position Information in Attention\n21:35 - Disentangled Query & Key construction in the Attention Formula\n25:50 - Efficient Relative Position Encodings\n28:40 - Enhanced Mask Decoder using Absolute Position Encodings\n35:30 - My Criticism of EMD\n38:05 - Experimental Results\n40:30 - Scaling up to 1.5 Billion Parameters\n44:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2006.03654\nCode: https://github.com/microsoft/DeBERTa\nHuggingface models: https://huggingface.co/models?search=deberta\n\nAbstract:\nRecent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).\n\nAuthors: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "455": "#fastweights #deeplearning #transformers\n\nTransformers are dominating Deep Learning, but their quadratic memory and compute requirements make them expensive to train and hard to use. Many papers have attempted to linearize the core module: the attention mechanism, using kernels - for example, the Performer. However, such methods are either not satisfactory or have other downsides, such as a reliance on random features. This paper establishes an intrinsic connection between linearized (kernel) attention and the much older Fast Weight Memory Systems, in part popularized by J\u00fcrgen Schmidhuber in the 90s. It shows the fundamental limitations of these algorithms and suggests new update rules and new kernels in order to fix these problems. The resulting model compares favorably to Performers on key synthetic experiments and real-world tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Fast Weight Systems\n7:00 - Distributed Storage of Symbolic Values\n12:30 - Autoregressive Attention Mechanisms\n18:50 - Connecting Fast Weights to Attention Mechanism\n22:00 - Softmax as a Kernel Method (Performer)\n25:45 - Linear Attention as Fast Weights\n27:50 - Capacity Limitations of Linear Attention\n29:45 - Synthetic Data Experimental Setup\n31:50 - Improving the Update Rule\n37:30 - Deterministic Parameter-Free Projection (DPFP) Kernel\n46:15 - Experimental Results\n50:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.11174\nCode: https://github.com/ischlag/fast-weight-transformers\nMachine Learning Street Talk on Kernels: https://youtu.be/y_RjsDHl5Y4\n\nAbstract:\nWe show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early '90s. From this observation we infer a memory capacity limitation of recent linearised softmax attention variants. With finite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.\n\nAuthors: Imanol Schlag, Kazuki Irie, J\u00fcrgen Schmidhuber\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "456": "#glom #hinton #capsules\n\nGeoffrey Hinton describes GLOM, a Computer Vision model that combines transformers, neural fields, contrastive learning, capsule networks, denoising autoencoders and RNNs. GLOM decomposes an image into a parse tree of objects and their parts. However, unlike previous systems, the parse tree is constructed dynamically and differently for each input, without changing the underlying neural network. This is done by a multi-step consensus algorithm that runs over different levels of abstraction at each location of an image simultaneously. GLOM is just an idea for now but suggests a radically new approach to AI visual scene understanding.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:10 - Object Recognition as Parse Trees\n5:40 - Capsule Networks\n8:00 - GLOM Architecture Overview\n13:10 - Top-Down and Bottom-Up communication\n18:30 - Emergence of Islands\n22:00 - Cross-Column Attention Mechanism\n27:10 - My Improvements for the Attention Mechanism\n35:25 - Some Design Decisions\n43:25 - Training GLOM as a Denoising Autoencoder & Contrastive Learning\n52:20 - Coordinate Transformations & Representing Uncertainty\n57:05 - How GLOM handles Video\n1:01:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2102.12627\n\nAbstract:\nThis paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language\n\nAuthors: Geoffrey Hinton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "457": "#openai #clip #microscope\n\nOpenAI does a huge investigation into the inner workings of their recent CLIP model via faceted feature visualization and finds amazing things: Some neurons in the last layer respond to distinct concepts across multiple modalities, meaning they fire for photographs, drawings, and signs depicting the same concept, even when the images are vastly distinct. Through manual examination, they identify and investigate neurons corresponding to persons, geographical regions, religions, emotions, and much more. In this video, I go through the publication and then I present my own findings from digging around in the OpenAI Microscope.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:35 - OpenAI Microscope\n7:10 - Categories of found neurons\n11:10 - Person Neurons\n13:00 - Donald Trump Neuron\n17:15 - Emotion Neurons\n22:45 - Region Neurons\n26:40 - Sparse Mixture of Emotions\n28:05 - Emotion Atlas\n29:45 - Adversarial Typographic Attacks\n31:55 - Stroop Test\n33:10 - My Findings in OpenAI Microscope\n33:30 - Superman Neuron\n33:50 - Resting B*tchface Neuron\n34:10 - Trash Bag Neuron\n35:25 - God Weightlifting Neuron\n36:40 - Organ Neuron\n38:35 - Film Spool Neuron\n39:05 - Feather Neuron\n39:20 - Spartan Neuron\n40:25 - Letter E Neuron\n40:35 - Cleanin Neuron\n40:45 - Frown Neuron\n40:55 - Lion Neuron\n41:05 - Fashion Model Neuron\n41:20 - Baseball Neuron\n41:50 - Bride Neuron\n42:00 - Navy Neuron\n42:30 - Hemp Neuron\n43:25 - Staircase Neuron\n43:45 - Disney Neuron\n44:15 - Hillary Clinton Neuron\n44:50 - God Neuron\n45:15 - Blurry Neuron\n45:35 - Arrow Neuron\n45:55 - Trophy Presentation Neuron\n46:10 - Receding Hairline Neuron\n46:30 - Traffic Neuron\n46:40 - Raised Hand Neuron\n46:50 - Google Maps Neuron\n47:15 - Nervous Smile Neuron\n47:30 - Elvis Neuron\n47:55 - The Flash Neuron\n48:05 - Beard Neuron\n48:15 - Kilt Neuron\n48:25 - Rainy Neuron\n48:35 - Electricity Neuron\n48:50 - Droplets Neuron\n49:00 - Escape Neuron\n49:25 - King Neuron\n49:35 - Country Neuron\n49:45 - Overweight Men Neuron\n49:55 - Wedding\n50:05 - Australia Neuron\n50:15 - Yawn Neuron\n50:30 - Bees & Simpsons Neuron\n50:40 - Mussles Neuron\n50:50 - Spice Neuron\n51:00 - Conclusion\n\nPaper: https://distill.pub/2021/multimodal-neurons/\nMy Findings: https://www.notion.so/CLIP-OpenAI-Microscope-Findings-27465eac373c451d8083428443e0837c\nMy Video on CLIP: https://youtu.be/T9XSU0pKX2E\nMy Video on Feature Visualizations & The OpenAI Microscope: https://youtu.be/Ok44otx90D4\n\nAbstract:\nIn 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry. The exciting thing wasn\u2019t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person\u2019s name. The neurons were multimodal. As the lead author would put it: \"You are looking at the far end of the transformation from metric, visual shapes to conceptual... information.\" We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. Like the biological multimodal neurons, these artificial neurons respond to the same subject in photographs, drawings, and images of their name.\n\nAuthors: Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "458": "#selfsupervisedlearning #yannlecun #facebookai\n\nDeep Learning systems can achieve remarkable, even super-human performance through supervised learning on large, labeled datasets. However, there are two problems: First, collecting ever more labeled data is expensive in both time and money. Second, these deep neural networks will be high performers on their task, but cannot easily generalize to other, related tasks, or they need large amounts of data to do so. In this blog post, Yann LeCun and Ishan Misra of Facebook AI Research (FAIR) describe the current state of Self-Supervised Learning (SSL) and argue that it is the next step in the development of AI that uses fewer labels and can transfer knowledge faster than current systems. They suggest as a promising direction to build non-contrastive latent-variable predictive models, like VAEs, but ones that also provide high-quality latent representations for downstream tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:15 - Supervised Learning, Self-Supervised Learning, and Common Sense\n7:35 - Predicting Hidden Parts from Observed Parts\n17:50 - Self-Supervised Learning for Language vs Vision\n26:50 - Energy-Based Models\n30:15 - Joint-Embedding Models\n35:45 - Contrastive Methods\n43:45 - Latent-Variable Predictive Models and GANs\n55:00 - Summary & Conclusion\n\nPaper (Blog Post): https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence\nMy Video on BYOL: https://www.youtube.com/watch?v=YPfUiOMYOEE\n\nERRATA:\n- The difference between loss and energy: Energy is for inference, loss is for training.\n- The R(z) term is a regularizer that restricts the capacity of the latent variable. I think I said both of those things, but never together.\n- The way I explain why BERT is contrastive is wrong. I haven't figured out why just yet, though :)\n\nVideo approved by Antonio.\n\nAbstract:\nWe believe that self-supervised learning (SSL) is one of the most promising ways to build such background knowledge and approximate a form of common sense in AI systems.\n\nAuthors: Yann LeCun, Ishan Misra\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "459": "#universalcomputation #pretrainedtransformers #finetuning\n\nLarge-scale pre-training and subsequent fine-tuning is a common recipe for success with transformer models in machine learning. However, most such transfer learning is done when a model is pre-trained on the same or a very similar modality to the final task to be solved. This paper demonstrates that transformers can be fine-tuned to completely different modalities, such as from language to vision. Moreover, they demonstrate that this can be done by freezing all attention layers, tuning less than .1% of all parameters. The paper further claims that language modeling is a superior pre-training task for such cross-domain transfer. The paper goes through various ablation studies to make its point.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - Frozen Pretrained Transformers\n4:50 - Evaluated Tasks\n10:05 - The Importance of Training LayerNorm\n17:10 - Modality Transfer\n25:10 - Network Architecture Ablation\n26:10 - Evaluation of the Attention Mask\n27:20 - Are FPTs Overfitting or Underfitting?\n28:20 - Model Size Ablation\n28:50 - Is Initialization All You Need?\n31:40 - Full Model Training Overfits\n32:15 - Again the Importance of Training LayerNorm\n33:10 - Conclusions & Comments\n\nPaper: https://arxiv.org/abs/2103.05247\nCode: https://github.com/kzl/universal-computation\n\nAbstract:\nWe investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.\n\nAuthors: Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "460": "Deploying AI/ML based applications is far from trivial. On top of the traditional DevOps challenges, you need to foster collaboration between multidisciplinary teams (data-scientists, data/ML engineers, software developers and DevOps), handle model and experiment versioning, data versioning, etc. Most ML/AI deployments involve significant manual work, but this is changing with the introduction of new frameworks that leverage cloud-native paradigms, Git and Kubernetes to automate the process of ML/AI-based application deployment.\n\nIn this session we will explain how ML Pipelines work, the main challenges and the different steps involved in producing models and data products (data gathering, preparation, training/AutoML, validation, model deployment, drift monitoring and so on). We will demonstrate how the development and deployment process can be greatly simplified and automated. We\u2019ll show how you can: a. maximize the efficiency and collaboration between the various teams, b. harness Git review processes to evaluate models, and c. abstract away the complexity of Kubernetes and DevOps.\n\nWe will demo how to enable continuous delivery of machine learning to production using Git, CI frameworks (e.g. GitHub Actions) with hosted Kubernetes, Kubeflow, MLOps orchestration tools (MLRun), and Serverless functions (Nuclio) using real-world application examples.\n\nPresenter:\n\nYaron Haviv, Co-Founder and CTO @Iguazio", "461": "#perceiver #deepmind #transformer\n\nInspired by the fact that biological creatures attend to multiple modalities at the same time, DeepMind releases its new Perceiver model. Based on the Transformer architecture, the Perceiver makes no assumptions on the modality of the input data and also solves the long-standing quadratic bottleneck problem. This is achieved by having a latent low-dimensional Transformer, where the input data is fed multiple times via cross-attention. The Perceiver's weights can also be shared across layers, making it very similar to an RNN. Perceivers achieve competitive performance on ImageNet and state-of-the-art on other modalities, all while making no architectural adjustments to input data.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:20 - Built-In assumptions of Computer Vision Models\n5:10 -  The Quadratic Bottleneck of Transformers\n8:00 - Cross-Attention in Transformers\n10:45 - The Perceiver Model Architecture & Learned Queries\n20:05 - Positional Encodings via Fourier Features\n23:25 - Experimental Results & Attention Maps\n29:05 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2103.03206\n\nMy Video on Transformers (Attention is All You Need): https://youtu.be/iDulhoQ2pro\n\nAbstract:\nBiological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.\n\nAuthors: Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "462": "#dreamcoder #programsynthesis #symbolicreasoning\n\nClassic Machine Learning struggles with few-shot generalization for tasks where humans can easily generalize from just a handful of examples, for example sorting a list of numbers. Humans do this by coming up with a short program, or algorithm, that explains the few data points in a compact way. DreamCoder emulates this by using neural guided search over a language of primitives, a library, that it builds up over time. By doing this, it can iteratively construct more and more complex programs by building on its own abstractions and therefore solve more and more difficult tasks in a few-shot manner by generating very short programs that solve the few given datapoints. The resulting system can not only generalize quickly but also delivers an explainable solution to its problems in form of a modular and hierarchical learned library. Combining this with classic Deep Learning for low-level perception is a very promising future direction.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:55 - DreamCoder System Architecture\n9:00 - Wake Phase: Neural Guided Search\n19:15 - Abstraction Phase: Extending the Internal Library\n24:30 - Dreaming Phase: Training Neural Search on Fictional Programs and Replays\n30:55 - Abstraction by Compressing Program Refactorings\n32:40 - Experimental Results on LOGO Drawings\n39:00 - Ablation Studies\n39:50 - Re-Discovering Physical Laws\n42:25 - Discovering Recursive Programming Algorithms\n44:20 - Conclusions & Discussion\n\nPaper: https://arxiv.org/abs/2006.08381\nCode: https://github.com/ellisk42/ec\n\nAbstract:\nExpert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.\n\nAuthors: Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, Joshua B. Tenenbaum\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "463": "#nerf #neuralrendering #deeplearning\n\nView Synthesis is a tricky problem, especially when only given a sparse set of images as an input. NeRF embeds an entire scene into the weights of a feedforward neural network, trained by backpropagation through a differential volume rendering procedure, and achieves state-of-the-art view synthesis. It includes directional dependence and is able to capture fine structural details, as well as reflection effects and transparency.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:50 - View Synthesis Task Description\n5:50 - The fundamental difference to classic Deep Learning\n7:00 - NeRF Core Concept\n15:30 - Training the NeRF from sparse views\n20:50 - Radiance Field Volume Rendering\n23:20 - Resulting View Dependence\n24:00 - Positional Encoding\n28:00 - Hierarchical Volume Sampling\n30:15 - Experimental Results\n33:30 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2003.08934\nWebsite & Code: https://www.matthewtancik.com/nerf\n\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\n\nAbstract:\nWe present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (\u03b8,\u03d5)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.\n\nAuthors: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "464": "#aiwinter #agi #embodiedcognition\n\nThe AI community has gone through regular cycles of AI Springs, where rapid progress gave rise to massive overconfidence, high funding, and overpromise, followed by these promises being unfulfilled, subsequently diving into periods of disenfranchisement and underfunding, called AI Winters. This paper examines the reasons for the repeated periods of overconfidence and identifies four fallacies that people make when they see rapid progress in AI.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:10 - AI Springs & AI Winters\n5:40 - Is the current AI boom overhyped?\n15:35 - Fallacy 1: Narrow Intelligence vs General Intelligence\n19:40 - Fallacy 2: Hard for humans doesn't mean hard for computers\n21:45 - Fallacy 3: How we call things matters\n28:15 - Fallacy 4: Embodied Cognition\n35:30 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.12871\n\nMy Video on Shortcut Learning: https://youtu.be/D-eg7k8YSfs\n\nAbstract:\nSince its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (\"AI spring\") and periods of disappointment, loss of confidence, and reduced funding (\"AI winter\"). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.\n\nAuthors: Melanie Mitchell\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "465": "#dino #facebook #selfsupervised\n\nSelf-Supervised Learning is the final frontier in Representation Learning: Getting useful features without any labels. Facebook AI's new system, DINO, combines advances in Self-Supervised Learning for Computer Vision with the new Vision Transformer (ViT) architecture and achieves impressive results without any labels. Attention maps can be directly interpreted as segmentation maps, and the obtained representations can be used for image retrieval and zero-shot k-nearest neighbor classifiers (KNNs).\n\nOUTLINE:\n0:00 - Intro & Overview\n6:20 - Vision Transformers\n9:20 - Self-Supervised Learning for Images\n13:30 - Self-Distillation\n15:20 - Building the teacher from the student by moving average\n16:45 - DINO Pseudocode\n23:10 - Why Cross-Entropy Loss?\n28:20 - Experimental Results\n33:40 - My Hypothesis why this works\n38:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.14294\nBlog: https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training\nCode: https://github.com/facebookresearch/dino\n\nMy Video on ViT: https://youtu.be/TrdevFK_am4\nMy Video on BYOL: https://youtu.be/YPfUiOMYOEE\n\nAbstract:\nIn this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\n\nAuthors: Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "466": "#involution #computervision #attention\n\nConvolutional Neural Networks (CNNs) have dominated computer vision for almost a decade by applying two fundamental principles: Spatial agnosticism and channel-specific computations. Involution aims to invert these principles and presents a spatial-specific computation, which is also channel-agnostic. The resulting Involution Operator and RedNet architecture are a compromise between classic Convolutions and the newer Local Self-Attention architectures and perform favorably in terms of computation accuracy tradeoff when compared to either.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Principles of Convolution\n10:50 - Towards spatial-specific computations\n17:00 - The Involution Operator\n20:00 - Comparison to Self-Attention\n25:15 - Experimental Results\n30:30 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2103.06255\nCode: https://github.com/d-li14/involution\n\nAbstract:\nConvolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL.\n\nAuthors: Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "467": "#ddpm #diffusionmodels #openai\n\nGANs have dominated the image generation space for the majority of the last decade. This paper shows for the first time, how a non-GAN model, a DDPM, can be improved to overtake GANs at standard evaluation metrics for image generation. The produced samples look amazing and other than GANs, the new model has a formal probabilistic foundation. Is there a future for GANs or are Diffusion Models going to overtake them for good?\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Denoising Diffusion Probabilistic Models\n11:30 - Formal derivation of the training loss\n23:00 - Training in practice\n27:55 - Learning the covariance\n31:25 - Improving the noise schedule\n33:35 - Reducing the loss gradient noise\n40:35 - Classifier guidance\n52:50 - Experimental Results\n\nPaper (this): https://arxiv.org/abs/2105.05233\nPaper (previous): https://arxiv.org/abs/2102.09672\nCode: https://github.com/openai/guided-diffusion\n\nAbstract:\nWe show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for sample quality using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\u00d7128, 4.59 on ImageNet 256\u00d7256, and 7.72 on ImageNet 512\u00d7512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.85 on ImageNet 512\u00d7512. We release our code at this https URL\n\nAuthors: Alex Nichol, Prafulla Dhariwal\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "468": "#expirespan #nlp #facebookai\n\nFacebook AI (FAIR) researchers present Expire-Span, a variant of Transformer XL that dynamically assigns expiration dates to previously encountered signals. Because of this, Expire-Span can handle sequences of many thousand tokens, while keeping the memory and compute requirements at a manageable level. It severely matches or outperforms baseline systems, while consuming much less resources. We discuss its architecture, advantages, and shortcomings.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Remembering the past in sequence models\n5:45 - Learning to expire past memories\n8:30 - Difference to local attention\n10:00 - Architecture overview\n13:45 - Comparison to Transformer XL\n18:50 - Predicting expiration masks\n32:30 - Experimental Results\n40:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2105.06548\nCode: https://github.com/facebookresearch/transformer-sequential\n\nADDENDUM: I mention several times that the gradient signal of the e quantity only occurs inside the R ramp. By that, I mean the gradient stemming from the model loss. The regularization loss acts also outside the R ramp.\n\nAbstract:\nAttention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.\n\nAuthors: Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, Angela Fan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "469": "#metarim #deeprl #catastrophicforgetting\n\nReinforcement Learning is very tricky in environments where the objective shifts over time. This paper explores agents in multi-task environments that are usually subject to catastrophic forgetting. Building on the concept of Recurrent Independent Mechanisms (RIM), the authors propose to separate the learning procedures for the mechanism parameters (fast) and the attention parameters (slow) and achieve superior results and more stability, and even better zero-shot transfer performance.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:30 - Recombining pieces of knowledge\n11:30 - Controllers as recurrent neural networks\n14:20 - Recurrent Independent Mechanisms\n21:20 - Learning at different time scales\n28:40 - Experimental Results & My Criticism\n44:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2105.08710\nRIM Paper: https://arxiv.org/abs/1909.10893\n\nAbstract:\nDecomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.\n\nAuthors: Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Sch\u00f6lkopf, Yoshua Bengio\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "470": "#reinforcementlearning #deepmind #agi\n\nWhat's the most promising path to creating Artificial General Intelligence (AGI)? This paper makes the bold claim that a learning agent maximizing its reward in a sufficiently complex environment will necessarily develop intelligence as a by-product, and that Reward Maximization is the best way to move the creation of AGI forward. The paper is a mix of philosophy, engineering, and futurism, and raises many points of discussion.\n\nOUTLINE:\n0:00 - Intro & Outline\n4:10 - Reward Maximization\n10:10 - The Reward-is-Enough Hypothesis\n13:15 - Abilities associated with intelligence\n16:40 - My Criticism\n26:15 - Reward Maximization through Reinforcement Learning\n31:30 - Discussion, Conclusion & My Comments\n\nPaper: https://www.sciencedirect.com/science/article/pii/S0004370221000862\n\nAbstract:\nIn this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.\n\nAuthors: David Silver, Satinder Singh, Doina Precup, Richard S. Sutton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "471": "#decisiontransformer #reinforcementlearning #transformer\n\nProper credit assignment over long timespans is a fundamental problem in reinforcement learning. Even methods designed to combat this problem, such as TD-learning, quickly reach their limits when rewards are sparse or noisy. This paper reframes offline reinforcement learning as a pure sequence modeling problem, with the actions being sampled conditioned on the given history and desired future rewards. This allows the authors to use recent advances in sequence modeling using Transformers and achieve competitive results in Offline RL benchmarks.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:15 - Offline Reinforcement Learning\n10:10 - Transformers in RL\n14:25 - Value Functions and Temporal Difference Learning\n20:25 - Sequence Modeling and Reward-to-go\n27:20 - Why this is ideal for offline RL\n31:30 - The context length problem\n34:35 - Toy example: Shortest path from random walks\n41:00 - Discount factors\n45:50 - Experimental Results\n49:25 - Do you need to know the best possible reward?\n52:15 - Key-to-door toy experiment\n56:00 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2106.01345\nWebsite: https://sites.google.com/berkeley.edu/decision-transformer\nCode: https://github.com/kzl/decision-transformer\n\nTrajectory Transformer: https://trajectory-transformer.github.io/\nUpside-Down RL: https://arxiv.org/abs/1912.02875\n\nAbstract:\nWe present a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\n\nAuthors: Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "472": "#implicitfunction #jax #autodiff\n\nMany problems in Machine Learning involve loops of inner and outer optimization. Finding update steps for the outer loop is usually difficult, because of the.need to differentiate through the inner loop's procedure over multiple steps. Such loop unrolling is very limited and constrained to very few steps. Other papers have found solutions around unrolling in very specific, individual problems. This paper proposes a unified framework for implicit differentiation of inner optimization procedures without unrolling and provides implementations that integrate seamlessly into JAX.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:05 - Automatic Differentiation of Inner Optimizations\n4:30 - Example: Meta-Learning\n7:45 - Unrolling Optimization\n13:00 - Unified Framework Overview & Pseudocode\n21:10 - Implicit Function Theorem\n25:45 - More Technicalities\n28:45 - Experiments\n\nERRATA:\n- Dataset Distillation is done with respect to the training set, not the validation or test set.\n\nPaper: https://arxiv.org/abs/2105.15183\nCode coming soon\n\nAbstract:\nAutomatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.\n\nAuthors: Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-L\u00f3pez, Fabian Pedregosa, Jean-Philippe Vert\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "473": "#reiforcementlearning #gan #imitationlearning\n\nLearning from demonstrations is a fascinating topic, but what if the demonstrations are not exactly the behaviors we want to learn? Can we adhere to a dataset of demonstrations and still achieve a specified goal? This paper uses GANs to combine goal-achieving reinforcement learning with imitation learning and learns to perform well at a given task while doing so in the style of a given presented dataset. The resulting behaviors include many realistic-looking transitions between the demonstrated movements.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Problem Statement\n6:10 - Reward Signals\n8:15 - Motion Prior from GAN\n14:10 - Algorithm Overview\n20:15 - Reward Engineering & Experimental Results\n30:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2104.02180\nMain Video: https://www.youtube.com/watch?v=wySUxZN_KbM\nSupplementary Video: https://www.youtube.com/watch?v=O6fBSMxThR4\n\nAbstract:\nSynthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character's behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.\n\nAuthors: Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, Angjoo Kanazawa\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "474": "#xcit #transformer #attentionmechanism\n\nAfter dominating Natural Language Processing, Transformers have taken over Computer Vision recently with the advent of Vision Transformers. However, the attention mechanism's quadratic complexity in the number of tokens means that Transformers do not scale well to high-resolution images. XCiT is a new Transformer architecture, containing XCA, a transposed version of attention, reducing the complexity from quadratic to linear, and at least on image data, it appears to perform on par with other models. What does this mean for the field? Is this even a transformer? What really matters in deep learning?\n\nOUTLINE:\n0:00 - Intro & Overview\n3:45 - Self-Attention vs Cross-Covariance Attention (XCA)\n19:55 - Cross-Covariance Image Transformer (XCiT) Architecture\n26:00 - Theoretical & Engineering considerations\n30:40 - Experimental Results\n33:20 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2106.09681\nCode: https://github.com/facebookresearch/xcit\n\nAbstract:\nFollowing their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a \"transposed\" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.\n\nAuthors: Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, Herv\u00e9 Jegou\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "475": "#adversarialexamples #dimpledmanifold #security\n\nAdversarial Examples have long been a fascinating topic for many Machine Learning researchers. How can a tiny perturbation cause the neural network to change its output by so much? While many explanations have been proposed over the years, they all appear to fall short. This paper attempts to comprehensively explain the existence of adversarial examples by proposing a view of the classification landscape, which they call the Dimpled Manifold Model, which says that any classifier will adjust its decision boundary to align with the low-dimensional data manifold, and only slightly bend around the data. This potentially explains many phenomena around adversarial examples. Warning: In this video, I disagree. Remember that I'm not an authority, but simply give my own opinions.\n\nOUTLINE:\n0:00 - Intro & Overview\n7:30 - The old mental image of Adversarial Examples\n11:25 - The new Dimpled Manifold Hypothesis\n22:55 - The Stretchy Feature Model\n29:05 - Why do DNNs create Dimpled Manifolds?\n38:30 - What can be explained with the new model?\n1:00:40 - Experimental evidence for the Dimpled Manifold Model\n1:10:25 - Is Goodfellow's claim debunked?\n1:13:00 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2106.10151\nMy replication code: https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64\nGoodfellow's Talk: https://youtu.be/CIfsB_EYsVI?t=4280\n\nAbstract:\nThe extreme fragility of deep neural networks when presented with tiny perturbations in their inputs was independently discovered by several research groups in 2013, but in spite of enormous effort these adversarial examples remained a baffling phenomenon with no clear explanation. In this paper we introduce a new conceptual framework (which we call the Dimpled Manifold Model) which provides a simple explanation for why adversarial examples exist, why their perturbations have such tiny norms, why these perturbations look like random noise, and why a network which was adversarially trained with incorrectly labeled images can still correctly classify test images. In the last part of the paper we describe the results of numerous experiments which strongly support this new model, and in particular our assertion that adversarial perturbations are roughly perpendicular to the low dimensional manifold which contains all the training examples.\n\nAbstract: Adi Shamir, Odelia Melamed, Oriel BenShmuel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "476": "#apple #icloud #privacy\n\nApple recently announced scanning all images uploaded to iCloud for CSAM (child abuse material), and that this scan would happen locally on users' phones. We take a look at the technical report and explore how the system works in detail, how it is designed to preserve user privacy, and what weak points it still has.\n\nOUTLINE:\n0:00 - Introduction\n3:05 - System Requirements\n9:15 - System Overview\n14:00 - NeuralHash\n20:45 - Private Set Intersection\n31:15 - Threshold Secret Sharing\n35:25 - Synthetic Match Vouchers\n38:20 - Problem 1: Who controls the database?\n42:40 - Problem 2: Adversarial Attacks\n49:40 - Comments & Conclusion\n\nPaper: https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf\nML News Episode about CSAM: https://youtu.be/gFkBqD2hbnU\n\nAbstract:\nCSAM Detection enables Apple to accurately identify and report iCloud users who store known Child Sexual Abuse Material (CSAM) in their iCloud Photos accounts. Apple servers flag accounts exceeding a threshold number of images that match a known database of CSAM image hashes so that Apple can provide relevant information to the National Center for Missing and Exploited Children (NCMEC). This process is secure, and is expressly designed to preserve user privacy.\nCSAM Detection provides these privacy and security assurances:\n\u2022 Apple does not learn anything about images that do not match the known CSAM database.\n\u2022 Apple can\u2019t access metadata or visual derivatives for matched CSAM images until a threshold of matches is exceeded for an iCloud Photos account.\n\u2022 The risk of the system incorrectly flagging an account is extremely low. In addition, Apple manually reviews all reports made to NCMEC to ensure reporting accuracy.\n\u2022 Users can\u2019t access or view the database of known CSAM images.\n\u2022 Users can\u2019t identify which images were flagged as CSAM by the system.\nFor detailed information about the cryptographic protocol and security proofs that the CSAM Detection process uses, see The Apple PSI System.\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "477": "#pondernet #deepmind #machinelearning\n\nHumans don't spend the same amount of mental effort on all problems equally. Instead, we respond quickly to easy tasks, and we take our time to deliberate hard tasks. DeepMind's PonderNet attempts to achieve the same by dynamically deciding how many computation steps to allocate to any single input sample. This is done via a recurrent architecture and a trainable function that computes a halting probability. The resulting model performs well in dynamic computation tasks and is surprisingly robust to different hyperparameter settings.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Problem Statement\n8:00 - Probabilistic formulation of dynamic halting\n14:40 - Training via unrolling\n22:30 - Loss function and regularization of the halting distribution\n27:35 - Experimental Results\n37:10 - Sensitivity to hyperparameter choice\n41:15 - Discussion, Conclusion, Broader Impact\n\nPaper: https://arxiv.org/abs/2107.05407\n\nAbstract:\nIn standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1\n\nAuthors: Andrea Banino, Jan Balaguer, Charles Blundell\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "478": "#attention #transformer #fastformer\n\nTransformers have become the dominant model class in the last few years for large data, but their quadratic complexity in terms of sequence length has plagued them until now. Fastformer claims to be the fastest and most performant linear attention variant, able to consume long contexts at once. This is achieved by a combination of additive attention and elementwise products. While initial results look promising, I have my reservations...\n\nOUTLINE:\n0:00 - Intro & Outline\n2:15 - Fastformer description\n5:20 - Baseline: Classic Attention\n10:00 - Fastformer architecture\n12:50 - Additive Attention\n18:05 - Query-Key element-wise multiplication\n21:35 - Redundant modules in Fastformer\n25:00 - Problems with the architecture\n27:30 - Is this even attention?\n32:20 - Experimental Results\n34:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2108.09084\n\nAbstract:\nTransformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.\n\nAuthors: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "479": "#alibi #transformers #attention\n\nTransformers are essentially set models that need additional inputs to make sense of sequence data. The most widespread additional inputs are position encodings or position embeddings, which add sequence index information in various forms. However, this has put a limit on the resulting model, which cannot run inference on sequences longer than it has been trained on, as it would encounter unfamiliar position encodings. ALiBi solves this by proposing simple linear fixed biases as position information, adding negligible overhead in time and memory, but surprisingly, the resulting model is able to handle inference on sequences many times as long as its training sequences.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - Position Encodings in Transformers\n4:55 - Sinusoidial Position Encodings\n11:50 - ALiBi Position Encodings\n20:50 - How to choose the slope parameter\n23:55 - Experimental Results\n29:10 - Comments & Conclusion\n\nPaper: https://ofir.io/train_short_test_long.pdf\nCode: https://github.com/ofirpress/attention_with_linear_biases\n\nAbstract:\nSince the introduction of the transformer model by Vaswani et al. (2017), a fundamental question remains open: how to achieve extrapolation at inference time to longer sequences than seen during training? We first show that extrapolation can be improved by changing the position representation method, though we find that existing proposals do not allow efficient extrapolation. We introduce a simple and efficient method, Attention with Linear Biases (ALiBi), that allows for extrapolation. ALiBi does not add positional embeddings to the word embeddings; instead, it biases the query-key attention scores with a term that is proportional to their distance. We show that this method allows training a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048, 11% faster and using 11% less memory. ALiBi\u2019s inductive bias towards recency allows it to outperform multiple strong position methods on the WikiText-103 benchmark. Finally, we provide analysis of ALiBi to understand why it leads to better performance.\n\nAuthors: Ofir Press, Noah A. Smith, Mike Lewis\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "480": "#inftyformer #infinityformer #transformer\n\nVanilla Transformers are excellent sequence models, but suffer from very harsch constraints on the length of the sequences they can process. Several attempts have been made to extend the Transformer's sequence length, but few have successfully gone beyond a constant factor improvement. This paper presents a method, based on continuous attention mechanisms, to attend to an unbounded past sequence by representing the past as a continuous signal, rather than a sequence. This enables the Infty-Former to effectively enrich the current context with global information, which increases performance on long-range dependencies in sequence tasks. Further, the paper presents the concept of sticky memories, which highlight past events that are of particular importance and elevates their representation in the long-term memory.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:10 - Sponsor Spot: Weights & Biases\n3:35 - Problem Statement\n8:00 - Continuous Attention Mechanism\n16:25 - Unbounded Memory via concatenation & contraction\n18:05 - Does this make sense?\n20:25 - How the Long-Term Memory is used in an attention layer\n27:40 - Entire Architecture Recap\n29:30 - Sticky Memories by Importance Sampling\n31:25 - Commentary: Pros and cons of using heuristics\n32:30 - Experiments & Results\n\nPaper: https://arxiv.org/abs/2109.00301\n\nSponsor: Weights & Biases\nhttps://wandb.me/start\n\nAbstract:\nTransformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the \u221e-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \u221e-former's attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain \"sticky memories\" while keeping a fixed computation budget. Experiments on a synthetic sorting task demonstrate the ability of the \u221e-former to retain information from long sequences. We also perform experiments on language modeling, by training a model from scratch and by fine-tuning a pre-trained language model, which show benefits of unbounded long-term memories.\n\nAuthors: Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "481": "#tvae #topographic #equivariant\n\nVariational Autoencoders model the latent space as a set of independent Gaussian random variables, which the decoder maps to a data distribution. However, this independence is not always desired, for example when dealing with video sequences, we know that successive frames are heavily correlated. Thus, any latent space dealing with such data should reflect this in its structure. Topographic VAEs are a framework for defining correlation structures among the latent variables and induce equivariance within the resulting model. This paper shows how such correlation structures can be built by correctly arranging higher-level variables, which are themselves independent Gaussians.\n\nOUTLINE:\n0:00 - Intro\n1:40 - Architecture Overview\n6:30 - Comparison to regular VAEs\n8:35 - Generative Mechanism Formulation\n11:45 - Non-Gaussian Latent Space\n17:30 - Topographic Product of Student-t\n21:15 - Introducing Temporal Coherence\n24:50 - Topographic VAE\n27:50 - Experimental Results\n31:15 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2109.01394\nCode: https://github.com/akandykeller/topographicvae\n\nAbstract:\nIn this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. \"capsules\") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.\n\nAuthors: T. Anderson Keller, Max Welling\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "482": "#gpt-3 #truth #conspiracy\n\nA new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people's criticisms of large language models, but is it really the correct conclusion?\n\nOUTLINE:\n0:00 - Intro\n0:30 - Twitter Paper Announcement\n4:10 - Large Language Models are to blame!\n5:50 - How was the dataset constructed?\n9:25 - The questions are adversarial\n12:30 - Are you surprised?!\n\nPaper: https://arxiv.org/abs/2109.07958\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "483": "#neurips #peerreview #nips\n\nThe peer-review system at Machine Learning conferences has come under much criticism over the last years. One major driver was the infamous 2014 NeurIPS experiment, where a subset of papers were given to two different sets of reviewers. This experiment showed that only about half of all accepted papers were consistently accepted by both committees and demonstrated significant influence of subjectivity. This paper revisits the data from the 2014 experiment and traces the fate of accepted and rejected papers during the 7 years since, and analyzes how well reviewers can assess future impact, among other things.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:20 - Recap: The 2014 NeurIPS Experiment\n5:40 - How much of reviewing is subjective?\n11:00 - Validation via simulation\n15:45 - Can reviewers predict future impact?\n23:10 - Discussion & Comments\n\nPaper: https://arxiv.org/abs/2109.09774\nCode: https://github.com/lawrennd/neurips2014/\n\nAbstract:\nIn this paper we revisit the 2014 NeurIPS experiment that examined inconsistency in conference peer review. We determine that 50% of the variation in reviewer quality scores was subjective in origin. Further, with seven years passing since the experiment we find that for accepted papers, there is no correlation between quality scores and impact of the paper as measured as a function of citation count. We trace the fate of rejected papers, recovering where these papers were eventually published. For these papers we find a correlation between quality scores and impact. We conclude that the reviewing process for the 2014 conference was good for identifying poor papers, but poor for identifying good papers. We give some suggestions for improving the reviewing process but also warn against removing the subjective element. Finally, we suggest that the real conclusion of the experiment is that the community should place less onus on the notion of top-tier conference publications when assessing the quality of individual researchers. For NeurIPS 2021, the PCs are repeating the experiment, as well as conducting new ones.\n\nAuthors: Corinna Cortes, Neil D. Lawrence\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "484": "#deeplearning #co2 #cost\n\nDeep Learning has achieved impressive results in the last years, not least due to the massive increases in computational power and data that has gone into these models. Scaling up currently promises to be a reliable way to create more performant systems, but how far can we go? This article explores the limits of exponential scaling in AI, and what people are doing to get around this problem\n\nOUTLINE:\n0:00 - Intro & Overview\n1:00 - Deep Learning at its limits\n3:10 - The cost of overparameterization\n5:40 - Extrapolating power usage and CO2 emissions\n10:45 - We cannot just continue scaling up\n13:25 - Current solution attempts\n15:25 - Aside: ImageNet V2\n17:50 - Are symbolic methods the way out?\n\nPaper: https://spectrum.ieee.org/deep-learning-computational-cost\n\nImage by Ralf Vetterle from Pixabay: https://pixabay.com/images/id-1752876/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "485": "#grokking #openai #deeplearning\n\nGrokking is a phenomenon when a neural network suddenly learns a pattern in the dataset and jumps from random chance generalization to perfect generalization very suddenly. This paper demonstrates grokking on small algorithmic datasets where a network has to fill in binary tables. Interestingly, the learned latent spaces show an emergence of the underlying binary operations that the data were created with.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:40 - The Grokking Phenomenon\n3:50 - Related: Double Descent\n7:50 - Binary Operations Datasets\n11:45 - What quantities influence grokking?\n15:40 - Learned Emerging Structure\n17:35 - The role of smoothness\n21:30 - Simple explanations win\n24:30 - Why does weight decay encourage simplicity?\n26:40 - Appendix\n28:55 - Conclusion & Comments\n\nPaper: https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf\n\nAbstract:\nIn this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \u201cgrokking\u201d a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.\n\nAuthors: Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin & Vedant Misra\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "486": "#gpt3 #knowledge #symbolic\n\nSymbolic knowledge models are usually trained on human-generated corpora that are cumbersome and expensive to create. Such corpora consist of structured triples of symbolic knowledge. This paper takes a different approach and attempts to generate such a corpus by prompting GPT-3. Results show that clever prompting, combined with targeted small critic models trained on human ratings can outperform both human-generated data, as well as the teacher model (GPT-3) itself. The results of this paper give a general recipe for automatically building corpora for various NLP tasks by extracting samples from large language models.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:30 - Sponsor: Weights & Biases\n4:15 - Commonsense Knowledge Graphs\n7:50 - ATOMIC dataset\n10:00 - Generating the corpus from a model\n13:00 - Prompting GPT-3\n15:30 - Generating Events\n18:40 - Generating Inferences\n23:00 - Evaluating the created dataset\n26:45 - Introducing the critic\n31:25 - Using the critic to filter the data\n36:30 - Training a student on the generated data\n41:00 - Key Findings\n44:45 - Comments & Conclusion\n\nPaper: https://arxiv.org/abs/2110.07178\nCode & Corpus: https://github.com/peterwestai2/symbolic-knowledge-distillation\n\nSponsor: Weights & Biases\nhttps://wandb.com\nhttps://community.wandb.ai/\n\nAbstract:\nThe common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.\n\nAuthors: Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "487": "#efficientzero #muzero #atari\n\nReinforcement Learning methods are notoriously data-hungry. Notably, MuZero learns a latent world model just from scalar feedback of reward- and policy-predictions, and therefore relies on scale to perform well. However, most RL algorithms fail when presented with very little data. EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low-sample setting. This could be a staple algorithm for future RL research.\n\nOUTLINE:\n0:00 - Intro & Outline\n2:30 - MuZero Recap\n10:50 - EfficientZero improvements\n14:15 - Self-Supervised consistency loss\n17:50 - End-to-end prediction of the value prefix\n20:40 - Model-based off-policy correction\n25:45 - Experimental Results & Conclusion\n\nPaper: https://arxiv.org/abs/2111.00210\nCode: https://github.com/YeWR/EfficientZero\nNote: code not there yet as of release of this video\n\nAbstract:\nReinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 190.4% mean human performance and 116.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.\n\nAuthors: Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "488": "#machinelearning #ardm #generativemodels\n\nDiffusion models have made large advances in recent months as a new type of generative models. This paper introduces Autoregressive Diffusion Models (ARDMs), which are a mix between autoregressive generative models and diffusion models. ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time. This paper applies ARDMs to both text and image data, and as an extension, the models can also be used to perform lossless compression.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:15 - Decoding Order in Autoregressive Models\n6:15 - Autoregressive Diffusion Models\n8:35 - Dependent and Independent Sampling\n14:25 - Application to Character-Level Language Models\n18:15 - How Sampling & Training Works\n26:05 - Extension 1: Parallel Sampling\n29:20 - Extension 2: Depth Upscaling\n33:10 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2110.02037\n\nAbstract:\nWe introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.\n\nAuthors: Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "489": "#deeplearning #backpropagation #simulation\n\nMore and more systems are made differentiable, which means that accurate gradients of these systems' dynamics can be computed exactly. While this development has led to a lot of advances, there are also distinct situations where backpropagation can be a very bad idea. This paper characterizes a few such systems in the domain of iterated dynamical systems, often including some source of stochasticity, resulting in chaotic behavior. In these systems, it is often better to use black-box estimators for gradients than computing them exactly.\n\nOUTLINE:\n0:00 - Foreword\n1:15 - Intro & Overview\n3:40 - Backpropagation through iterated systems\n12:10 - Connection to the spectrum of the Jacobian\n15:35 - The Reparameterization Trick\n21:30 - Problems of reparameterization\n26:35 - Example 1: Policy Learning in Simulation\n33:05 - Example 2: Meta-Learning Optimizers\n36:15 - Example 3: Disk packing\n37:45 - Analysis of Jacobians\n40:20 - What can be done?\n45:40 - Just use Black-Box methods\n\nPaper: https://arxiv.org/abs/2111.05803\n\nAbstract:\nDifferentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.\n\nAuthors: Luke Metz, C. Daniel Freeman, Samuel S. Schoenholz, Tal Kachman\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "490": "#grafting #adam #sgd\n\nThe last years in deep learning research have given rise to a plethora of different optimization algorithms, such as SGD, AdaGrad, Adam, LARS, LAMB, etc. which all claim to have their special peculiarities and advantages. In general, all algorithms modify two major things: The (implicit) learning rate schedule, and a correction to the gradient direction. This paper introduces grafting, which allows to transfer the induced learning rate schedule of one optimizer to another one. In that, the paper shows that much of the benefits of adaptive methods (e.g. Adam) are actually due to this schedule, and not necessarily to the gradient direction correction. Grafting allows for more fundamental research into differences and commonalities between optimizers, and a derived version of it makes it possible to computes static learning rate corrections for SGD, which potentially allows for large savings of GPU memory.\n\nOUTLINE\n0:00 - Rant about Reviewer #2\n6:25 - Intro & Overview\n12:25 - Adaptive Optimization Methods\n20:15 - Grafting Algorithm\n26:45 - Experimental Results\n31:35 - Static Transfer of Learning Rate Ratios\n35:25 - Conclusion & Discussion\n\nPaper (OpenReview): https://openreview.net/forum?id=FpKgG31Z_i9\nOld Paper (Arxiv): https://arxiv.org/abs/2002.11803\n\nOur Discord: https://discord.gg/4H8xxDF\n\nAbstract:\nIn the empirical science of training large neural networks, the learning rate schedule is a notoriously challenging-to-tune hyperparameter, which can depend on all other properties (architecture, optimizer, batch size, dataset, regularization, ...) of the problem. In this work, we probe the entanglements between the optimizer and the learning rate schedule. We propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Using grafting, we discover a non-adaptive learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance. Besides providing a resource-saving tool for practitioners, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning.\n\nAuthors: Anonymous (Under Review)\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "491": "#imle #backpropagation #discrete\n\nBackpropagation is the workhorse of deep learning, but unfortunately, it only works for continuous functions that are amenable to the chain rule of differentiation. Since discrete algorithms have no continuous derivative, deep networks with such algorithms as part of them cannot be effectively trained using backpropagation. This paper presents a method to incorporate a large class of algorithms, formulated as discrete exponential family distributions, into deep networks and derives gradient estimates that can easily be used in end-to-end backpropagation. This enables things like combinatorial optimizers to be part of a network's forward propagation natively.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:25 - Sponsor: Weights & Biases\n6:15 - Problem Setup & Contributions\n8:50 - Recap: Straight-Through Estimator\n13:25 - Encoding the discrete problem as an inner product\n19:45 - From algorithm to distribution\n23:15 - Substituting the gradient\n26:50 - Defining a target distribution\n38:30 - Approximating marginals via perturb-and-MAP\n45:10 - Entire algorithm recap\n56:45 - Github Page & Example\n\nPaper: https://arxiv.org/abs/2106.01798\nCode (TF): https://github.com/nec-research/tf-imle\nCode (Torch): https://github.com/uclnlp/torch-imle\n\nOur Discord: https://discord.gg/4H8xxDF\n\nSponsor: Weights & Biases\nhttps://wandb.com\n\nAbstract:\nCombining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.\n\nAuthors: Mathias Niepert, Pasquale Minervini, Luca Franceschi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "492": "#scalingtransformers #terraformer #sparsity\n\nTransformers keep pushing the state of the art in language and other domains, mainly due to their ability to scale to ever more parameters. However, this scaling has made it prohibitively expensive to run a lot of inference requests against a Transformer, both in terms of compute and memory requirements. Scaling Transformers are a new kind of architecture that leverage sparsity in the Transformer blocks to massively speed up inference, and by including additional ideas from other architectures, they create the Terraformer, which is both fast, accurate, and consumes very little memory.\n\nOUTLINE:\n0:00 - Intro & Overview\n4:10 - Recap: Transformer stack\n6:55 - Sparse Feedforward layer\n19:20 - Sparse QKV Layer\n43:55 - Terraformer architecture\n55:05 - Experimental Results & Conclusion\n\nPaper: https://arxiv.org/abs/2111.12763\nCode: https://github.com/google/trax/blob/master/trax/examples/Terraformer_from_scratch.ipynb\n\nAbstract:\nLarge Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.\n\nAuthors: Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, \u0141ukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, Jonni Kanerva\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "493": "#nuwa #microsoft #generative\n\nN\u00dcWA is a unifying architecture that can ingest text, images, and videos and brings all of them into a quantized latent representation to support a multitude of visual generation tasks, such as text-to-image, text-guided video manipulation, or sketch-to-video. This paper details how the encoders for the different modalities are constructed, and how the latent representation is transformed using their novel 3D nearby self-attention layers. Experiments are shown on 8 different visual generation tasks that the model supports.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:20 - Sponsor: ClearML\n3:35 - Tasks & Naming\n5:10 - The problem with recurrent image generation\n7:35 - Creating a shared latent space w/ Vector Quantization\n23:20 - Transforming the latent representation\n26:25 - Recap: Self- and Cross-Attention\n28:50 - 3D Nearby Self-Attention\n41:20 - Pre-Training Objective\n46:05 - Experimental Results\n50:40 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2111.12417\nGithub: https://github.com/microsoft/NUWA\n\nSponsor: ClearML\nhttps://clear.ml\n\nAbstract:\nThis paper presents a unified multimodal pre-trained model called N\u00dcWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate N\u00dcWA on 8 downstream tasks. Compared to several strong baselines, N\u00dcWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.\n\nAuthors: Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "494": "#lama #inpainting #deeplearning\n\nAt the end of the video is an interview with the paper authors!\nLaMa is a system that is amazing at removing foreground objects from images, especially when those objects cover a large part of the image itself. LaMa is specifically trained to reconstruct large masked areas and includes global information throughout its forward propagation by using Fourier Convolutions in its layers. This makes it incredibly effective at reconstructing periodic structures with long-range consistency, compared to regular convolutions.\n\nOUTLINE:\n0:00 - Intro\n0:45 - Sponsor: ClearML\n3:30 - Inpainting Examples\n5:05 - Live Demo\n6:40 - Locality as a weakness of convolutions\n10:30 - Using Fourier Transforms for global information\n12:55 - Model architecture overview\n14:35 - Fourier convolution layer\n21:15 - Loss function\n24:25 - Mask generation algorithm\n25:40 - Experimental results\n28:25 - Interview with the authors\n\nPaper: https://arxiv.org/abs/2109.07161\nCode: https://github.com/saic-mdal/lama\nOnline Demo: https://cleanup.pictures/\n\nSponsor: ClearML\nhttps://clear.ml\n\nAbstract:\nModern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter&time costs than the competitive baselines. The code is available at \\url{this https URL}.\n\nAuthors: Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "495": "#glide #openai #diffusion\n\nDiffusion models learn to iteratively reverse a noising process that is applied repeatedly during training. The result can be used for conditional generation as well as various other tasks such as inpainting. OpenAI's GLIDE builds on recent advances in diffusion models and combines text-conditional diffusion with classifier-free guidance and upsampling to achieve unprecedented quality in text-to-image samples.\n\nTry it yourself: https://huggingface.co/spaces/valhalla/glide-text2im\n\nOUTLINE:\n0:00 - Intro & Overview\n6:10 - What is a Diffusion Model?\n18:20 - Conditional Generation and Guided Diffusion\n31:30 - Architecture Recap\n34:05 - Training & Result metrics\n36:55 - Failure cases & my own results\n39:45 - Safety considerations\n\nPaper: https://arxiv.org/abs/2112.10741\nCode & Model: https://github.com/openai/glide-text2im\n\nMore diffusion papers:\nhttps://arxiv.org/pdf/2006.11239.pdf\nhttps://arxiv.org/pdf/2102.09672.pdf\n\nAbstract:\nDiffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at this https URL.\n\nAuthors: Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "496": "#playerofgames #deepmind #alphazero\n\nSpecial Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\nGames have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\n\nOUTLINE:\n0:00 - Introduction\n2:50 - What games can Player of Games be trained on?\n4:00 - Tree search algorithms (AlphaZero)\n8:00 - What is different in imperfect information games?\n15:40 - Counterfactual Value- and Policy-Networks\n18:50 - The Player of Games search procedure\n28:30 - How to train the network?\n34:40 - Experimental Results\n47:20 - Discussion & Outlook\n\nPaper: https://arxiv.org/abs/2112.03178\n\nAbstract:\nGames have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.\n\nAuthors: Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "497": "#minerl #minecraft #deeplearning\n\nThe MineRL BASALT challenge has no reward functions or technical descriptions of what's to be achieved. Instead, the goal of each task is given as a short natural language string, and the agent is evaluated by a team of human judges who rate both how well the goal has been fulfilled, as well as how human-like the agent behaved. In this video, I interview KAIROS, the winning team of the 2021 challenge, and discuss how they used a combination of machine learning, efficient data collection, hand engineering, and a bit of knowledge about Minecraft to beat all other teams.\n\nOUTLINE:\n0:00 - Introduction\n4:10 - Paper Overview\n11:15 - Start of Interview\n17:05 - First Approach\n20:30 - State Machine\n26:45 - Efficient Label Collection\n30:00 - Navigation Policy\n38:15 - Odometry Estimation\n46:00 - Pain Points & Learnings\n50:40 - Live Run Commentary\n58:50 - What other tasks can be solved?\n1:01:55 - What made the difference?\n1:07:30 - Recommendations & Conclusion\n1:11:10 - Full Runs: Waterfall\n1:12:40 - Full Runs: Build House\n1:17:45 - Full Runs: Animal Pen\n1:20:50 - Full Runs: Find Cave\n\nPaper: https://arxiv.org/abs/2112.03482\nCode: https://github.com/viniciusguigo/kairos_minerl_basalt\nChallenge Website: https://minerl.io/basalt/\n\nPaper Title: Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft\n\nAbstract:\nReal-world tasks of interest are generally poorly defined by human-readable descriptions and have no pre-defined reward signals unless it is defined by a human designer. Conversely, data-driven algorithms are often designed to solve a specific, narrowly defined, task with performance metrics that drives the agent's learning. In this work, we present the solution that won first place and was awarded the most human-like agent in the 2021 NeurIPS Competition MineRL BASALT Challenge: Learning from Human Feedback in Minecraft, which challenged participants to use human data to solve four tasks defined only by a natural language description and no reward function. Our approach uses the available human demonstration data to train an imitation learning policy for navigation and additional human feedback to train an image classifier. These modules, together with an estimated odometry map, are then combined into a state-machine designed based on human knowledge of the tasks that breaks them down in a natural hierarchy and controls which macro behavior the learning agent should follow at any instant. We compare this hybrid intelligence approach to both end-to-end machine learning and pure engineered solutions, which are then judged by human evaluators. Codebase is available at this https URL.\n\nAuthors: Vinicius G. Goecks, Nicholas Waytowich, David Watkins, Bharat Prakash\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "498": "#deeplearning #noether #symmetries\n\nThis video includes an interview with first author Ferran Alet!\nEncoding inductive biases has been a long established methods to provide deep networks with the ability to learn from less data. Especially useful are encodings of symmetry properties of the data, such as the convolution's translation invariance. But such symmetries are often hard to program explicitly, and can only be encoded exactly when done in a direct fashion. Noether Networks use Noether's theorem connecting symmetries to conserved quantities and are able to dynamically and approximately enforce symmetry properties upon deep neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n18:10 - Interview Start\n21:20 - Symmetry priors vs conserved quantities\n23:25 - Example: Pendulum\n27:45 - Noether Network Model Overview\n35:35 - Optimizing the Noether Loss\n41:00 - Is the computation graph stable?\n46:30 - Increasing the inference time computation\n48:45 - Why dynamically modify the model?\n55:30 - Experimental Results & Discussion\n\nPaper: https://arxiv.org/abs/2112.03321\nWebsite: https://dylandoblar.github.io/noether-networks/\nCode: https://github.com/dylandoblar/noether-networks\n\nAbstract:\nProgress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.\n\nAuthors: Ferran Alet, Dylan Doblar, Allan Zhou, Joshua Tenenbaum, Kenji Kawaguchi, Chelsea Finn\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "499": "#deeplearning #neuralinterpreter #ai\n\nThis video includes an interview with the paper's authors!\nWhat if we treated deep networks like modular programs? Neural Interpreters divide computation into small modules and route data to them via a dynamic type inference system. The resulting model combines recurrent elements, weight sharing, attention, and more to tackle both abstract reasoning, as well as computer vision tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Model Overview\n7:00 - Interpreter weights and function code\n9:40 - Routing data to functions via neural type inference\n14:55 - ModLin layers\n18:25 - Experiments\n21:35 - Interview Start\n24:50 - General Model Structure\n30:10 - Function code and signature\n40:30 - Explaining Modulated Layers\n49:50 - A closer look at weight sharing\n58:30 - Experimental Results\n\nPaper: https://arxiv.org/abs/2110.06399\n\nGuests:\nNasim Rahaman: https://twitter.com/nasim_rahaman\nFrancesco Locatello: https://twitter.com/FrancescoLocat8\nWaleed Gondal: https://twitter.com/Wallii_gondal\n\nAbstract:\nModern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \\emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization\n\nAuthors: Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, Bernhard Sch\u00f6lkopf\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "500": "#deeplearning #symbolic #research\n\nThis video includes an interview with first author St\u00e9phane d'Ascoli (https://sdascoli.github.io/).\nDeep neural networks are typically excellent at numeric regression, but using them for symbolic computation has largely been ignored so far. This paper uses transformers to do symbolic regression on integer and floating point number sequences, which means that given the start of a sequence of numbers, the model has to not only predict the correct continuation, but also predict the data generating formula behind the sequence. Through clever encoding of the input space and a well constructed training data generation process, this paper's model can learn and represent many of the sequences in the OEIS, the online encyclopedia of integer sequences and it also features an interactive demo if you want to try it by yourself. \n\nOUTLINE:\n0:00 - Introduction\n2:20 - Summary of the Paper\n16:10 - Start of Interview\n17:15 - Why this research direction?\n20:45 - Overview of the method\n30:10 - Embedding space of input tokens\n33:00 - Data generation process\n42:40 - Why are transformers useful here?\n46:40 - Beyond number sequences, where is this useful?\n48:45 - Success cases and failure cases\n58:10 - Experimental Results\n1:06:30 - How did you overcome difficulties?\n1:09:25 - Interactive demo\n\nPaper: https://arxiv.org/abs/2201.04600\nInteractive demo: https://symbolicregression.metademolab.com/\n\nAbstract:\nSymbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. bessel0(x)\u2248sin(x)+cos(x)\u03c0x\u221a and 1.644934\u2248\u03c02/6. An interactive demonstration of our models is provided at this https URL.\n\nAuthors: St\u00e9phane d'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, Fran\u00e7ois Charton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "501": "#deeplearning #brain #neuroscience\n\nOriginally, Deep Learning sprang into existence inspired by how the brain processes information, but the two fields have diverged ever since. However, given that deep models can solve many perception tasks with remarkable accuracy, is it possible that we might be able to learn something about how the brain works by inspecting our models? I speak to Patrick Mineault about his blog post \"2021 in review: unsupervised brain models\" and we explore why neuroscientists are taking interest in unsupervised and self-supervised deep neural networks in order to explain how the brain works. We discuss a series of influential papers that have appeared last year, and we go into the more general questions of connecting neuroscience and machine learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:35 - Start of Interview\n10:30 - Visual processing in the brain\n12:50 - How does deep learning inform neuroscience?\n21:15 - Unsupervised training explains the ventral stream\n30:50 - Predicting own motion parameters explains the dorsal stream\n42:20 - Why are there two different visual streams?\n49:45 - Concept cells and representation learning\n56:20 - Challenging the manifold theory\n1:08:30 - What are current questions in the field?\n1:13:40 - Should the brain inform deep learning?\n1:18:50 - Neuromatch Academy and other endeavours\n\nBlog Post: https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/\nPatrick's Blog: https://xcorr.net/\nTwitter: https://twitter.com/patrickmineault\nNeuromatch Academy: https://academy.neuromatch.io/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "502": "#gpt3 #embodied #planning\n\nIn this video: Paper explanation, followed by first author interview with Wenlong Huang.\nLarge language models contain extraordinary amounts of world knowledge that can be queried in various ways. But their output format is largely uncontrollable. This paper investigates the VirtualHome environment, which expects a particular set of actions, objects, and verbs to be used. Turns out, with proper techniques and only using pre-trained models (no fine-tuning), one can translate unstructured language model outputs into the structured grammar of the environment. This is potentially very useful anywhere where the models' world knowledge needs to be provided in a particular structured format.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:45 - The VirtualHome environment\n6:25 - The problem of plan evaluation\n8:40 - Contributions of this paper\n16:40 - Start of interview\n24:00 - How to use language models with environments?\n34:00 - What does model size matter?\n40:00 - How to fix the large models' outputs?\n55:00 - Possible improvements to the translation procedure\n59:00 - Why does Codex perform so well?\n1:02:15 - Diving into experimental results\n1:14:15 - Future outlook\n\nPaper: https://arxiv.org/abs/2201.07207\nWebsite: https://wenlong.page/language-planner/\nCode: https://github.com/huangwl18/language-planner\nWenlong's Twitter: https://twitter.com/wenlong_huang\n\nAbstract:\nCan world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \"make breakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into low-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at this https URL\n\nAuthors: Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "503": "#hypertransformer #metalearning #deeplearning\n\nThis video contains a paper explanation and an interview with author Andrey Zhmoginov!\nFew-shot learning is an interesting sub-field in meta-learning, with wide applications, such as creating personalized models based on just a handful of data points. Traditionally, approaches have followed the BERT approach where a large model is pre-trained and then fine-tuned. However, this couples the size of the final model to the size of the model that has been pre-trained. Similar problems exist with \"true\" meta-learners, such as MaML. HyperTransformer fundamentally decouples the meta-learner from the size of the final model by directly predicting the weights of the final model. The HyperTransformer takes the few-shot dataset as a whole into its context and predicts either one or multiple layers of a (small) ConvNet, meaning its output are the weights of the convolution filters. Interestingly, and with the correct engineering care, this actually appears to deliver promising results and can be extended in many ways.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:05 - Weight-generation vs Fine-tuning for few-shot learning\n10:10 - HyperTransformer model architecture overview\n22:30 - Why the self-attention mechanism is useful here\n34:45 - Start of Interview\n39:45 - Can neural networks even produce weights of other networks?\n47:00 - How complex does the computational graph get?\n49:45 - Why are transformers particularly good here?\n58:30 - What can the attention maps tell us about the algorithm?\n1:07:00 - How could we produce larger weights?\n1:09:30 - Diving into experimental results\n1:14:30 - What questions remain open?\n\nPaper: https://arxiv.org/abs/2201.04182\n\nERRATA: I introduce Max Vladymyrov as Mark Vladymyrov\n\nAbstract:\nIn this work we propose a HyperTransformer, a transformer-based model for few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable. Finally, we extend our approach to a semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance.\n\nAuthors: Andrey Zhmoginov, Mark Sandler, Max Vladymyrov\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "504": "#security #censorship #ai\n\nMost of us conceive the internet as a free and open space where we are able to send traffic between any two nodes, but for large parts of the world this is not the case. Entire nations have large machinery in place to survey all internet traffic and automated procedures to block any undesirable connections. Evading such censorship has been largely a cat-and-mouse game between security researchers and government actors. A new system, called Geneva, uses a Genetic Algorithm in combination with Evolutionary Search in order to dynamically evade such censorship and adjust itself in real-time to any potential response by its adversaries. In this video, I talk to Security researcher Kevin Bock, who is one of Geneva's main contributors and member of the Breakerspace project. We talk about the evolution of internet censorship, how to evade it, how to mess with the censors' infrastructure, as well as the broader emerging connections between AI and Security.\n\nOUTLINE:\n0:00 - Intro\n3:30 - What is automated censorship in networks?\n7:20 - The evolution of censorship vs evasion\n12:40 - Why do we need a dynamic, evolving system?\n16:30 - The building blocks of Geneva\n23:15 - Introducing evolution\n28:30 - What's the censors' response?\n31:45 - How was Geneva's media reception?\n33:15 - Where do we go from here?\n37:30 - Can we deliberately attack the censors?\n47:00 - On responsible disclosure\n49:40 - Breakerspace: Security research for undergrads\n50:40 - How often do you get into trouble?\n52:10 - How can I get started in security?\n\nLearn more at:\n- Geneva (& more) project page: https://censorship.ai\n- Open Observatory of Network Interference: https://ooni.org\n- Censored Planet: https://censoredplanet.org\n- Breakerspace: https://breakerspace.cs.umd.edu\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "505": "#cm3 #languagemodel #transformer\n\nThis video contains a paper explanation and an incredibly informative interview with first author Armen Aghajanyan.\nAutoregressive Transformers have come to dominate many fields in Machine Learning, from text generation to image creation and many more. However, there are two problems. First, the collected data is usually scraped from the web and uni- or bi-modal and throws away a lot of structure of the original websites, and second, language modelling losses are uni-directional. CM3 addresses both problems: It directly operates on HTML and includes text, hyperlinks, and even images (via VQGAN tokenization) and can therefore be used in plenty of ways: Text generation, captioning, image creation, entity linking, and much more. It also introduces a new training strategy called Causally Masked Language Modelling, which brings a level of bi-directionality into autoregressive language modelling. In the interview after the paper explanation, Armen and I go deep into the how and why of these giant models, we go over the stunning results and we make sense of what they mean for the future of universal models.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:30 - Directly learning the structure of HTML\n12:30 - Causally Masked Language Modelling\n18:50 - A short look at how to use this model\n23:20 - Start of interview\n25:30 - Feeding language models with HTML\n29:45 - How to get bi-directionality into decoder-only Transformers?\n37:00 - Images are just tokens\n41:15 - How does one train such giant models?\n45:40 - CM3 results are amazing\n58:20 - Large-scale dataset collection and content filtering\n1:04:40 - More experimental results\n1:12:15 - Why don't we use raw HTML?\n1:18:20 - Does this paper contain too many things?\n\nPaper: https://arxiv.org/abs/2201.07520\n\nAbstract:\nWe introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.\n\nAuthors: Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "506": "#wikipedia #reinforcementlearning #languagemodels\n\nTransformers have come to overtake many domain-targeted custom models in a wide variety of fields, such as Natural Language Processing, Computer Vision, Generative Modelling, and recently also Reinforcement Learning. This paper looks at the Decision Transformer and shows that, surprisingly, pre-training the model on a language-modelling task significantly boosts its performance on Offline Reinforcement Learning. The resulting model achieves higher scores, can get away with less parameters, and exhibits superior scaling properties. This raises many questions about the fundamental connection between the domains of language and RL.\n\nOUTLINE:\n0:00 - Intro\n1:35 - Paper Overview\n7:35 - Offline Reinforcement Learning as Sequence Modelling\n12:00 - Input Embedding Alignment & other additions\n16:50 - Main experimental results\n20:45 - Analysis of the attention patterns across models\n32:25 - More experimental results (scaling properties, ablations, etc.)\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "507": "#ai #alphacode #deepmind\n\nAlphaCode is an automated system that can solve competitive programing exercises. The authors found an interesting combination of language models, large-scale sampling, and clever techniques to filter and subsequently cluster the resulting programs, which lets the system perform on the level of an average competitor in real competitions. In this video, we take a deep dive into AlphaCode's design, architecture, and experimental evaluation. The paper is very well structured and the empirical results are super interesting!\n\nOUTLINE:\n0:00 - Intro\n2:10 - Paper Overview\n3:30 - An example problem from competitive programming\n8:00 - AlphaCode system overview\n14:00 - Filtering out wrong solutions\n17:15 - Clustering equivalent generated programs\n21:50 - Model configurations & engineering choices\n24:30 - Adding privileged information to the input & more tricks\n28:15 - Experimental Results (very interesting!)\n\nPaper: https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\nCode: https://github.com/deepmind/code_contests\n\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. Evaluated on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in programming competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\n\nAuthors: Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu and Oriol Vinyals\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "508": "#openai #math #imo\n\nFormal mathematics is a challenging area for both humans and machines. For humans, formal proofs require very tedious and meticulous specifications of every last detail and results in very long, overly cumbersome and verbose outputs. For machines, the discreteness and sparse reward nature of the problem presents a significant problem, which is classically tackled by brute force search, guided by a couple of heuristics. Previously, language models have been employed to better guide these proof searches and delivered significant improvements, but automated systems are still far from usable. This paper introduces another concept: An expert iteration procedure is employed to iteratively produce more and more challenging, but solvable problems for the machine to train on, which results in an automated curriculum, and a final algorithm that performs well above the previous models. OpenAI used this method to even solve two problems of the international math olympiad, which was previously infeasible for AI systems.\n\nOUTLINE:\n0:00 - Intro\n2:35 - Paper Overview\n5:50 - How do formal proofs work?\n9:35 - How expert iteration creates a curriculum\n16:50 - Model, data, and training procedure\n25:30 - Predicting proof lengths for guiding search\n29:10 - Bootstrapping expert iteration\n34:10 - Experimental evaluation & scaling properties\n40:10 - Results on synthetic data\n44:15 - Solving real math problems\n47:15 - Discussion & comments\n\nPaper: https://arxiv.org/abs/2202.01344\nminiF2F benchmark: https://github.com/openai/miniF2F\n\nAbstract:\nWe explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.\n\nAuthors: Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "509": "#deepmind #rl #society\n\nThis is an in-depth paper review, followed by an interview with the papers' authors!\nSociety is ruled by norms, and most of these norms are very useful, such as washing your hands before cooking. However, there also exist plenty of social norms which are essentially arbitrary, such as what hairstyles are acceptable, or what words are rude. These are called \"silly rules\". This paper uses multi-agent reinforcement learning to investigate why such silly rules exist. Their results indicate a plausible mechanism, by which the existence of silly rules drastically speeds up the agents' acquisition of the skill of enforcing rules, which generalizes well, and therefore a society that has silly rules will be better at enforcing rules in general, leading to faster adaptation in the face of genuinely useful norms.\n\nOUTLINE:\n0:00 - Intro\n3:00 - Paper Overview\n5:20 - Why are some social norms arbitrary?\n11:50 - Reinforcement learning environment setup\n20:00 - What happens if we introduce a \"silly\" rule?\n25:00 - Experimental Results: how silly rules help society\n30:10 - Isolated probing experiments\n34:30 - Discussion of the results\n37:30 - Start of Interview\n39:30 - Where does the research idea come from?\n44:00 - What is the purpose behind this research?\n49:20 - Short recap of the mechanics of the environment\n53:00 - How much does such a closed system tell us about the real world?\n56:00 - What do the results tell us about silly rules?\n1:01:00 - What are these agents really learning?\n1:08:00 - How many silly rules are optimal?\n1:11:30 - Why do you have separate weights for each agent?\n1:13:45 - What features could be added next?\n1:16:00 - How sensitive is the system to hyperparameters?\n1:17:20 - How to avoid confirmation bias?\n1:23:15 - How does this play into progress towards AGI?\n1:29:30 - Can we make real-world recommendations based on this?\n1:32:50 - Where do we go from here?\n\nPaper: https://www.pnas.org/doi/10.1073/pnas.2106028118\nBlog: https://deepmind.com/research/publications/2021/Spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents\n\nAbstract:\nThe fact that humans enforce and comply with norms is an important reason why humans enjoy higher levels of cooperation and welfare than other animals. Some norms are relatively easy to explain; they may prohibit obviously harmful or uncooperative actions. But many norms are not easy to explain. For example, most cultures prohibit eating certain kinds of foods and almost all societies have rules about what constitutes appropriate clothing, language, and gestures. Using a computational model focused on learning shows that apparently pointless rules can have an indirect effect on welfare. They can help agents learn how to enforce and comply with norms in general, improving the group\u2019s ability to enforce norms that have a direct effect on welfare.\n\nAuthors: Raphael K\u00f6ster, Dylan Hadfield-Menell, Richard Everett, Laura Weidinger, Gillian K. Hadfield, Joel Z. Leibo\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "510": "#vos #outliers #deeplearning\nSponsor: Assembly AI\nCheck them out here: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic1\n\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\n\nOUTLINE:\n0:00 - Intro\n2:00 - Sponsor: Assembly AI (Link below)\n4:05 - Paper Overview\n6:45 - Where do traditional classifiers fail?\n11:00 - How object detectors work\n17:00 - What are virtual outliers and how are they created?\n24:00 - Is this really an appropriate model for outliers?\n26:30 - How virtual outliers are used during training\n34:00 - Plugging it all together to detect outliers\n\nPaper: https://arxiv.org/abs/2202.01197\nCode: https://github.com/deeplearning-wisc/vos\n\nAbstract:\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\n\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "511": "#deeplearning #objectdetection #outliers\n\nAn interview with the authors of \"Virtual Outlier Synthesis\".\nWatch the paper review video here: https://youtu.be/i-J4T3uLC9M\n\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\n\nOUTLINE:\n0:00 - Intro\n2:20 - What was the motivation behind this paper?\n5:30 - Why object detection?\n11:05 - What's the connection to energy-based models?\n12:15 - Is a Gaussian mixture model appropriate for high-dimensional data?\n16:15 - What are the most important components of the method?\n18:30 - What are the downstream effects of the regularizer?\n22:00 - Are there severe trade-offs to outlier detection?\n23:55 - Main experimental takeaways?\n26:10 - Why do outlier detection in the last layer?\n30:20 - What does it take to finish a research projects successfully?\n\nPaper: https://arxiv.org/abs/2202.01197\nCode: https://github.com/deeplearning-wisc/vos\n\nAbstract:\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\n\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "512": "#multitasklearning #biology #neuralnetworks\n\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\n\nOUTLINE:\n0:00 - Introduction\n1:20 - Paper Overview\n3:15 - Catastrophic forgetting in continuous and multi-task learning\n9:30 - Dendrites in biological neurons\n16:55 - Sparse representations in biology\n18:35 - Active dendrites in deep learning\n34:15 - Experiments on multi-task learning\n39:00 - Experiments in continual learning and adaptive prototyping\n49:20 - Analyzing the inner workings of the algorithm\n53:30 - Is this the same as just training a larger network?\n59:15 - How does this relate to attention mechanisms?\n1:02:55 - Final thoughts and comments\n\nPaper: https://arxiv.org/abs/2201.00042\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\n\nERRATA:\n- I was made aware of this by https://twitter.com/ChainlessCoder: \"That axon you showed of the pyramidal neuron, is actually the apical dendrite of the neuron\". Sorry, my bad :)\n\nAbstract:\nA key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\n\nAuthors: Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "513": "#blip #review #ai\n\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\n\nSponsor: Zeta Alpha\nhttps://zeta-alpha.com\nUse code YANNIC for 20% off!\n\nOUTLINE:\n0:00 - Intro\n0:50 - Sponsor: Zeta Alpha\n3:40 - Paper Overview\n6:40 - Vision-Language Pre-Training\n11:15 - Contributions of the paper\n14:30 - Model architecture: many parts for many tasks\n19:50 - How data flows in the model\n26:50 - Parameter sharing between the modules\n29:45 - Captioning & Filtering bootstrapping\n41:10 - Fine-tuning the model for downstream tasks\n\nPaper: https://arxiv.org/abs/2201.12086\nCode: https://github.com/salesforce/BLIP\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\n\nAbstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\n\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "514": "#deeplearning #nlp #sampling\n\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\n\nSponsor: Fully Connected by Weights & Biases\nhttps://wandb.ai/fully-connected\n\nOUTLINE:\n0:00 - Intro\n1:50 - Sponsor: Fully Connected by Weights & Biases\n4:10 - Paper Overview\n7:40 - What's the problem with sampling?\n11:45 - Beam Search: The good and the bad\n14:10 - Top-k and Nucleus Sampling\n16:20 - Why the most likely things might not be the best\n21:30 - The expected information content of the next word\n25:00 - How to trade off information and likelihood\n31:25 - Connections to information theory and psycholinguistics\n36:40 - Introducing Typical Sampling\n43:00 - Experimental Evaluation\n44:40 - My thoughts on this paper\n\nPaper: https://arxiv.org/abs/2202.00666\nCode: https://github.com/cimeister/typical-sampling/blob/3e676cfd88fa2e6a24f2bdc6f9f07fddb87827c2/src/transformers/generation_logits_process.py#L242-L272\n\nAbstract:\nDespite achieving incredibly low perplexities on myriad natural language corpora, today's language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (\u00e0 la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.\n\nAuthors: Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "515": "#nlp #gpt3 #prompt\n\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\n\nSponsor: Introduction to Graph Neural Networks Course\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Introduction to GNNs Course (link in description)\n1:30 - Paper Overview: Improve GPT-3 after deployment via user feedback\n5:30 - Proposed memory-based architecture\n13:00 - A detailed look at the components\n15:00 - Example tasks\n24:30 - My concerns with the example setup\n26:20 - Baselines used for comparison\n29:50 - Experimental Results\n34:20 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/2201.06009\nCode & Data: https://github.com/madaan/memprompt\n\nAbstract:\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \"What word is similar to good?\" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. All the code and data is available at this https URL.\n\nAuthors: Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "516": "#reinforcementlearning #ai #explained\n\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\n\nOUTLINE:\n0:00 - Intro\n1:10 - Paper Overview: Language for exploration\n5:40 - The MiniGrid & MiniHack environments\n7:00 - Annotating states with language\n9:05 - Baseline algorithm: AMIGo\n12:20 - Adding language to AMIGo\n22:55 - Baseline algorithm: NovelD and Random Network Distillation\n29:45 - Adding language to NovelD\n31:50 - Aren't we just using extra data?\n34:55 - Investigating the experimental results\n40:45 - Final comments\n\nPaper: https://arxiv.org/abs/2202.08938\n\nAbstract:\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\n\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rockt\u00e4schel, Edward Grefenstette\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "517": "#dsi #search #google\n\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model's weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\n\nSponsor: Diffgram\nhttps://diffgram.com?ref=yannic\n\nOUTLINE:\n0:00 - Intro\n0:45 - Sponsor: Diffgram\n1:35 - Paper overview\n3:15 - The search problem, classic and neural\n8:15 - Seq2seq for directly predicting document IDs\n11:05 - Differentiable search index architecture\n18:05 - Indexing\n25:15 - Retrieval and document representation\n33:25 - Training DSI\n39:15 - Experimental results\n49:25 - Comments & Conclusions\n\nPaper: https://arxiv.org/abs/2202.06991\n\nAbstract:\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "518": "#ai #accel #evolution\n\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\n\nOUTLINE:\n0:00 - Intro & Demonstration\n3:50 - Paper overview\n5:20 - The ACCEL algorithm\n15:25 - Looking at the pseudocode\n23:10 - Approximating regret\n33:45 - Experimental results\n40:00 - Discussion & Comments\n\nWebsite: https://accelagent.github.io\nPaper: https://arxiv.org/abs/2203.01302\n\nAbstract:\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\n\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rockt\u00e4schel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "519": "#saycan #robots #ai\n\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy's ability to progress towards the goal, given by the language model, and the policy's probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nSponsor: Zeta Alpha\nhttps://zeta-alpha.com\nUse code YANNIC for 20% off!\n\nOUTLINE:\n0:00 - Introduction & Overview\n3:20 - Sponsor: Zeta Alpha\n5:00 - Using language models for action planning\n8:00 - Combining LLMs with learned atomic skills\n16:50 - The full SayCan system\n20:30 - Experimental setup and data collection\n21:25 - Some weaknesses & strengths of the system\n27:00 - Experimental results\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "520": "#parti #ai #aiart \n\nParti is a new autoregressive text-to-image model that shows just how much scale can achieve. This model's outputs are crips, accurate, realistic, and can combine arbitrary styles, concepts, and fulfil even challenging requests.\n\nOUTLINE:\n0:00 - Introduction\n2:40 - Example Outputs\n6:00 - Model Architecture\n17:15 - Datasets (incl. PartiPrompts)\n21:45 - Experimental Results\n27:00 - Picking a cherry tree\n29:30 - Failure cases\n33:20 - Final comments\n\nWebsite: https://parti.research.google/\nPaper: https://arxiv.org/abs/2206.10789\nGithub: https://github.com/google-research/parti\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "521": "#openai #vpt #minecraft \n\nMinecraft is one of the harder challenges any RL agent could face. Episodes are long, and the world is procedurally generated, complex, and huge. Further, the action space is a keyboard and a mouse, which has to be operated only given the game's video input. OpenAI tackles this challenge using Video PreTraining, leveraging a small set of contractor data in order to pseudo-label a giant corpus of scraped footage of gameplay. The pre-trained model is highly capable in basic game mechanics and can be fine-tuned much better than a blank slate model. This is the first Minecraft agent that achieves the elusive goal of crafting a diamond pickaxe all by itself.\n\nOUTLINE:\n0:00 - Intro\n3:50 - How to spend money most effectively?\n8:20 - Getting a large dataset with labels\n14:40 - Model architecture\n19:20 - Experimental results and fine-tuning\n25:40 - Reinforcement Learning to the Diamond Pickaxe\n30:00 - Final comments and hardware\n\nBlog: https://openai.com/blog/vpt/\nPaper: https://arxiv.org/abs/2206.11795\nCode & Model weights: https://github.com/openai/Video-Pre-Training\n\nAbstract:\nPretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.\n\nAuthors: Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "522": "#jepa #ai #machinelearning \n\nYann LeCun's position paper on a path towards machine intelligence combines Self-Supervised Learning, Energy-Based Models, and hierarchical predictive embedding models to arrive at a system that can teach itself to learn useful abstractions at multiple levels and use that as a world model to plan ahead in time.\n\nOUTLINE:\n0:00 - Introduction\n2:00 - Main Contributions\n5:45 - Mode 1 and Mode 2 actors\n15:40 - Self-Supervised Learning and Energy-Based Models\n20:15 - Introducing latent variables\n25:00 - The problem of collapse\n29:50 - Contrastive vs regularized methods\n36:00 - The JEPA architecture\n47:00 - Hierarchical JEPA (H-JEPA)\n53:00 - Broader relevance\n56:00 - Summary & Comments\n\nPaper: https://openreview.net/forum?id=BZ5a1r-kVsf\n\nAbstract: How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons?  This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.\n\nAuthor: Yann LeCun\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "523": "#alphatensor #deepmind #ai \n\nMatrix multiplication is the most used mathematical operation in all of science and engineering. Speeding this up has massive consequences. Thus, over the years, this operation has become more and more optimized. A fascinating discovery was made when it was shown that one actually needs less than N^3 multiplication operations to multiply to NxN matrices. DeepMind goes a step further and creates AlphaTensor, a Deep Reinforcement Learning algorithm that plays a single-player game, TensorGame, in order to find even more optimized algorithms for matrix multiplication. And it turns out, there exists a plethora of undiscovered matrix multiplication algorithms, which not only will make everything from computers to smart toasters faster, but also bring new insights into fundamental math and complexity theory.\n\nSponsor: Assembly AI\nLink: https://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic_sentiment\n\nOUTLINE:\n0:00 - Intro\n1:50 - Sponsor: Assembly AI (link in description)\n3:25 - What even is Matrix Multiplication?\n6:10 - A very astounding fact\n8:45 - Trading multiplications for additions\n12:35 - Matrix Multiplication as a Tensor\n17:30 - Tensor Decompositions\n20:30 - A formal way of finding multiplication algorithms\n31:00 - How to formulate this as a game?\n39:30 - A brief primer on AlphaZero / MCTS\n45:40 - The Results\n48:15 - Optimizing for different hardware\n52:40 - Expanding fundamental math\n53:45 - Summary & Final Comments\n\nPaper: https://www.nature.com/articles/s41586-022-05172-4\nTitle: Discovering faster matrix multiplication algorithms with reinforcement learning\n\nAbstract:\nImproving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\u2009\u00d7\u20094 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.\n\nAuthors: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis & Pushmeet Kohli\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "524": "#neuralnetworks #machinelearning #ai \n\nAlexander Mattick joins me to discuss the paper \"Neural Networks are Decision Trees\", which has generated a lot of hype on social media. We ask the question: Has this paper solved one of the large mysteries of deep learning and opened the black-box neural networks up to interpretability?\n\nOUTLINE:\n0:00 - Introduction\n2:20 - Aren't Neural Networks non-linear?\n5:20 - What does it all mean?\n8:00 - How large do these trees get?\n11:50 - Decision Trees vs Neural Networks\n17:15 - Is this paper new?\n22:20 - Experimental results\n27:30 - Can Trees and Networks work together?\n\nPaper: https://arxiv.org/abs/2210.05189\n\nAbstract:\nIn this manuscript, we show that any feedforward neural network having piece-wise linear activation functions can be represented as a decision tree. The representation is equivalence and not an approximation, thus keeping the accuracy of the neural network exactly as is. We believe that this work paves the way to tackle the black-box nature of neural networks. We share equivalent trees of some neural networks and show that besides providing interpretability, tree representation can also achieve some computational advantages. The analysis holds both for fully connected and convolutional networks, which may or may not also include skip connections and/or normalizations.\n\nAuthor: Caglar Aytekin\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "525": "#ai #language #knowledge \n\nLarge Language Models have the ability to store vast amounts of facts about the world. But little is known, how these models actually do this. This paper aims at discovering the mechanism and location of storage and recall of factual associations in GPT models, and then proposes a mechanism for the targeted editing of such facts, in form of a simple rank-one update to a single MLP layer. This has wide implications both for how we understand such models' inner workings, and for our ability to gain greater control over such models in the future.\n\nOUTLINE:\n0:00 - Introduction\n1:40 - What are the main questions in this subfield?\n6:55 - How causal tracing reveals where facts are stored\n18:40 - Clever experiments show the importance of MLPs\n24:30 - How do MLPs store information?\n29:10 - How to edit language model knowledge with precision?\n36:45 - What does it mean to know something?\n39:00 - Experimental Evaluation & the CounterFact benchmark\n45:40 - How to obtain the required latent representations?\n51:15 - Where is the best location in the model to perform edits?\n58:00 - What do these models understand about language?\n1:02:00 - Questions for the community\n\nPaper: https://arxiv.org/abs/2202.05262\nFollow-up paper on Mass-Editing Memory in a Transformer: https://arxiv.org/abs/2210.07229\n\nAbstract:\nWe analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\n\nAuthors: Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "526": "#ai #galactica #meta\n\nGalactica is a language model trained on a curated corpus of scientific documents, such as papers, knowledge bases, reviews, and other articles. The model can be used in a generative fasion to assist scientific writing, do reference prediction, and much more, including a new approach to do step-by-step reasoning using a clever encoding of intermediate steps. This video explains the paper, but also dives into the drama that ensued once Meta released a public demo of the model.\n\nOUTLINE:\n0:00 - Introduction\n1:30 - Drama around the public demo\n16:00 - Start of paper review\n20:30 - Dataset construction and encoding\n23:30 - Encoding step-by-step reasoning using a scratchpad\n33:00 - Modelling scientific references & citations\n35:05 - Prompt Pre-Training\n37:10 - Architecture details\n38:30 - Experimental results\n49:20 - Conclusion\n\nPaper: https://galactica.org/static/paper.pdf\nWebsite: https://galactica.org/explore/\n\nAbstract:\nInformation overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.\n\nAuthors: Ross Taylor Marcin Kardas Guillem Cucurull Thomas Scialom Anthony Hartshorn Elvis Saravia Andrew Poulton Viktor Kerkez Robert Stojnic\n\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "527": "#ai #cicero #diplomacy \n\nA team from Meta AI has developed Cicero, an agent that can play the game Diplomacy, in which players have to communicate via chat messages to coordinate and plan into the future.\n\nPaper Title: Human-level play in the game of Diplomacy by combining language models with strategic reasoning\n\nCommented game by human expert: https://www.youtube.com/watch?v=u5192bvUS7k\n\nOUTLINE:\n0:00 - Introduction\n9:50 - AI in cooperation games\n13:50 - Cicero agent overview\n25:00 - A controllable dialogue model\n36:50 - Dialogue-conditional strategic planning\n49:00 - Message filtering\n53:45 - Cicero's play against humans\n55:15 - More examples & discussion\n\nHomepage: https://ai.facebook.com/research/cicero/\nCode: https://github.com/facebookresearch/diplomacy_cicero\nBlog: https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/\nPaper: https://www.science.org/doi/10.1126/science.ade9097\n\nAbstract:\nDespite much progress in training AI systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players' beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\n\nAuthors: Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "528": "#ai #meta #languagemodel \n\nLLaMA is a series of large language models from 7B to 65B parameters, trained by Meta AI. They train for longer on more data and show that something like gpt-3 can be outperformed by significantly smaller models when trained like this. Meta also releases the trained models to the research community.\n\nOUTLINE:\n0:00 - Introduction & Paper Overview\n4:30 - Rant on Open-Sourcing\n8:05 - Training Data\n12:40 - Training Hyperparameters\n14:50 - Architecture Modifications\n17:10 - Optimizer\n19:40 - Efficient Implementation\n26:15 - Main Results\n38:00 - Some more completions\n40:00 - Conclusion\n\n\nPaper: https://arxiv.org/abs/2302.13971\nWebsite: https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n\nAbstract:\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\n\nAuthors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "529": "#gpt4 #chatgpt #openai \n\nReferences:\nhttps://openai.com/product/gpt-4\nhttps://openai.com/research/gpt-4\nhttps://cdn.openai.com/papers/gpt-4.pdf\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "530": "#ai #transformer #gpt4 \n\nThis paper promises to scale transformers to 1 million tokens and beyond. We take a look at the technique behind it: The Recurrent Memory Transformer, and what its strenghts and weaknesses are.\n\nOUTLINE:\n0:00 - Intro\n2:15 - Transformers on long sequences\n4:30 - Tasks considered\n8:00 - Recurrent Memory Transformer\n19:40 - Experiments on scaling and attention maps\n24:00 - Conclusion\n\nPaper: https://arxiv.org/abs/2304.11062\n\nAbstract:\nThis technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.\n\nAuthors: Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "531": "#gpt4 #ai #prompt \n\nTree-of-Thought improves prompting of large language models (LLMs) by generalizing the concept of Chain-of-Thought prompting and introduces a tree search across language model thoughts, including state evaluation and backtracking. Experiments on toy tasks show large improvements over both classic and Chain-of-Thought prompting.\n\nOUTLINE:\n0:00 - Introduction\n1:20 - From Chain-of-Thought to Tree-of-Thought\n11:10 - Formalizing the algorithm\n16:00 - Game of 24 & Creative writing\n18:30 - Crosswords\n23:30 - Is this a general problem solver?\n26:50 - Ablation studies\n28:55 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.10601\n\nAbstract:\nLanguage models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.\n\nAuthors: Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "532": "#gpt4 #rwkv #transformer \n\nWe take a look at RWKV, a highly scalable architecture between Transformers and RNNs.\n\nFully Connected (June 7th in SF) Promo Link: https://www.fullyconnected.com/?promo=ynnc\n\nOUTLINE:\n0:00 - Introduction\n1:50 - Fully Connected In-Person Conference in SF June 7th\n3:00 - Transformers vs RNNs\n8:00 - RWKV: Best of both worlds\n12:30 - LSTMs\n17:15 - Evolution of RWKV's Linear Attention\n30:40 - RWKV's Layer Structure\n49:15 - Time-Parallel vs Sequence Mode\n53:55 - Experimental Results & Limitations\n58:00 - Visualizations\n1:01:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.13048\nCode: https://github.com/BlinkDL/RWKV-LM\n\nAbstract:\nTransformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.\n\nAuthors: Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "533": "#stablediffusion #ai #watermark \n\nWatermarking the outputs of generative models is usually done as a post-processing step on the model outputs. Tree-Ring Watermarks are applied in the latent space at the beginning of a diffusion process, which makes them nearly undetectable, robust to strong distortions, and only recoverable by the model author. It is a very promising technique with applications potentially beyond watermarking itself.\n\nOUTLINE:\n0:00 - Introduction & Overview\n1:30 - Why Watermarking?\n4:20 - Diffusion Models Recap\n13:40 - Inverting Diffusion Models\n17:05 - Tree-Ring Watermarking\n26:15 - Effects of Tree-Ring Watermarks\n30:00 - Experimental Results\n32:40 - Limitations\n34:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.20030\n\nAbstract:\nWatermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at this https URL.\n\nAuthors: Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", "534": "Take the Deep Learning Specialization: http://bit.ly/2VDnWxz\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "535": "Take the Deep Learning Specialization: http://bit.ly/3amgU4n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "536": "Take the Deep Learning Specialization: http://bit.ly/3cpg1K9\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "537": "Take the Deep Learning Specialization: http://bit.ly/2VDOhvx\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "538": "Take the Deep Learning Specialization: http://bit.ly/2PGCWHg\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "539": "Take the Deep Learning Specialization: http://bit.ly/2x5Z9YT\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "540": "Take the Deep Learning Specialization: http://bit.ly/2PGxIeE\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "541": "Take the Deep Learning Specialization: http://bit.ly/3cAd49Y\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "542": "Take the Deep Learning Specialization: http://bit.ly/3anivHa\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "543": "Take the Deep Learning Specialization: http://bit.ly/2vzq1jp\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "544": "Take the Deep Learning Specialization: http://bit.ly/2VGtjfI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "545": "Take the Deep Learning Specialization: http://bit.ly/2TwhvKf\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "546": "Take the Deep Learning Specialization: http://bit.ly/2vxxRu1\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "547": "Take the Deep Learning Specialization: http://bit.ly/2VGFA3w\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "548": "Take the Deep Learning Specialization: http://bit.ly/2x6x2J9\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "549": "Take the Deep Learning Specialization: http://bit.ly/2PWDKrR\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "550": "Take the Deep Learning Specialization: http://bit.ly/38iUGz1\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "551": "Take the Deep Learning Specialization: http://bit.ly/2vBcQOW\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "552": "Take the Deep Learning Specialization: http://bit.ly/3cqn45p\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "553": "Take the Deep Learning Specialization: http://bit.ly/2Tx5XGn\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "554": "Take the Deep Learning Specialization: http://bit.ly/2PFq843\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "555": "Take the Deep Learning Specialization: http://bit.ly/2vBG4xl\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "556": "Take the Deep Learning Specialization: http://bit.ly/2Tx69W7\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "557": "Take the Deep Learning Specialization: http://bit.ly/2TvWKhI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "558": "Take the Deep Learning Specialization: http://bit.ly/3anfsyN\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "559": "Take the Deep Learning Specialization: http://bit.ly/2VF2f00\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "560": "Take the Deep Learning Specialization: http://bit.ly/2PGrI5o\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "561": "Take the Deep Learning Specialization: http://bit.ly/2vAwCKt\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "562": "Take the Deep Learning Specialization: http://bit.ly/2x614g3\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "563": "Take the Deep Learning Specialization: http://bit.ly/2vBGGmD\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "564": "Take the Deep Learning Specialization: http://bit.ly/2xdG0Et\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "565": "Take the Deep Learning Specialization: http://bit.ly/2VMuKZT\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "566": "Take the Deep Learning Specialization: http://bit.ly/39xFIXq\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "567": "Take the Deep Learning Specialization: http://bit.ly/38u7YIW\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "568": "Take the Deep Learning Specialization: http://bit.ly/39EsebZ\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "569": "Take the Deep Learning Specialization: http://bit.ly/2OYiqBI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "570": "Take the Deep Learning Specialization: http://bit.ly/2wWBgmn\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "571": "Take the Deep Learning Specialization: http://bit.ly/3bEJYFN\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "572": "Take the Deep Learning Specialization: http://bit.ly/2HoKiun\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "573": "Take the Deep Learning Specialization: http://bit.ly/38tqcLF\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "574": "Take the Deep Learning Specialization: http://bit.ly/38vsKIW\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "575": "Take the Deep Learning Specialization: http://bit.ly/2wmfW9O\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "576": "Take the Deep Learning Specialization: http://bit.ly/3cmtNgK\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "577": "Take the Deep Learning Specialization: http://bit.ly/3csURe6\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "578": "Take the Deep Learning Specialization: http://bit.ly/2TwMTIp\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "579": "Take the Deep Learning Specialization: http://bit.ly/2PGJRAh\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "580": "Take the Deep Learning Specialization: http://bit.ly/2uLX3wo\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "581": "Take the Deep Learning Specialization: http://bit.ly/2TuCcGp\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "582": "Take the Deep Learning Specialization: http://bit.ly/3cA9P2i\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "583": "Take the Deep Learning Specialization: http://bit.ly/3asFxN7\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "584": "Take the Deep Learning Specialization: http://bit.ly/32IwkNS\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "585": "Take the Deep Learning Specialization: http://bit.ly/3anvAjI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "586": "Take the Deep Learning Specialization: http://bit.ly/2uNnPoh\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "587": "Take the Deep Learning Specialization: http://bit.ly/3crP5ti\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "588": "Take the Deep Learning Specialization: http://bit.ly/39iF1kn\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "589": "Take the Deep Learning Specialization: http://bit.ly/2wlj7OU\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "590": "Take the Deep Learning Specialization: http://bit.ly/2TwPwKh\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "591": "Take the Deep Learning Specialization: http://bit.ly/2wne28V\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "592": "Take the Deep Learning Specialization: http://bit.ly/3aeHnku\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "593": "Take the Deep Learning Specialization: http://bit.ly/2VB5LbX\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "594": "Take the Deep Learning Specialization: http://bit.ly/38jCe9e\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "595": "Take the Deep Learning Specialization: http://bit.ly/2IfZoml\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "596": "Take the Deep Learning Specialization: http://bit.ly/39h9zmx\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "597": "Take the Deep Learning Specialization: http://bit.ly/32IxMzO\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "598": "Take the Deep Learning Specialization: http://bit.ly/2IcuTOr\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "599": "Take the Deep Learning Specialization: http://bit.ly/2wksNJw\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "600": "Take the Deep Learning Specialization: http://bit.ly/32KQSWb\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "601": "Take the Deep Learning Specialization: http://bit.ly/39mD6eI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "602": "Take the Deep Learning Specialization: http://bit.ly/2IavakT\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "603": "Take the Deep Learning Specialization: http://bit.ly/2IjAPVz\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "604": "Take the Deep Learning Specialization: http://bit.ly/38kVjbc\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "605": "Take the Deep Learning Specialization: http://bit.ly/32JMjv2\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "606": "Take the Deep Learning Specialization: http://bit.ly/32Iw01H\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "607": "Take the Deep Learning Specialization: http://bit.ly/3aqFCk3\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "608": "Take the Deep Learning Specialization: http://bit.ly/2VEe1I1\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "609": "Take the Deep Learning Specialization: http://bit.ly/3cn54J7\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "610": "Take the Deep Learning Specialization: http://bit.ly/3aq55ui\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "611": "Take the Deep Learning Specialization: http://bit.ly/2VDnWxz\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "612": "Take the Deep Learning Specialization: http://bit.ly/3amgU4n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "613": "Take the Deep Learning Specialization: http://bit.ly/3cpg1K9\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "614": "Take the Deep Learning Specialization: http://bit.ly/2VDOhvx\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "615": "Take the Deep Learning Specialization: http://bit.ly/2PGCWHg\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "616": "Take the Deep Learning Specialization: http://bit.ly/2x5Z9YT\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "617": "Take the Deep Learning Specialization: http://bit.ly/2PGxIeE\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "618": "Take the Deep Learning Specialization: http://bit.ly/3cAd49Y\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "619": "Take the Deep Learning Specialization: http://bit.ly/3anivHa\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "620": "Take the Deep Learning Specialization: http://bit.ly/2vzq1jp\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "621": "Take the Deep Learning Specialization: http://bit.ly/2VGtjfI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "622": "Take the Deep Learning Specialization: http://bit.ly/2TwhvKf\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "623": "Take the Deep Learning Specialization: http://bit.ly/2vxxRu1\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "624": "Take the Deep Learning Specialization: http://bit.ly/2VGFA3w\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "625": "Take the Deep Learning Specialization: http://bit.ly/2x6x2J9\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "626": "Take the Deep Learning Specialization: http://bit.ly/2PWDKrR\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "627": "Take the Deep Learning Specialization: http://bit.ly/38iUGz1\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "628": "Take the Deep Learning Specialization: http://bit.ly/2vBcQOW\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "629": "Take the Deep Learning Specialization: http://bit.ly/3cqn45p\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "630": "Take the Deep Learning Specialization: http://bit.ly/2Tx5XGn\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "631": "Take the Deep Learning Specialization: http://bit.ly/2PFq843\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "632": "Take the Deep Learning Specialization: http://bit.ly/2vBG4xl\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "633": "Take the Deep Learning Specialization: http://bit.ly/2Tx69W7\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "634": "Take the Deep Learning Specialization: http://bit.ly/2TvWKhI\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "635": "Take the Deep Learning Specialization: http://bit.ly/3anfsyN\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "636": "Take the Deep Learning Specialization: http://bit.ly/2VF2f00\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "637": "Take the Deep Learning Specialization: http://bit.ly/2PGrI5o\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "638": "Take the Deep Learning Specialization: http://bit.ly/2vAwCKt\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "639": "Take the Deep Learning Specialization: http://bit.ly/2x614g3\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "640": "Take the Deep Learning Specialization: http://bit.ly/2vBGGmD\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "641": "Take the Deep Learning Specialization: http://bit.ly/2xdG0Et\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "642": "Take the Deep Learning Specialization: http://bit.ly/2VMuKZT\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "643": "Take the Deep Learning Specialization: http://bit.ly/39xFIXq\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "644": "Take the Deep Learning Specialization: http://bit.ly/38u7YIW\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "645": "Take the Deep Learning Specialization: http://bit.ly/2TTyw1d\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "646": "Take the Deep Learning Specialization: http://bit.ly/2IohyTa\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "647": "Take the Deep Learning Specialization: http://bit.ly/2vIiFtZ\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "648": "Take the Deep Learning Specialization: http://bit.ly/2TG7xWD\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "649": "Take the Deep Learning Specialization: http://bit.ly/32T2ne0\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "650": "Take the Deep Learning Specialization: http://bit.ly/2TrivAA\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "651": "Take the Deep Learning Specialization: http://bit.ly/39ufNQm\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "652": "Take the Deep Learning Specialization: http://bit.ly/32QyMC6\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "653": "Take the Deep Learning Specialization: http://bit.ly/38rt8rc\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "654": "Take the Deep Learning Specialization: http://bit.ly/2VNkn8g\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "655": "Take the Deep Learning Specialization: http://bit.ly/2VOTUqW\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "656": "Take the Deep Learning Specialization: http://bit.ly/32UaAij\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "657": "Take the Deep Learning Specialization: http://bit.ly/3cAOp59\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "658": "Take the Deep Learning Specialization: http://bit.ly/32SsRMN\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "659": "Take the Deep Learning Specialization: http://bit.ly/2xePxLt\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "660": "Take the Deep Learning Specialization: http://bit.ly/3cytlMv\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "661": "Take the Deep Learning Specialization: http://bit.ly/3azTrNr\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "662": "Take the Deep Learning Specialization: http://bit.ly/3czgKIT\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "663": "Take the Deep Learning Specialization: http://bit.ly/2ToRc9O\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "664": "Take the Deep Learning Specialization: http://bit.ly/2TodFUt\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "665": "Take the Deep Learning Specialization: http://bit.ly/39tdEnP\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "666": "Take the Deep Learning Specialization: http://bit.ly/32QFZC8\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "667": "", "668": "", "669": "", "670": "", "671": "", "672": "", "673": "This interview is published from deeplearning.ai\u2019s Deep Learning Specialization (https://www.coursera.org/specializations/deep-learning) on Coursera.\n\nIt is part of the course on \u201cConvolutional Neural Networks\u201d (https://www.coursera.org/learn/convolutional-neural-networks).", "674": "Dawn Song is well-known for her research on the intersection of deep learning and security. Aside from her research, Song is also a Professor in the Department of Electrical Engineering and Computer Science at UC Berkeley and CEO of Oasis Labs, a blockchain startup that is creating a privacy-first cloud computing platform no blockchain. She has received several awards for her work including the MacArthur Fellowship, the Guggenheim Fellowship, and Best Paper awards from top conferences.\n\nAndrew sits down with Song to chat about her unconventional career path and her current research projects.\n\nHere\u2019s what you\u2019ll learn in the interview:\n\n00:34: How Song first got started in deep learning and security\n4:00: How Song self-designed a reading program structured around representational learning\n13:22: How computer security can help deep learning\n17:03: Song\u2019s research on how to build resilient machine learning systems\n21:55 How a \u201cconsistency check\u201d approach can defend against attacks\n25:31: Song\u2019s work in AI and data privacy\n27:49: How deep learning can help computer security\n30:16: How Song\u2019s startup, Oasis Labs, is creating privacy-preserving smart contracts\n34:42: Song\u2019s advice for learners breaking into a new field\n\n\nWant to build your own career in deep learning? Get started by taking the Deep Learning Specialization.", "675": "Even the heroes of deep learning had to start somewhere! Hear how Pieter Abbeel and Andrej Karpathy first entered the world of AI.\n\nTake the Deep Learning Specialization: http://bit.ly/2OotIP4 \nCheck out all our courses: https://deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch\n\ndeeplearning.ai Twitter: https://twitter.com/deeplearningai_\ndeeplearning.ai Linkedin: https://www.linkedin.com/company/deeplearningai/", "676": "", "677": "\"A machine capable of learning? That sounds wonderful.\" Learn how heroes of deep learning Yann LeCun and Ruslan Salakhutdinov first became interested in AI.\n\nTake the Deep Learning Specialization: http://bit.ly/3c7dHau\nCheck out all our courses: deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch\n\nFollow us:\nTwitter: twitter.com/deeplearningai_\nFacebook: facebook.com/deeplearningHQ/\nLinkedin: linkedin.com/company/deeplearningai", "678": "", "679": "Andrew Ng and Helmuth Trefftz sit down during a Pie & AI meetup in Medell\u00edn, Colombia on August 22, 2019. Andrew explains why deeplearning.ai, Landing AI, and AI Fund chose to open their first international office in Medell\u00edn. He also discusses a government strategy for developing countries to harness AI and the future of deep learning technology.", "680": "Andrew Ng speaks about the progress of AI, how to accelerate AI adoption, and what's around the corner for AI at Amazon re:MARS 2019 in Las Vegas, Nevada.\n\nWant to leverage AI to drive business value across your company? Get started by taking AI for Everyone at coursera.org/learn/ai-for-everyone!", "681": "To celebrate the launch of the final course in the deeplearning.ai TensorFlow Specialization, we hosted our third Pie & AI meetup at Google's headquarters in Sunnyvale! Andrew was joined by TensorFlow Specialization instructor Laurence Moroney, TensorFlow Program Manager Alina Shinkarsky, and 175 Bay Area deep learners.\n\nHere are the topics Andrew, Laurence, and Alina covered:\n\nFireside chat:\n\n0:04: The state of AI now and in 20 years\n5:20: How to address the shortage of AI professionals\n10:19: Andrew and Laurence's favorite parts about creating the deeplearning.ai TensorFlow Specialization\n14:25: Why the TensorFlow Specialization focuses on Keras\n16:02: How someone completely new to AI can break into the field\n19:22: Recent advancements in AI that Andrew and Laurence are excited about\n\nAudience Q&A:\n\n22:08: RNNs vs. attention models\n22:58: What it takes for a small startup to be successful in AI (Editor's note: Unfortunately the video feed quality was reduced for the rest of the video. We apologize for the inconvenience.)\n24:37: Knowledge graphs and whether it will bring AI applications to the next level\u00a0\n29:15: How federated learning and differential privacy will emerge in the future\n32:37: The skills a successful AI practitioner should develop\n37:58: Working in academia vs. industry\u00a0\n\n\nWant to build applications with real-world data in TensorFlow? Enroll in the deeplearning.ai TensorFlow Specialization!", "682": "Professor Andrew Ng is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor at Stanford University. In 2011 he led the development of Stanford University\u2019s main MOOC (Massive Open Online Courses) platform and also taught an online Machine Learning class that was offered to over 100,000 students, leading to the founding of Coursera.\n\nNovember 7, 2017", "683": "Andrew Ng is one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He co-founded Coursera and Google Brain, launched deeplearning.ai, Landing.ai, and the AI fund, and was the Chief Scientist at Baidu. As a Stanford professor, and with Coursera and deeplearning.ai, he has helped educate and inspire millions of students including me.\n\nThis episode is presented by Cash App. Download it & use code \"LexPodcast\":\nCash App (App Store): https://apple.co/2sPrUHe\nCash App (Google Play): https://bit.ly/2MlvP5w\n\nPODCAST INFO:\nPodcast website:\nhttps://lexfridman.com/podcast\nApple Podcasts:\nhttps://apple.co/2lwqZIr\nSpotify:\nhttps://spoti.fi/2nEwCF8\nRSS:\nhttps://lexfridman.com/feed/podcast/\nFull episodes playlist:\nhttps://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\nClips playlist:\nhttps://www.youtube.com/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41\n\nEPISODE LINKS:\nAndrew Twitter: https://twitter.com/AndrewYNg\nAndrew Facebook: https://www.facebook.com/andrew.ng.96\nAndrew LinkedIn: https://www.linkedin.com/in/andrewyng/\ndeeplearning.ai: https://www.deeplearning.ai\nlanding.ai: https://landing.ai\nAI Fund: https://aifund.ai/\nAI for Everyone: https://www.coursera.org/learn/ai-for-everyone\nThe Batch newsletter: https://www.deeplearning.ai/thebatch/\n\nOUTLINE:\n0:00 - Introduction\n2:23 - First few steps in AI\n5:05 - Early days of online education\n16:07 - Teaching on a whiteboard\n17:46 - Pieter Abbeel and early research at Stanford\n23:17 - Early days of deep learning\n32:55 - Quick preview: deeplearning.ai, landing.ai, and AI fund\n33:23 - deeplearning.ai: how to get started in deep learning\n45:55 - Unsupervised learning\n49:40 - deeplearning.ai (continued)\n56:12 - Career in deep learning\n58:56 - Should you get a PhD?\n1:03:28 - AI fund - building startups\n1:11:14 - Landing.ai - growing AI efforts in established companies\n1:20:44 - Artificial general intelligence\n\nCONNECT:\n- Subscribe to this YouTube channel\n- Twitter: https://twitter.com/lexfridman\n- LinkedIn: https://www.linkedin.com/in/lexfridman\n- Facebook: https://www.facebook.com/LexFridmanPage\n- Instagram: https://www.instagram.com/lexfridman\n- Medium: https://medium.com/@lexfridman\n- Support on Patreon: https://www.patreon.com/lexfridman", "684": "It\u2019s easy for humans to tell the difference between a shirt and shoes, but not so easy for a computer. Andrew and Laurence explain why in Course 1 of our TensorFlow in Practice Specialization, available for $49 or to audit for free!\n\nTake the TensorFlow in Practice Specialization: http://bit.ly/38LojcE\nCheck out all our courses: deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai/", "685": "Andrew Ng takes you through the situations in which ML works well and doesn't work well in AI For Everyone.\n\nEnroll in AI For Everyone: http://bit.ly/2O4UOe0\nCheck out all our courses at www.deeplearning.ai!\nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai/", "686": "Laurence Moroney and Andrew Ng explain how to train a model more efficiently in a web browser. \n\nTake the TensorFlow: Data and Deployment Specialization: http://bit.ly/36eadyO\nCheck out all our courses: deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai/", "687": "The TensorFlow: Data and Deployment Specialization is now available on Coursera! Enroll now: http://bit.ly/3aeQS41\n\nCheck out all our courses: www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai/\"", "688": "You can find automation opportunities everywhere if you know how to look for them.\n\nTake AI For Everyone: http://bit.ly/3bwXtYj \nCheck out all our courses: deeplearning.ai \nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch \n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_ \nFacebook: https://www.facebook.com/deeplearningHQ/ \nLinkedin: https://www.linkedin.com/company/deeplearningai/", "689": "The final course of TensorFlow: Data and Deployment is out! Laurence and Andrew walk through what you'll learn. \n\nEnroll in TensorFlow: Data and Deployment: http://bit.ly/2OKQHEk\nCheck out all our courses: deeplearning.ai \nSubscribe to The Batch, our weekly newsletter: deeplearning.ai/thebatch \n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_ \nFacebook: https://www.facebook.com/deeplearningHQ/ \nLinkedin: https://www.linkedin.com/company/deeplearningai/", "690": "Laurence Moroney and Andrew Ng explain how TensorFlow makes it easy to augment image data.\n\nTake the TensorFlow in Practice Specialization: http://bit.ly/39BY2P9\nCheck out all our courses: deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "691": "Take a first look at the AI For Medicine Specialization! Andrew and instructor Pranav Rajpurkar walk you through the course topics. \n\nCourse 1 and 2 of the AI For Medicine Specialization will be available on Coursera on April 15. The third course will be available by the end of May.\n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "692": "The AI For Medicine Specialization will teach you to handle practical challenges when training models on medical data. Instructor Pranav Rajpurkar walks you through some key challenges. \n\nCourse 1 and 2 of the AI For Medicine Specialization will be available on Coursera on April 15. The third course will be available by the end of May.\n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "693": "Check out the Course 2 content we have in store for you! Andrew and Pranav walk through the topics of each week in our upcoming AI For Medicine Specialization.\n\nCourse 1 and 2 of the AI For Medicine Specialization will be available on Coursera on April 15. The third course will be available by the end of May.\n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "694": "Less than 24 hours until you can get your hands on our new AI For Medicine Specialization! While you wait, Pranav explains how deep learning can detect cancerous tissue.\n\nCourse 1 and 2 of the AI For Medicine Specialization will be available on Coursera on April 15. The third course will be available by the end of May.\n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "695": "Take the Deep Learning Specialization: http://bit.ly/3anPA5Y\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "696": "Take the Deep Learning Specialization: http://bit.ly/3azPTKP\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "697": "Take the Deep Learning Specialization: http://bit.ly/2PQrQQd\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "698": "Take the Deep Learning Specialization: http://bit.ly/2TF1B06\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "699": "Take the Deep Learning Specialization: http://bit.ly/2PN9yir\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "700": "Take the Deep Learning Specialization: http://bit.ly/2TIrbRM\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "701": "Take the Deep Learning Specialization: http://bit.ly/32V2BBe\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "702": "Take the Deep Learning Specialization: http://bit.ly/2IpF5Dk\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "703": "Take the Deep Learning Specialization: http://bit.ly/2PQjB6J\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "704": "Take the Deep Learning Specialization: http://bit.ly/330te8c\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "705": "Take the Deep Learning Specialization: http://bit.ly/39teu3R\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "706": "Take the Deep Learning Specialization: http://bit.ly/38sgOXN\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "707": "Take the Deep Learning Specialization: http://bit.ly/2IlGB9n\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "708": "Take the Deep Learning Specialization: http://bit.ly/2vyXdI6\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "709": "Take the Deep Learning Specialization: http://bit.ly/2TG0xZJ\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "710": "Take the Deep Learning Specialization: http://bit.ly/2TK8I7D\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "711": "Take the Deep Learning Specialization: http://bit.ly/2Tzr4ry\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "712": "Take the Deep Learning Specialization: http://bit.ly/3cyxAHV\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "713": "Take the Deep Learning Specialization: http://bit.ly/2VMlo09\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "714": "Take the Deep Learning Specialization: http://bit.ly/2vKdud0\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "715": "Take the Deep Learning Specialization: http://bit.ly/2IlIHpJ\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "716": "Take the Deep Learning Specialization: http://bit.ly/38pPolj\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "717": "Take the Deep Learning Specialization: http://bit.ly/39thYn3\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "718": "Take the Deep Learning Specialization: http://bit.ly/39u2Aa3\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "719": "Take the Deep Learning Specialization: http://bit.ly/2xgYjZz\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "720": "Take the Deep Learning Specialization: http://bit.ly/39sXdI6\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "721": "Take the Deep Learning Specialization: http://bit.ly/2TowhDV\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "722": "Take the Deep Learning Specialization: http://bit.ly/2TpUIRb\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "723": "Take the Deep Learning Specialization: http://bit.ly/2IpmuHg\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "724": "Take the Deep Learning Specialization: http://bit.ly/38lZe7G\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "725": "Take the Deep Learning Specialization: http://bit.ly/2wz3fZ6\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "726": "Take the Deep Learning Specialization: http://bit.ly/2TrvD8P\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "727": "Take the Deep Learning Specialization: http://bit.ly/3cvH3jc\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "728": "Take the Deep Learning Specialization: http://bit.ly/2uW9GFg\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "729": "Take the Deep Learning Specialization: http://bit.ly/2TtgW58\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "730": "Take the Deep Learning Specialization: http://bit.ly/2PQaZNs\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "731": "Take the Deep Learning Specialization: http://bit.ly/2Trz3bs\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "732": "Take the Deep Learning Specialization: http://bit.ly/2uWTio5\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "733": "Take the Deep Learning Specialization: http://bit.ly/2Tqxw5z\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "734": "Take the Deep Learning Specialization: http://bit.ly/32Rqs4S\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "735": "Take the Deep Learning Specialization: http://bit.ly/39rGF37\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "736": "Take the Deep Learning Specialization: http://bit.ly/32ZsrEn\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "737": "Take the Deep Learning Specialization: http://bit.ly/32TM54x\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "738": "Take the Deep Learning Specialization: http://bit.ly/38t3F0o\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "739": "Take the Deep Learning Specialization: http://bit.ly/38rjc0E\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "740": "Take the Deep Learning Specialization: http://bit.ly/2wzZnY8\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "741": "Take the Deep Learning Specialization: http://bit.ly/38m3B2A\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "742": "Take the Deep Learning Specialization: http://bit.ly/32Rrm1g\nCheck out all our courses: https://www.deeplearning.ai\nSubscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deeplearningai", "743": "Heroes of NLP is a video interview series featuring Andrew Ng, the founder of DeepLearning.AI, in conversation with thought leaders in NLP. Watch Andrew lead an enlightening discourse around how these industry and academic experts started in AI, their previous and current research projects, how their understanding of AI has changed through the decades, and what advice they can provide for learners of NLP. \n\nLooking to build a career in NLP? Enroll in the DeepLearning.AI NLP Specialization: https://bit.ly/33BaG0b\n\nComment below with links if you recognize any of the papers referenced in this interview!\n\nThis is an interview featuring Andrew Ng and Chris Manning, professor of Computer Science and Linguistics at Stanford University.\n\n00:58 - 05:39 Your journey getting started in AI \n05:42 - 07:22 Transformer \n07:23 - 19:36 Neural machine translation\n19:40 - 23:48 The paper on linear attention\n23:48 - 27:08 GloVe\n27:08 - 37:33 Thoughts on the trends of bigger and bigger NLP models, GPT-3 and whether the scale is a path to AGI\n37:34 - 41:38  What makes a good researcher and how does a student become creative\n41:53 - 45:20 Advice for someone looking to build a career in AI/NLP", "744": "Heroes of NLP is a video interview series featuring Andrew Ng, the founder of DeepLearning.AI, in conversation with thought leaders in NLP. Watch Andrew lead an enlightening discourse around how these industry and academic experts started in AI, their previous and current research projects, how their understanding of AI has changed through the decades, and what advice they can provide for learners of NLP. \n\nLooking to build a career in NLP? Enroll in the DeepLearning.AI NLP Specialization: https://bit.ly/33BaG0b\n\nComment below with links if you recognize any of the papers referenced in this interview!\n\nThis is an interview featuring Andrew Ng and Kathleen McKeown, a professor of Computer Science at Columbia University. \n\n01:02 - 04:03  How you became an NLP researcher\n04:03 - 07:44 Advice for someone trying to break into NLP themselves and wondering if they know enough or are good enough or shouldn\u2019t be in this field\n07:45 - 12:29 Current work at Columbia University on interdisciplinary work and novel summarization \n13:00 - 19:56 How do you pick research topics to work on\n19:57 - 25:10 current work about analyzing texts from African American community \n25:12 - 29:30 Thoughts on how the field of NLP has evolved over the years\n29:40 - 34:11What do you find most exciting in terms of emerging NLP technologies?", "745": "Heroes of NLP is a video interview series featuring Andrew Ng, the founder of DeepLearning.AI, in conversation with thought leaders in NLP. Watch Andrew lead an enlightening discourse around how these industry and academic experts started in AI, their previous and current research projects, how their understanding of AI has changed through the decades, and what advice they can provide for learners of NLP. \n\nLooking to build a career in NLP? Enroll in the DeepLearning.AI NLP Specialization: https://bit.ly/33BaG0b\n\nComment below with links if you recognize any of the papers referenced in this interview!\n\nThis is an interview featuring Andrew Ng and Oren Etioni, CEO of the Allen Institute for Artificial Intelligence. \n\n00:54 - 03:44 How did you get started in AI? Tell us your personal story\n03:44 - 07:43 Open Information Extraction from the web\n07:43 - 13:28 Semantic Scholar project; and the story behind it help sort out rapidly growing literature with the rise of COVID-10\n13:28 - 17:57 Advice for someone that\u2019s looking to work on or to launch a startup in NLP and the \u201cdirty little secret of data\u201d\n17:57 - 21:00 Prediction on the future of NLP trends when researchers are building bigger and bigger models\n21:00 - 23:35 Green AI\n23:35 - 26:38 Advice for someone trying to build a career and looking at academia and in industry\n26:38 - 31:44 Thoughts on bias, audit and regulating AI \n31:44 - 34:09 Advice for someone trying to break into NLP", "746": "Heroes of NLP is a video interview series featuring Andrew Ng, the founder of DeepLearning.AI, in conversation with thought leaders in NLP. Watch Andrew lead an enlightening discourse around how these industry and academic experts started in AI, their previous and current research projects, how their understanding of AI has changed through the decades, and what advice they can provide for learners of NLP. \n\nLooking to build a career in NLP? Enroll in the DeepLearning.AI NLP Specialization: https://bit.ly/33BaG0b\n\nComment below with links if you recognize any of the papers referenced in this interview!\n\nThis is an interview featuring Andrew Ng and Quoc Le, Research Scientist at Google Brain. \n\n01:12 - 08:08 Your journey getting into AI\n08:08 - 12:00 The story behind Google Cat project\n12:00 - 19:32 Sequence-to-Sequence Model\n19:32 - 28:18 Meena Chatbot project\n28:30 - 33:17 What are you most excited about in terms of NLP future? \n33:18 - 38:56 Advice for someone build their careers in AI", "747": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\n\nThis is a welcome video from Course 1, Week 1, Lesson 1. \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "748": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 2 video on \"Steps of an ML project\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "749": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 3 video on \"Steps of an ML project\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "750": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 4 video on \"Case study: speech recognition\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "751": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 5 video on \"Key challenges\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "752": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 6 video on \"Deployment patterns\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "753": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 7 video on \"Monitoring\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "754": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 1, Lesson 8 video on \"Pipeline monitoring\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "755": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 1 video on \"Modeling overview\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "756": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 2 video on \"Key challenges\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "757": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 3 video on \"Why low average test error isn\u00b4t good enough\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA\nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "758": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 4 video on \"Establish a baseline\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "759": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 5 video on \"Tips for getting started\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here:https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "760": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 6 video on \"Error analysis example\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "761": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 7 video on \"Prioritizing what to work on\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "762": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 8 video on \"Skewed datasets\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "763": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 9 video on \"Performance auditing\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "764": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 10 video on \"Data-centric AI development\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "765": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 11 video on \"A useful picture of data augmentation\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "766": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 12 video on \"Data augmentation\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "767": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 13 video on \"Can adding data hurt?\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "768": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 14 video on \"Adding features\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "769": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 15 video on \"Experiment tracking\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA   \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "770": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 2, Lesson 16 video on \"From big data to good data\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "771": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 1 video on \"Why is data definition hard?\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "772": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 2 video on \"More label ambiguity examples\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "773": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 3 video on \"Major types of data problems\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "774": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 4 video on \"Small data and label consistency\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "775": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 5 video on \"Improving label consistency\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "776": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 6 video on \"Human-level performance (HLP)\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "777": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 7 video on \"Raising HLP\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "778": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 8 video on \"Obtaining data\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "779": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 9 video on \"Data pipeline\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "780": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 10 video on \"Meta-data, data provenance and lineage \". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "781": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 11 video on \"Balanced train/ dev/ test splits\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "782": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 12 video on \"What is scoping?\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "783": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 13 video on \"Scoping process\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA  \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "784": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 14 video on \"Diligence on feasibility and value\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "785": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 15 video on \"Diligence on value\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep...", "786": "The Machine Learning Engineering for Production (MLOps) Specialization teaches you how to conceptualize, build, and maintain integrated systems that continuously operate in production. In this Specialization, you will become familiar with the capabilities, challenges, and consequences of machine learning engineering in production. By the end, you will be ready to employ your new production-ready skills to participate in the development of leading-edge AI technology and solve real-world problems.\nThis is a video from Course 1, Week 3, Lesson 16 video on \"Milestones and resourcing\". \n\nTo learn more about this and other topics and access the full course videos and assignments, enroll in the Specialization here: https://bit.ly/3v8pxwA \nCheck out all our programs: https://bit.ly/3L9rnmQ \nSubscribe to The Batch, our weekly newsletter: https://bit.ly/3vxv52R\n\nFollow us: \nTwitter: https://twitter.com/deeplearningai_\nFacebook: https://www.facebook.com/deeplearningHQ/\nLinkedin: https://www.linkedin.com/company/deep..."}, "tags": {"0": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "dqn", "deep q learning", "deep q networks", "q learning", "qlearning", "rl", "drl", "deep rl", "deep reinforcement learning", "deepmind", "david silver", "atari", "pong", "breakout", "space invaders", "agent", "cnn", "convolutional neural network", "bellman"], "1": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "classic", "alexnet", "hinton", "geoff hinton", "imagenet", "convolution", "convolutional neural network", "architecture", "dropout", "data augmentation", "cnns", "computer vision", "image classification", "object recognition", "classifier", "max pool", "pretraining", "deep neural networks"], "2": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gan", "generator", "discriminator", "convolution", "deconvolution", "goodfellow", "bengio", "convolutional neural network", "mnist", "cifar10", "generative", "generative model", "image generation", "face model", "latent space", "interpolation", "minmax", "nash equilibrium", "game theory"], "3": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "jeff dean", "mikolov", "word2vec", "word vectors", "word representations", "nlp", "natural language processing", "sentiment classification", "king", "queen", "man", "woman", "arithmetic", "latent space", "distributed", "country", "capital", "semantic", "synonyms", "skip gram", "negative sampling", "nce", "noise contrastive estimation"], "4": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "computer vision", "kaiming he", "google", "resnet", "resnet50", "resnet151", "deep neural network", "imagenet", "residual", "identity function", "very deep", "convolutional neural network", "bottleneck", "overfitting"], "5": ["machine learning phd", "how to do a phd in machine learning", "phd advice", "machine learning phd thesis topics", "machine learning phd topics", "how to machine learning phd", "how to select a thesis topic", "how to machine learning conferences", "how to write a machine learning paper", "advice for phd students", "advice for new phd students", "how to survive a phd", "what to do in a machine learning phd", "deep learning phd advice", "machine learning phd thesis", "machine learning phd thesis topic"], "6": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "lama", "inpainting", "gan", "adversarial", "loss function", "fourier transform", "fft", "fast fourier transform", "fourier convolution", "fast fourier convolution", "fourier convolution layer", "global information", "generative model", "periodic strucutre", "best inpainting", "ai inpainting", "first author interview", "lama inpainting", "mask filling", "large mask inpainting", "remove from picture", "ai image editing"], "7": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "reinforcement learning", "ai for go", "ai go", "ai chess", "chess ai", "stockfish", "alphazero", "alpha zero", "muzero", "player of games", "pog", "deepmind", "deepmind games", "imperfect information games", "ai for poker", "perfect vs imperfect information", "public state", "scotland yard", "ai for scotland yard", "reinforcement learning poker", "ai no limit holdem", "counterfactual regret minimization", "tree search"], "8": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "minecraft", "minerl", "minerl basalt", "minecraft machine learning", "minecraft ai", "human-like ai", "minecraft bot", "minecraft ai challenge", "minecraft reinforcement learning", "behavior cloning", "kairos", "minecraft kairos", "minerl kairos", "minerl winners", "interview", "with the authors", "minecraft deep learning", "minecraft behavior cloning", "gail", "generative adversarial imitation learning", "state machine"], "9": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "noether networks", "noether's theroem", "noether theorem", "symmetries", "neural network bias", "neural network symmetries", "inductive biases", "conserved quantities", "pendulum", "neural network physics", "deep learning physics", "deep learning symmetries", "group convolutions", "with the authors", "paper explained", "deep learning prediction", "test time optimization", "tailoring", "neural network tailoring"], "10": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "neural interpreters", "dynamic inference", "neural programming", "neural functions", "recurrent networks", "yoshua bengio", "mila", "schoelkopf", "attention", "modlin", "modulated linear layer", "weight sharing", "recurrent modules", "function modules", "sparse neural networks", "interview", "first author interview", "with the authors", "dynamic inference with neural interpreters", "deep neural interpreters"], "11": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "research", "symbolic", "symbolic regression", "neuro symbolic computation", "integer sequences", "oeis", "number sequences", "ai number sequences", "machine learning sequences", "integer sequence rules", "embedding space", "transformers", "attention mechanism", "sequence generation", "learning number sequences", "predicting number sequences", "facebook ai", "meta ai", "beam search", "symbolic vs numeric"], "12": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "leahy", "eleuther", "eleutherai", "eleuther ai", "connor leahy", "coreweave", "gooseai", "goose ai", "gpt neo", "gpt-neo", "gpt-neox", "gpt-neox-20b", "gpt-j", "open source", "huggingface", "transformer", "transformer models", "gpt-3", "open source gpt-3", "download gpt-neox", "gpu cluster", "large language model", "large language models", "machine learning tutorial"], "13": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "xcorr", "patrick mineault", "unsupervised models", "neuroscience", "neuroscience and deep learning", "deep learning brain", "machine learning brain", "brain models", "how does the brain work", "deep learning and neuroscience", "self-supervised models", "representation learning", "does the brain do representation learning", "does the brain work like a deep neural network", "neurips"], "14": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "natural language processing", "training data", "deep learning tutorial", "nlp", "gpt3", "gpt 3", "codex", "openai codex", "large language models", "gpt 3 planning", "zero-shot planning", "zero shot learning", "virtualhome", "virtual home", "bert", "bert model", "bert translation", "bert embedding", "pieter abbeel", "reinforcement learning", "human language learning"], "15": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "metalearning", "meta learning", "neural network", "unsupervised learning", "few shot learning", "google", "google research", "google ai", "transformer", "meta transformer", "hypertransformer", "hyper transformer", "generate the weights of a neural network", "privacy", "personalization", "interview", "paper explained", "semi-supervised learning"], "16": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "security", "machine learning in security", "ai security", "ai network security", "deep learning censorship", "ai censorship", "internet censorship", "geneva", "vpn", "genetic algorithms", "genetic algorithm", "genetic algorithm example", "real world genetic algorithm", "ai in the real world", "firewall", "evolution", "evolutionary search", "maryland", "breakerspace", "encryption", "amplification"], "17": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cm3", "facebook ai", "fair", "meta ai", "language model", "language modelling", "gpt-3", "gpt 3", "gpt3", "dall-e", "ru-dalle", "text to image", "ai image generation", "ai internet", "language model html", "transformer html", "large language models", "transformer", "autoregressive", "causal masking", "causally masked language model", "bidirectional", "bert", "masked language modelling"], "18": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpu", "tpu", "ipu", "wave computing", "dataflow", "near memory compute", "ai accelerators", "deep learning hardware", "sambanova", "cerebras", "graphcore", "mythic", "optical computing", "lightmatter", "groq", "why are gpus so fast", "why does deep learning need gpus", "do i need a gpu for deep learning", "transformers hardware", "hardware matrix multiplication", "fast deep learning", "machine learning hardware"], "19": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "20": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "21": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "alphacode", "alpha code", "deepmind", "deepmind code", "deepmind alphacode", "alphacoder", "codex", "copilot", "ai code", "ai programmer", "ai competitive programming", "ai leetcode", "machine learning leetcode", "deepmind leetcode", "codeforces", "large scale sampling", "language models", "language models for code", "ai python programmer", "deep mind", "fuzzing", "google deepmind", "competitive programming ai", "interview"], "22": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "formal math", "ai math", "ai math prover", "machine learning for math", "ml math", "artificial intelligence math", "ai mathematics", "automated proof search", "mini f2f", "ai imo", "ai math olympiad", "openai mathematics", "openai formal math", "language models formal math", "lean", "lean prover", "lean proof", "lean math", "ai lean environment", "ai proves theorems", "ai theorem prover"], "23": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "ml and society", "ai and society", "sociology and machine learning", "machine learning for sociology", "machine learning for economics", "ai microeconomics", "reinforcement learning economics", "society simulations", "silly rules", "social norms", "social norms enforcement", "why do social norms exist", "why do silly rules exist", "deep mind society"], "24": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "paper explained", "virtual outliers", "how to detect outliers", "deep learning outliers", "deep learning outlier detection", "vos", "deep learning energy", "latent space outliers", "density estimation", "classification boundaries", "generative models"], "25": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "active dendrites", "neurons dendrites", "biological deep learning", "deep learning biology", "numenta", "numenta research", "numenta deep learning", "dendrites deep learning", "deep learning tutorial", "hierarchical temporal memory", "computational neuroscience", "reinforcement learning", "robotics", "multi task learning", "continuous learning", "continual learning", "permuted mnist"], "26": null, "27": null, "28": null, "29": null, "30": null, "31": null, "32": null, "33": null, "34": null, "35": null, "36": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "sebastian risi", "copenhagen", "minecraft ai", "self-assembly", "self assembly", "nanobots", "swarm bots", "swarm ai", "evolution ai", "evolutionary methods", "genetic algorithms", "neural cellular automata", "cellular automata", "nca", "graph neural networks", "gnns", "self organization", "ant colony ai", "swarm intelligence", "interview", "emergence", "emergent properties"], "37": null, "38": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "glide", "diffusion", "clip-guided diffusion", "diffusion models", "clip-guided diffusion models", "generative models", "image to text", "generate image from text", "ai text to image", "machine learning text to image", "text 2 image", "classifier-free guidance", "noise process", "posterior", "variational lower bound", "log likelihood", "dalle", "dall-e", "ai drawing", "ai images"], "39": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "diffusion models", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "generative models", "parti", "google parti", "google party", "google pathways", "google imagen", "image", "dalle", "dalle2", "dalle 2", "dall e 2", "dall e 2 vs graphic designer", "anubis"], "40": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "imagen", "dalle", "dalle 2", "dall e", "dall e 2", "midjourney", "midjourney diffusion", "generative models", "ai art", "aiart", "mlnews", "ml news", "kilcher news", "ml news yannic", "google imagen", "cogview", "cog view", "cog view 2", "dalle mini", "dalle-mini", "dalle mega"], "41": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "stabilityai", "stabiliity ai", "stablediffusion", "stable diffusion", "eleuther ai", "laion", "laion 5b", "open source", "ai art", "diffusion models", "open source ai art"], "42": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "watermarks", "watermarking", "ai watermarking", "dalle", "stable diffusion", "diffusion explained", "stable diffusion explained"], "43": ["deep learning", "reinforcement learning", "deep reinforcement learning", "deep rl", "schmidhuber", "environment model", "imagination", "vae", "rnn", "lstm"], "44": null, "45": ["machine learning", "artificial intelligence", "ai", "deep learning", "unsupervised learning", "research", "academia", "paper", "review", "agents", "tasks"], "46": ["machine learning", "artificial intelligence", "ai", "deep learning", "reinforcement learning", "deep mind", "research", "academia", "paper", "review", "imagination", "planning", "agents"], "47": ["deep learning", "reinforcement learning", "deep mind", "academic", "paper", "research"], "48": ["machine learning", "reinforcement learning", "meta-learning", "deep rl", "deep reinforcement learning", "deep neural network", "atari", "alphago", "deepmind", "google", "td-gammon", "episodic memory", "inductive bias", "bias variance tradeoff"], "49": ["deep learning", "reinforcement learning", "deep reinforcement learning", "world model", "hierarchical reinforcement learning", "planning", "salesforce", "research", "machine learning", "navigation", "pivot states", "ai", "artificial intelligence"], "50": ["ml", "machine learning", "reinforcement learning", "recipe", "text-based games", "text games", "natural language processing", "nlp", "actor", "critic", "GRU", "embedding", "pretraining", "artificial intelligence", "ai", "competition", "microsoft"], "51": ["machine learning", "ml", "ai", "artificial intellgence", "deepmind", "reinforcement learning", "deep rl", "a2c", "a3c", "actor", "critic", "distributed", "scale", "bias", "off-policy", "policy gradient", "deepmind lab", "vtrace"], "52": ["ml", "ai", "machine learning", "reinforcement learning", "deep rl", "deepmind", "google", "starcraft", "alphastar", "alphago", "alphazero", "value function", "policy", "vtrace", "upgo", "terran", "protoss", "zerg", "build order", "strategy", "pointer network", "transformer", "league training", "league", "battlenet", "artificial intelligence", "bot", "rl", "deep reinforcement learning", "model-free", "exploiters", "self-play", "ficticious self-play", "rts"], "53": ["ml", "ai", "machine learning", "artificial ingelligence", "deep learning", "reinforcement learning", "model-free", "model-based", "search", "markov", "mdp", "pomdp", "implicit", "expectation", "wake-sleep"], "54": ["ml", "ai", "machine learning", "reinforcement learning", "deep rl", "deepmind", "google", "alphago", "alphazero", "value function", "policy", "artificial intelligence", "rl", "deep reinforcement learning", "model-free", "model-based", "environment model", "hidden representation", "latent state", "transition", "chess", "shogi", "go", "atari"], "55": ["rl", "reinforcement learning", "ai", "artificial intelligence", "udrl", "schmidhuber", "policy", "value", "reward"], "56": ["machine learning", "ml", "reinforcement learning", "rl", "ai", "artificial intelligence", "uber", "exploration", "hard exploration", "research", "novelty", "graph", "robustify", "explore", "montezuma", "montezuma's revenge", "pitfall", "atari"], "57": ["deep learning", "machine learning", "arxiv", "google", "rnn", "recurrent", "deepmind", "r2d2", "ngu", "reinforcement learning", "deep q learning", "replay buffer", "exploration", "exploitation", "tradeoff", "policy", "lstm", "atari"], "58": ["deep learning", "machine learning", "arxiv", "google", "rnn", "recurrent", "reinforcement learning", "deep reinforcement learning", "imagination", "latent space", "world model", "control", "deepmind", "deep mind"], "59": ["deep learning", "machine learning", "arxiv", "evolution", "reinforcement learning", "neat", "open-ended", "never ending", "population", "bipedal walker"], "60": ["deep learning", "machine learning", "unbounded", "open-ended", "evolution", "evolutionary", "uber", "uber ai", "distributed", "reinforcement learning", "rl", "generative"], "61": ["deep learning", "machine learning", "rl", "reinforcement learning", "unsupervised", "contrast", "contrastive", "encoder", "self-supervised", "deep rl", "representation", "representation learning", "query", "key"], "62": ["deep learning", "machine learning", "reinforcement learning", "deep rl", "off-policy", "on-policy", "replay buffer", "dataset", "benchmark", "berkeley", "rail", "offline", "online"], "63": ["deep learning", "machine learning", "reinforcement learning", "vector to go", "vtg", "continuous", "control", "robot", "concurrent", "deep rl", "deep neural networks", "berkeley", "google", "grasping", "qlearning"], "64": ["deep learning", "reinforcement learning", "society", "gini index", "welfare", "taxes", "brackets", "progressive", "regressive", "us", "poor", "rich", "equality", "redistribution", "outer loop", "world", "resources", "labor", "trade", "neural networks", "ppo"], "65": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "reinforcement learning", "deep reinforcement learning", "gans", "gan", "deconvolution", "computer chip", "gpu", "tpu", "fpga", "netlist", "constrained", "google"], "66": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "sac", "ppo", "deep rl", "deep reinforcement learning", "dreamer", "curl", "pixel", "pretraining", "deepmind", "openai", "berkeley"], "67": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "deep rl", "planning", "alphago", "alphazero", "alpha go", "alpha zero", "mcts", "monte carlo", "tree search", "subdivision", "recursive", "training data", "hindsight experience replay"], "68": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "deep rl", "deep reinforcement learning", "novelty", "curiosity", "intrinsic reward", "dreamer", "planet", "control", "walker", "run forward", "imaginary", "imagination", "planning", "google", "neural network", "actor", "critic", "uncertainty", "information gain", "mutual information", "model"], "69": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "deep rl", "human", "prior", "objects", "game", "video game", "key", "visuals", "enemy", "ladder", "gravity", "ablation"], "70": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "model predictive control", "dae", "denoising autoencoders", "trajectory", "trajectory optimization", "planning", "adversarial attack", "errors", "open loop", "closed loop", "joint", "probability", "derivative", "gaussian", "experience", "learned model", "world model", "model predictive", "mpc"], "71": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "deep rl", "control", "planning", "world model", "dads", "skills", "latent", "high level", "unsupervised", "tree search", "deep reinforcement learning", "mujoco", "ant", "google"], "72": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "level design", "game design", "video game", "sobokan", "sokoban", "zelda", "maze", "agent", "turtle", "observation", "reward", "action", "space", "deep rl", "deep reinforcement learning", "content", "minecraft"], "73": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "dqn", "deep q learning", "deep q networks", "q learning", "qlearning", "rl", "drl", "deep rl", "deep reinforcement learning", "deepmind", "david silver", "atari", "pong", "breakout", "space invaders", "agent", "cnn", "convolutional neural network", "bellman"], "74": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "rl", "deep rl", "deep reinforcement learning", "on-policy", "on policy", "off policy", "replay buffer", "normalization", "initialization", "control", "continuous control", "deep neural networks", "agent", "environment", "mujoco", "hyperparameters", "learning rate", "optimizer", "adam", "entropy", "regularization", "grid search"], "75": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "hebbian", "vision", "car", "ant", "quadruped", "neuroplasticity", "fire together wire together", "reinforcement learning", "deep rl", "deep reinforcement learning", "policy network", "policy gradient", "evolutionary methods", "evolution step", "population", "correlation", "gradient", "episode", "random", "adaptive", "reconfigure", "damage", "injury", "agent"], "76": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "deep rl", "q learning", "deep reinforcement learning", "q learning machine learning", "deep q learning", "successor features", "deep mind", "zero shot", "environment", "agent", "task", "linear", "regression", "reward", "mila", "neural network", "reinforcement learning", "value function", "state value function", "state value"], "77": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "nlp", "transformer", "gpt", "gpt3", "gpt-3", "gpt-2", "natural language processing", "summarization", "extractive", "reddit", "attention mechanism", "language model", "natural language understanding", "human feedback", "human in the loop", "active learning", "reward", "reward model", "reinforcement learning", "deep reinforcement learning", "deep rl", "ppo", "proximal policy optimization", "adversarial example", "broader impact"], "78": null, "79": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "chess", "kramnik", "fide", "rules", "alphago", "alpha go", "alphazero", "alpha zero", "mu zero", "muzero", "google", "reinforcement learning", "mcts", "rule change", "other rules", "alternate rules", "torpedo", "no castling", "pawn sideways", "self capture", "entropy", "opening theory", "rule based systems", "berlin defense", "opening", "stalemate", "deep rl", "deep reinforcement learning", "alphazero chess", "alphazero analysis"], "80": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "poker", "deep neural networks", "facebook", "facebook ai", "rebel", "holdem", "texas holdem", "rock paper scissors", "liars dice", "liar dice", "self play", "nash equilibrium", "alpha go", "alphazero", "zero sum", "policy", "cfr", "counterfactual regret minimization", "tree search", "monte carlo tree search", "mcts", "public belief state", "infostate", "value function", "supergradient", "strategy", "actor critic", "imperfect information"], "81": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "reinforcement learning", "deep reinforcement learning", "dreamer", "dreamer v2", "dreamer rl", "dreamer reinforcement learning", "google reinforcement learning", "deepmind reinforcement learning", "google ai", "world model", "world model reinforcement learning", "google deepmind world model", "google deepmind reinforcement learning", "atari reinforcement learning", "atari world model", "rainbow", "muzero"], "82": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "recurrent independent mechanisms", "metarim", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "machine learning paper", "deep reinforcement learning", "reinforcement learning meta learning", "yoshua bengio", "bentio mila", "grid world", "fast and slow learning", "reinforcement learning attention", "catastrophic forgetting", "lifelong learning", "multitask learning"], "83": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "decisiontransformer", "decision transformer", "berkeley", "uc berkeley", "facebook ai language", "fair", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "transformers for reinforcement learning", "transformers for rl", "transformer reinforcement learning", "sequence modeling", "sequence modelling", "sequence modeling reinforcement learning", "reinforcement learning with transformers"], "84": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "reinforcement learning", "imitation learning", "uc berkeley", "sergey levine", "sergey levine reinforcement learning", "pieter abbeel", "pieter abbeel reinforcement learning", "walk and punch", "learning from demonstration", "amp", "adversarial motion priors", "physics based reinforcement learning", "3d reinforcement learning"], "85": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "muzero", "alphazero", "berkeley", "pieter abbeel", "dreamer", "dreamerv2", "atari", "reinforcement learning", "deep reinforcement learning", "world model", "learned world model", "latent world model", "alphago", "deep rl", "model-based reinforcement learning", "how does muzero work", "efficientzero", "efficientzero model", "atari 100k", "sample-efficient reinforcement learning"], "86": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "reinforcement learning", "ai for go", "ai go", "ai chess", "chess ai", "stockfish", "alphazero", "alpha zero", "muzero", "player of games", "pog", "deepmind", "deepmind games", "imperfect information games", "ai for poker", "perfect vs imperfect information", "public state", "scotland yard", "ai for scotland yard", "reinforcement learning poker", "ai no limit holdem", "counterfactual regret minimization", "tree search"], "87": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "88": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "89": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "ml and society", "ai and society", "sociology and machine learning", "machine learning for sociology", "machine learning for economics", "ai microeconomics", "reinforcement learning economics", "society simulations", "silly rules", "social norms", "social norms enforcement", "why do social norms exist", "why do silly rules exist", "deep mind society"], "90": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "machine learning news", "ml paper", "machine learning paper", "language", "nlp", "natural language processing", "stanford", "reinforcement learning", "data science", "deep learning tutorial", "deep learning paper", "language in reinforcement learning", "rl nlp", "nlp rl", "nlp reinforcement learning", "exploration exploitation", "rl exploration"], "91": null, "92": null, "93": null, "94": null, "95": null, "96": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "deepmind alphatensor", "alpha tensor", "deepmind math", "google deep mind", "google deepmind", "matrix multiplication", "ai matrix multiplication", "matrix multiplication reinforcement learning", "alphazero", "alpha zero", "alphazero math", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "alphatensor explained", "alpha tensor explained"], "97": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "introduction to deep learning", "deep learning tutorial", "meta", "meta ai", "meta cicero", "cicero ai", "meta cicero ai", "diplomacy ai", "web diplomacy", "facebook ai", "fair ai", "language model", "politics ai", "geopolitics ai", "ai online game"], "98": ["gpt2", "transformer", "language model", "deep learning", "nlp", "openai", "security", "translation", "neural network", "attention", "attention mechanism", "unsupervised learning", "controversy"], "99": ["bert", "deep learning", "attention", "unsupervised", "nlp", "transformer", "squad", "wordpiece", "embeddings", "language", "language modeling", "attention layers", "bidirectional", "elmo", "natural language processing", "machine learning", "word vectors", "pretrained", "fine tuning"], "100": ["NeurIPS2018", "NIPS2018", "NLP", "deep learning", "RNN"], "101": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "tensor2tensor", "rnn", "recurrent", "seq2seq"], "102": ["deep learning", "machine learning", "artificial intelligence", "ai", "nlp", "natural language processing", "bert", "xlnet", "transformer", "transformer xl", "attention", "attention layer", "language model", "language modeling", "pretraining", "autoregressive", "autoencoder", "permutation", "google", "carnegie mellon", "cmu", "state of the art", "masked language model"], "103": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "tensor2tensor", "rnn", "recurrent", "seq2seq", "bert", "unsupervised", "squad", "wordpiece", "embeddings", "language", "language modeling", "attention layers", "bidirectional", "elmo", "word vectors", "pretrained", "fine tuning"], "104": ["ml", "machine learning", "reinforcement learning", "recipe", "text-based games", "text games", "natural language processing", "nlp", "actor", "critic", "GRU", "embedding", "pretraining", "artificial intelligence", "ai", "competition", "microsoft"], "105": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "memory", "lsh", "locality sensitive hashing", "reversible", "revertible", "flow", "long sequence"], "106": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "long sequence", "memory", "gpt-2", "Megatron", "Microsoft", "distributed", "parallelism"], "107": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "attention mechanism", "attention", "transformer", "rnn", "recurrent", "seq2seq", "facebook", "fair", "research", "math", "integral", "ode"], "108": ["deep learning", "machine learning", "nlp", "natural language processing", "arxiv", "attention", "evaluation", "cheat", "easy", "hard", "adversarial", "counterfactual", "hand-crafted", "test set", "supervised"], "109": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "seq2seq", "autoregressive", "independence", "decoding"], "110": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "attention mechanism", "attention", "transformer", "bert", "roberta", "mlm", "convolution", "memory", "linear", "sliding", "dilated", "sparse"], "111": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "chatbot", "dialogue", "persona", "vegan", "turing test", "natural language processing", "transformer", "generator", "context"], "112": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "bert", "nlp", "natural language processing", "wikitables", "sql", "tabular", "aggregations", "structured", "google"], "113": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "code", "pytorch", "bert", "pretrained", "lightning", "live", "tutorial", "pip", "nlp", "transformers", "tokenizers", "sequence", "sentiment", "imdb", "dataset", "full", "github"], "114": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "microsoft", "openai", "msbuild", "build", "code", "gpt2", "language model", "completion", "intellisense", "intellicode", "vscode", "github", "python", "code completion", "smart", "generate", "function body", "docstring", "name", "arguments", "programmer", "stackoverflow", "dataset", "interpolate"], "115": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "bert", "nlp", "lottery ticket", "good", "bad", "winning", "pruning", "weights", "attention", "transformer", "heads", "multi-head", "fine-tuning", "glue", "benchmark"], "116": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformers", "attention", "nlp", "natural language processing", "gpt3", "gpt-3", "gpt2", "gpt-2", "openai", "language model", "mlm", "autoregressive", "heads", "bert", "turing", "microsoft", "question answering", "news", "glue", "superglue", "sota", "preplexity", "corpus", "common crawl", "wikipedia", "natural questions", "boolq", "math", "strings", "context", "deep language", "zero shot", "few shot", "training data"], "117": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "machine translation", "google", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "memory", "lsh", "locality sensitive hashing", "reversible", "revertible", "flow", "long sequence"], "118": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "prune", "pruning", "transfer learning", "weights", "magnitude", "gradient", "moving", "small", "importance", "huggingface", "nlp", "natural language processing", "squad", "mnli", "bert", "transformer", "attention", "cnn", "distillation", "teacher", "sparse", "sparsity", "question answering", "mobile", "edge", "tune", "fine-tune"], "119": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "mt", "machine translation", "transformer", "bert", "lstm", "attention", "wmt", "wikipedia", "backtranslation", "bleu", "rouge", "ngrams", "score", "metric", "comparison", "human raters", "google", "google research", "automatic", "overlap", "distribution shift"], "120": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "121": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "facebook", "linear", "quadratic", "transformer", "attention", "self-attention", "multi-head attention", "t2t", "vasvani", "bert", "devlin", "roberta", "glue", "language modeling", "perplexity", "dot product", "johnson", "lindenstrauss", "random projection"], "122": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cnn", "visual", "resnet", "caption", "nlp", "transformer", "vasvani", "attention", "text", "coco", "imagenet", "convolutional neural network", "adaptation", "transfer learning", "quality", "unsupervised", "self-supervised"], "123": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "math", "derivative", "ode", "pde", "solution", "integral", "gradient", "jacobian", "mathematics", "language model", "transformer", "symbolic", "numeric", "stability", "equilibrium", "attention", "tokens", "dataset", "abstract"], "124": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "billion", "parameters", "float32", "attention mechanism", "transformer", "scale", "gpt-3", "google", "gshard", "xla", "sharding", "parallelism", "mixture of experts", "trillion", "tpus", "distributed", "m4", "multilingual translation", "natural language processing"], "125": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "bert", "transformer", "mlm", "language model", "masked language modeling", "proteins", "protein", "amino acid", "primary", "secondary", "tertiary", "structure", "helix", "strand", "band", "sheet", "turn", "binding site", "contact map", "dna", "rna", "amino acids", "proline", "phenylalanine"], "126": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "attention", "attention mechanism", "linear", "linear transformer", "linformer", "reformer", "idiap", "epfl", "queries", "keys", "softmax", "kernel", "routing", "inner product", "rnn", "recurrent neural network", "transformer", "bert", "autoregressive", "dimensions", "topic modeling", "language model"], "127": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "jeff dean", "mikolov", "word2vec", "word vectors", "word representations", "nlp", "natural language processing", "sentiment classification", "king", "queen", "man", "woman", "arithmetic", "latent space", "distributed", "country", "capital", "semantic", "synonyms", "skip gram", "negative sampling", "nce", "noise contrastive estimation"], "128": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "google research", "bigbird", "big bird", "bert", "attention", "attention is all you need", "longformer", "random attention", "quadratic attention", "attention mechanism", "qa", "natural questions", "hotpot qa", "genomics", "nlp", "natural language processing", "transformer", "transformers", "fully connected", "sparse attention", "graph", "star graph", "turing complete", "universal approximation", "window attention", "convolution"], "129": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "schmidhuber", "hochreiter", "lstm", "gru", "rnn", "hopfield", "attention", "attention is all you need", "transformer", "bert", "query", "key", "value", "routing", "pattern", "retrieval", "store", "error", "exponental", "binary", "continuous", "hopfield network", "lse", "energy function", "update rule", "metastable", "separation"], "130": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "orqa", "qa", "question answering", "google", "kenton", "wikipedia", "mlm", "bert", "masked language modeling", "realm", "t5", "transformer", "inner product", "mips", "index", "pretraining", "ict", "inverse cloze task", "google ai", "search", "retrieval", "documents", "natural questions", "open domain", "attention", "salient", "masking", "encoder"], "131": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "nlp", "transformer", "gpt", "gpt3", "gpt-3", "gpt-2", "natural language processing", "summarization", "extractive", "reddit", "attention mechanism", "language model", "natural language understanding", "human feedback", "human in the loop", "active learning", "reward", "reward model", "reinforcement learning", "deep reinforcement learning", "deep rl", "ppo", "proximal policy optimization", "adversarial example", "broader impact"], "132": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "natural language understanding", "data science", "transformer", "attention", "attention mechanism", "transformers", "attention is all you need", "gpus", "tpu", "linformer", "reformer", "explanation", "imagenet64", "kernels", "gaussian kernel", "softmax", "softmax kernel", "approximation", "random features", "random positive features", "random fourier features", "google", "favor", "machine translation"], "133": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "bert", "gpt", "gpt2", "gpt-2", "gpt3", "gpt-3", "gpt 2", "gpt 3", "knowledge graph", "knowledge base", "language", "natural language understanding", "berkeley", "uc berkeley", "dawn song", "unsupervised", "extraction", "corpus", "wikidata", "wikipedia", "entity linking", "entity recognition", "spacy", "attention", "attention matrix", "beam search", "viterbi", "causal attention", "language model", "autoregressive"], "134": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "apple", "openai", "berkeley", "stanford", "carlini", "dawn song", "google ai", "nlp", "natural language processing", "gpt", "gpt2", "gpt-2", "gpt3", "gpt-3", "gpt 2", "gpt 3", "bert", "transformers", "attention", "training data", "security", "leak", "privacy", "data protection", "ethics", "broader impact", "likelihood", "perplexity", "entropy", "url", "uuid", "personal information", "address", "private", "user data", "gdpr", "adversarial", "zlib"], "135": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt", "gpt-3", "visual transformer", "transformer", "transformers", "attention mechanism", "vqvae", "vq vae", "vq-vae", "codebook", "relaxation", "gumbel", "text", "images", "nlp", "natural language processing", "autoregressive", "grid", "encoder", "decoder", "gpt3", "avocado chair", "porcupine sphere", "animations", "fisheye", "text to image", "image captioning", "openai", "sutskever", "dali", "dalle", "walle", "vector quantized", "hierarchical", "gan", "generative", "likelihood"], "136": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "sutskever", "radford", "meme", "dalle", "dall-e", "images", "vision", "text", "nlp", "natural language processing", "resnet", "vision transformer", "transformer", "visual transformer", "sota", "state of the art", "zero shot", "zero-shot", "few shot", "few-shot", "unsupervised", "contrastive", "simclr", "efficientnet", "noisy student", "representation", "embedding", "latent", "natural language", "prompt engineering", "bias", "scale", "distribution shift"], "137": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention", "transformer", "attention mechanism", "google", "google brain", "shazeer", "trillion", "trillion parameter", "language model", "gpt3", "gpt-3", "gpt 3", "t5", "sharding", "mesh", "mtf", "mesh tensorflow", "query", "key", "value", "feed forward", "experts", "routing", "mixture of experts", "sparse", "sparse experts", "data parallelism", "model parallelism", "expert parallelism", "trillion parameters", "perplexity", "scaling", "flops", "bfloat16"], "138": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformer", "rnn", "lstm", "seq2seq", "gpt3", "gpt-3", "nlp", "natural language processing", "language modelling", "feedback transformers", "memory", "attention", "attention mechanism", "attention is all you need", "facebook ai", "fair", "long range", "complex", "reasoning", "bert", "autoregressive", "reinforcement learning", "abstraction", "representation", "higher layers", "attention matrix", "recurrent neural networks"], "139": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "machine learning explained", "transformers explained", "nystrom", "nystromformer", "nystromer", "nystrom approximation", "self attention", "attention mechanism", "attention is all you need", "transformer", "linear transformer", "linformer", "linear attention", "machine learning tutorial", "quadratic attention", "matrix approximation", "low rank", "landmark points", "landmarks", "matrix reconstruction", "fast attention"], "140": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "huggingface", "huggingface transformers", "microsoft", "microsoft research", "bert", "roberta", "deberta", "nlp", "natural language processing", "glue", "superglue", "state of the art", "transformers", "attention", "attention mechanism", "disentanglement", "disentangled representation", "positional encodings", "position embeddings", "masked language modelling", "pretraining", "open source"], "141": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "alibi", "transformer", "position encoding", "position embeddings", "fair", "google", "attention is all you need", "causal masking", "causal attention", "attentin matrix", "attention matrix", "vasvani", "sinusoidal position encodings", "learned position embeddings", "train short test long", "alibi position encodings", "transformer position encodings", "transformer position embeddings", "transformer long sequences"], "142": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "inftyformer", "infinityformer", "infty former", "infinity former", "transformer", "transformers", "transformer linear", "linear attention", "unbounded memory transformer", "continuous attention", "attention mechanism", "continuous attention mechanism", "radial basis function", "radial basis functions", "ridge regression", "long term memory", "long term memory explained"], "143": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt-3", "truthful", "truthfulqa", "conspiracy", "conspiracy theories", "large language models", "ezra klein", "inverse scaling", "openai", "gpt-j", "gpt-neo", "imitative falsehoods", "adversarial", "informativeness", "evaluation", "trustworthy", "ml bias", "are language models biased", "is gpt-3 truthful", "question answering", "harmful prompt", "helpful prompt"], "144": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt-3", "knowledge distillation", "teacher", "student", "nlp", "natural language processing", "gpt3", "prompt engineering", "symbolic knowledge", "symbolic reasoning", "symbolic nlp", "knowledge graphs", "triples", "what does gpt-3 know", "does gpt-3 understand"], "145": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "terraformer", "scaling transformers", "nli", "nlp", "natural language processing", "transformers memory", "deep learning memory", "fast transformer", "fast transformers", "attention", "attention mechanism", "attention is all you need", "bert", "gpt-3", "google research", "reversible layers", "reformer", "sparse attention", "sparse feedforward", "low-rank"], "146": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "leahy", "eleuther", "eleutherai", "eleuther ai", "connor leahy", "coreweave", "gooseai", "goose ai", "gpt neo", "gpt-neo", "gpt-neox", "gpt-neox-20b", "gpt-j", "open source", "huggingface", "transformer", "transformer models", "gpt-3", "open source gpt-3", "download gpt-neox", "gpu cluster", "large language model", "large language models", "machine learning tutorial"], "147": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "natural language processing", "training data", "deep learning tutorial", "nlp", "gpt3", "gpt 3", "codex", "openai codex", "large language models", "gpt 3 planning", "zero-shot planning", "zero shot learning", "virtualhome", "virtual home", "bert", "bert model", "bert translation", "bert embedding", "pieter abbeel", "reinforcement learning", "human language learning"], "148": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cm3", "facebook ai", "fair", "meta ai", "language model", "language modelling", "gpt-3", "gpt 3", "gpt3", "dall-e", "ru-dalle", "text to image", "ai image generation", "ai internet", "language model html", "transformer html", "large language models", "transformer", "autoregressive", "causal masking", "causally masked language model", "bidirectional", "bert", "masked language modelling"], "149": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "alphacode", "alpha code", "deepmind", "deepmind code", "deepmind alphacode", "alphacoder", "codex", "copilot", "ai code", "ai programmer", "ai competitive programming", "ai leetcode", "machine learning leetcode", "deepmind leetcode", "codeforces", "large scale sampling", "language models", "language models for code", "ai python programmer", "deep mind", "fuzzing", "google deepmind", "competitive programming ai"], "150": null, "151": null, "152": null, "153": null, "154": null, "155": null, "156": null, "157": null, "158": null, "159": null, "160": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "161": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "galactica", "meta", "meta ai", "facebook ai", "ai science", "galactica ai", "galactica model", "yann lecun", "research", "fair", "deep learning tutorial", "what is deep learning", "introduction to deep learning"], "162": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chatgpt", "chat gpt", "openai chat gpt", "openai chatbot gpt", "openai chatbot", "gpt-3 chatbot", "gpt-4", "gpt 3 chatbot", "ml news", "mlnews", "ai news", "what is deep learning", "deep learning tutorial", "chatgpt jailbreak"], "163": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chatgpt", "chat gpt", "openai chat gpt", "chat gpt alternative", "chat gpt alternative open source", "chatgpt alternative", "open source chatgpt", "open source language model", "chatgpt github", "openassistant", "open assistant", "open-assistant", "open assistant laion", "open assistant yannic", "chatgpt free", "chat gpt free", "openai", "instructgpt"], "164": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "meta ai", "meta llama", "llama llm", "gpt-3", "large language models", "transformers", "chatgpt", "instruction tuning", "llama-i", "llama paper"], "165": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "openai", "gpt 4", "gpt-4", "gpt4", "chatgpt4", "chatgpt 4", "gpt 4 paper"], "166": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformers", "longformer", "transformer long documents", "paper explained", "recurrent transformer", "transformer xl", "what is deep learning", "mit deep learning", "deep learning basics"], "167": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "168": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt4", "gpt 4", "recurrent neural network", "rwkv", "rvkw", "transformer alternative", "gpt-4", "deep learning tutorial", "what is deep learning"], "169": ["machine learning", "deep learning", "adversarial examples", "adversarial samples", "pgd", "projected gradient descent", "vulnerabiliby", "security", "artificial intelligence", "MIT", "geometry", "classifier", "deep neural network", "attack", "convolutional neural networks", "research", "robust features", "robust classifier", "robust network", "neural network"], "170": ["deep learning", "machine learning", "nlp", "natural language processing", "arxiv", "attention", "evaluation", "cheat", "easy", "hard", "adversarial", "counterfactual", "hand-crafted", "test set", "supervised"], "171": ["deep learning", "machine learning", "adversarial examples", "iid", "ood", "distribution", "bias", "discrimination", "neural networks", "bugs", "distortions", "data pipeline", "causality", "intention", "grounding"], "172": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "apple", "openai", "berkeley", "stanford", "carlini", "dawn song", "google ai", "nlp", "natural language processing", "gpt", "gpt2", "gpt-2", "gpt3", "gpt-3", "gpt 2", "gpt 3", "bert", "transformers", "attention", "training data", "security", "leak", "privacy", "data protection", "ethics", "broader impact", "likelihood", "perplexity", "entropy", "url", "uuid", "personal information", "address", "private", "user data", "gdpr", "adversarial", "zlib"], "173": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "adversarial examples", "goodfellow", "goodfellow adversarial attacks", "adversarial attacks on neural networks", "features not bugs", "madry", "dimpled manifold", "why do adversarial examples exist", "adversarial examples explanation", "adversarial attacks explanation", "computer vision", "decision boundary", "data manifold", "low dimensional manifold", "what are adversarial examples", "what is deep learning"], "174": ["machine learning", "ai", "artificial intelligence", "open ended learning", "quality diversity", "conference", "icml", "icml2019", "tutorial", "population-based search", "goal switching", "serendipidy", "evolution"], "175": ["machine learning", "ai", "artificial intelligence", "open ended learning", "quality diversity", "conference", "icml", "icml2019", "tutorial", "population-based search", "goal switching", "serendipidy", "evolution", "interview", "podcast"], "176": ["machine learning", "bias", "variance", "tradeoff", "generalization", "overfitting", "interpolation", "parameters", "model class", "complexity", "deep learning", "neural networks", "overparameterization", "erm", "random fourier features"], "177": ["machine learning", "deep learning", "dl", "neural network", "training", "convergence", "loss", "importance", "speed-up", "faster", "ai", "dnn", "deep neural network", "backprop", "backpropagation", "cifar10", "svhn", "classifier"], "178": ["ml", "machine learning", "cnn", "imagenet", "pretraining", "finetuning", "fine-tuning", "google", "benchmark", "initialization", "supervised", "unsupervised", "bert", "artificial intelligence", "score"], "179": ["deep learning", "machine learning", "neural networks", "pruning", "distillation", "quantization", "size", "weights", "optimization", "training", "generalization", "overparameterization", "winning ticket", "winning lottery ticket", "arxiv"], "180": ["deep learning", "machine learning", "arxiv", "google", "semi-supervised", "unlabeled", "augmentation", "research", "randaugment"], "181": ["deep learning", "machine learning", "neural networks", "multi task", "conflicting gradients", "magnitudes", "adam", "sgd", "momentum", "optimization", "projection"], "182": ["deep learning", "machine learning", "imagenet", "visualization", "features", "intermediate", "hidden layers", "activations", "patterns", "openai", "google", "interactive", "explanation"], "183": ["deep learning", "machine learning", "adversarial examples", "iid", "ood", "distribution", "bias", "discrimination", "neural networks", "bugs", "distortions", "data pipeline", "causality", "intention", "grounding"], "184": ["deep learning", "machine learning", "biologically plausible", "neural networks", "spiking", "neurons", "neuroscience", "hinton", "google", "deepmind", "brain", "cells", "soma", "axon", "interneurons", "action potential", "backprop"], "185": ["deep learning", "machine learning", "supervised learning", "classification", "classifier", "labels", "pretraining", "unsupervised", "self-supervised", "representation learning", "representations", "hidden space", "loss function", "google", "mit", "imagenet"], "186": ["deep learning", "machine learning", "imagenet", "cifar10", "cifar10.1", "generalization", "overfitting", "mturk", "arxiv", "vision", "models", "research", "hardness", "accuracy", "classifier", "resnet"], "187": ["deep learning", "machine learning", "initialization", "mask", "arxiv", "uber", "training", "subnetwork", "overparameterization", "zero", "frozen", "weights"], "188": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "brain", "cnn", "convolutional neural network", "resnet", "residual network", "pretraining", "finetuning", "vtab", "imagenet", "cifar", "state of the art", "pretrained", "computer vision"], "189": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "ebm", "energy function", "gradient descent", "relational neural network", "latent", "attention", "entities", "spatial relation", "inference time", "reasoning", "demonstration"], "190": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "brain", "pipeline", "bottleneck", "speed", "gpu", "tpu", "idle", "network", "distributed", "preprocessing", "augmentation"], "191": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "investigation", "linear probes", "usefulness", "representations", "intermediate", "hidden layers", "self-supervised", "rotnet", "crop", "augmentation", "color jitter", "dataset"], "192": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "193": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "classifier", "dnn", "cnn", "high dimensions", "class boundaries", "mixing", "interpolation", "latent", "beta", "regularizer", "regularization", "generalization", "adversarial examples", "smooth"], "194": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "195": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "196": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "197": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "198": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gnn", "transformer", "graph", "biology", "neurons", "axon", "dendrites", "plausible", "biologically plausible", "backprop", "backpropagation", "dfa", "feedback alignment", "random projections"], "199": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ssl", "semi-supervised", "transfer learning", "cnn", "resnet", "efficientnet", "noise", "augmentation", "data augmentation", "randaugment", "dropout", "stochastic depth", "google", "distillation", "self-training", "knowledge distillation", "imagenet", "unsupervised", "unlabeled", "unlabelled", "jft"], "200": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "billion", "parameters", "float32", "attention mechanism", "transformer", "scale", "gpt-3", "google", "gshard", "xla", "sharding", "parallelism", "mixture of experts", "trillion", "tpus", "distributed", "m4", "multilingual translation", "natural language processing"], "201": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ensemble", "pytorch", "lightning", "cifar10", "github", "vim", "code", "cuda", "gpu", "research", "ml", "ml research", "how to", "implement", "live coding", "python", "self", "distillation", "born again", "deep ensembles", "cnn", "resnet", "vgg", "torchvision", "imagenet"], "202": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "supsup", "supermasks", "lottery ticket", "lottery ticket hypothesis", "gradient", "entropy", "surplus", "superfluous neurons", "lifelong learning", "multitask learning", "catastrophic forgetting", "continuous learning", "binary mask", "random network", "optimization", "hopfield network", "gradient descent", "superposition"], "203": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "supsup", "supermasks", "lottery ticket", "lottery ticket hypothesis", "gradient", "entropy", "surplus", "superfluous neurons", "lifelong learning", "multitask learning", "catastrophic forgetting", "continuous learning", "binary mask", "random network", "optimization", "hopfield network", "gradient descent", "superposition"], "204": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ensembles", "bayesian", "modes", "loss function", "nonconvex", "google", "deepmind", "stan fort", "foundational", "weight space", "labels", "agreement", "minima", "loss landscape", "trajectory", "local minima", "optimization"], "205": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nas", "nas-bench", "architecture search", "initialization", "untrained", "cifar10", "imagenet", "neural architecture search", "controller", "rnn", "correlation", "gradient", "jacobian", "linearization"], "206": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "schmidhuber", "hochreiter", "lstm", "gru", "rnn", "hopfield", "attention", "attention is all you need", "transformer", "bert", "query", "key", "value", "routing", "pattern", "retrieval", "store", "error", "exponental", "binary", "continuous", "hopfield network", "lse", "energy function", "update rule", "metastable", "separation"], "207": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cnn", "imagenet", "resnet", "radioactive", "fake", "feature", "feature space", "feature extractor", "facebook ai", "fair", "deep neural networks", "classifier", "classes", "backpropagation", "black box", "white box", "detect", "features", "privacy", "adversarial examples", "tagging", "inria"], "208": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "biology", "biological", "alive", "living", "message passing", "global state", "local state", "information", "cellular automata", "neural cellular automata", "neural ca", "convolution", "recurrent", "rnn", "pixels", "cell state", "latent state", "distill", "distill pub", "mnist", "neural network", "digit classification"], "209": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "optimization", "lstm", "taskset", "google", "google research", "compute", "outer optimization", "adam", "adamw", "sgd", "momentum", "learning rate", "gradient", "learned optimizer", "second moment", "cnn", "rnn", "paper explained", "neural network", "gradient descent", "hyper parameters", "grid search", "mnist", "cifar10", "imagenet"], "210": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "optimization", "polyak", "nesterov", "benchmark", "cnn", "cifar", "mnist", "adam", "adagrad", "adadelta", "momentum", "sgd", "gradient", "learning rate", "tuning", "budget", "default parameters", "comparison", "grid search", "random search", "random seed", "vae", "learning rate schedule", "cosine decay", "trapezoid", "improvement", "best optimizer", "best optimizer for deep learning", "stochastic gradient descent"], "211": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "pipeline", "ml pipeline", "deep networks", "epidemiology", "theoretical", "underspecification", "overparameterization", "overfitting", "generalization", "out of distribution", "bert", "gender", "stereotypes", "distribution shift", "analysis", "performance", "bias", "correlation", "problems", "quality assurance"], "212": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "backpropagation", "computation", "autograph", "tensorflow", "pytorch", "torch", "autodiff", "differentiation", "backprop", "biologically plausible", "neurons", "error signal", "predictive coding", "variational", "gaussian", "iterative", "local updates", "distributed", "inner loop", "brain", "neuroscience", "deep neural networks", "analyzed", "hand drawing", "cnn", "rnn", "lstm", "convolutional neural network", "recurrent neural network", "hebian"], "213": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep neural networks", "neural networks gradient descent", "kernel machines", "kernel trick", "svm", "support vector machine", "sgd", "stochastic gradient descent", "machine learning theory", "pedro domingos", "linear regression", "nearest neighbor", "representations", "data representations", "representation learning", "proof", "math proof", "learning theory", "representer theorem"], "214": ["deep learning", "machine learning", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "openai emotions", "openai dalle", "openai clip", "openai microscope", "openai clip microscope", "alec radford", "emotion neuron", "deep learning emotion", "chris olah", "chris olah openai", "neural network feature visualization", "multimodal neural network", "what does a neural network learn", "what do neural networks learn", "how do neural networks work", "what does openai do", "faceted visualization"], "215": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "berkeley", "google brain", "facebook ai research", "pretrained transformers", "gpt-3", "huggingface", "language model", "fine-tuning", "finetuning", "out of domain generalization", "universal computation", "can transformers solve xor", "transformer mnist", "transformer cifar10", "fine tuning transformer", "gpt-2", "pretrained language model"], "216": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "implicit differentiation", "implicit function theorem", "imaml", "inner optimization", "inner optimization procedure", "how to backpropagate through sgd", "backpropagate through optimizer", "outer optimization loop", "bi-level optimization", "implicit graident", "gradient of optimizer", "dictionary learning", "dataset distillation", "google research", "what is deep learning", "deep learning tutorial"], "217": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "pondernet", "deepmind", "pondernet learning to ponder", "deepmind pondernet", "pondernet explained", "dynamic computation", "deep learning classic algorithms", "halting probability", "deep learning recurrent computation", "dynamic recurrent network", "broader impact", "deep network learning to stop"], "218": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "grokking", "openai", "double descent", "belkin", "overfitting", "bias variance", "steps", "training", "binary tables", "binary operations", "binary operation", "multiplication table", "algorithmic datasets", "groups", "s5 group", "deep learning algorithmic", "deep learning generalization", "generalization research", "why do neural networks generalize"], "219": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "backpropagation", "all you need", "gradients", "machine learning gradients", "differentiable environment", "differentiable physics", "differentiable simulation", "when to use gradients", "when not to use gradients", "when to avoid gradients", "google research", "google ai"], "220": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "grafting", "learning rate", "deep learning learning rate", "neural network learning rate", "adaptive learning rate", "adaptive optimizer", "learning rate grafting", "optimizer grafting", "adam", "sgd", "adagrad", "lars", "lamb", "openreview", "reviewer", "automatic learning rate", "learning rate decay", "learning rate warmup"], "221": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "imle", "implicit mle", "maximum likelihood", "backpropagation through algorithms", "deep learning discrete", "discrete deep learning", "discrete backpropagation", "gradient discrete", "gradient of an algorithm"], "222": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "research", "symbolic", "symbolic regression", "neuro symbolic computation", "integer sequences", "oeis", "number sequences", "ai number sequences", "machine learning sequences", "integer sequence rules", "embedding space", "transformers", "attention mechanism", "sequence generation", "learning number sequences", "predicting number sequences", "facebook ai", "meta ai", "beam search", "symbolic vs numeric"], "223": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "paper explained", "virtual outliers", "how to detect outliers", "deep learning outliers", "deep learning outlier detection", "vos", "deep learning energy", "latent space outliers", "density estimation", "classification boundaries", "generative models"], "224": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "paper explained", "virtual outliers", "how to detect outliers", "deep learning outliers", "deep learning outlier detection", "vos", "deep learning energy", "latent space outliers", "density estimation", "classification boundaries", "generative models"], "225": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "active dendrites", "neurons dendrites", "biological deep learning", "deep learning biology", "numenta", "numenta research", "numenta deep learning", "dendrites deep learning", "deep learning tutorial", "hierarchical temporal memory", "computational neuroscience", "reinforcement learning", "robotics", "multi task learning", "continuous learning", "continual learning", "permuted mnist"], "226": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "active dendrites", "neurons dendrites", "biological deep learning", "deep learning biology", "numenta", "numenta research", "numenta deep learning", "dendrites deep learning", "deep learning tutorial", "hierarchical temporal memory", "computational neuroscience", "reinforcement learning", "robotics", "multi task learning", "continuous learning", "continual learning", "permuted mnist"], "227": null, "228": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "jepa", "h-jepa", "yann lecun", "lecun", "agi", "artificial general intelligence", "openreview"], "229": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "neuralmagic", "neural magic", "deepsparse", "deep sparse", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "cpu vs gpu", "deep learning on cpu", "deep learning cpu vs gpu"], "230": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "deepmind alphatensor", "alpha tensor", "deepmind math", "google deep mind", "google deepmind", "matrix multiplication", "ai matrix multiplication", "matrix multiplication reinforcement learning", "alphazero", "alpha zero", "alphazero math", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "alphatensor explained", "alpha tensor explained"], "231": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "232": null, "233": ["gpt2", "transformer", "language model", "deep learning", "nlp", "openai", "security", "translation", "neural network", "attention", "attention mechanism", "unsupervised learning", "controversy"], "234": ["bert", "deep learning", "attention", "unsupervised", "nlp", "transformer", "squad", "wordpiece", "embeddings", "language", "language modeling", "attention layers", "bidirectional", "elmo", "natural language processing", "machine learning", "word vectors", "pretrained", "fine tuning"], "235": ["NIPS", "NeurIPS", "nips 2018", "neurips 2018", "nips name change", "machine learning", "deep learning", "community", "sexism", "diversity", "inclusion", "bias", "gender", "women", "tech", "women in tech", "women in stem", "majority vote", "minorities", "statistics", "computer science", "harassment"], "236": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "tensor2tensor", "rnn", "recurrent", "seq2seq"], "237": ["deep learning", "reinforcement learning", "deep mind", "academic", "paper", "research"], "238": ["machine learning", "deep learning", "adversarial examples", "adversarial samples", "pgd", "projected gradient descent", "vulnerabiliby", "security", "artificial intelligence", "MIT", "geometry", "classifier", "deep neural network", "attack", "convolutional neural networks", "research", "robust features", "robust classifier", "robust network", "neural network"], "239": ["deep learning", "machine learning", "artificial intelligence", "ai", "nlp", "natural language processing", "bert", "xlnet", "transformer", "transformer xl", "attention", "attention layer", "language model", "language modeling", "pretraining", "autoregressive", "autoencoder", "permutation", "google", "carnegie mellon", "cmu", "state of the art", "masked language model"], "240": ["deep learning", "neural networks", "adversarial examples", "machine learning", "bengio", "classification", "smooth", "flat representations", "ai", "artificial intelligence", "supervised learning", "regluarization", "regularizer", "hidden representations", "overconfidence"], "241": ["machine learning", "deep learning", "capsules", "capsule networks", "google brain", "hinton", "jeff hinton", "geoff hinton", "routing", "neural networks", "convolution", "convolutional neural networks", "deep neural networks", "cnns", "mnist", "multimnist", "disentanglement", "architecture", "reconstruction", "alternative", "dnn", "ml", "ai", "artificial intelligence", "brain", "visual system", "classifier", "image", "nonlinearity", "entities", "objects", "capsule", "network"], "242": ["deep learning", "memes", "meme review", "artificial intelligence", "review", "discussion", "reaction", "ai", "machine learning", "ml", "dnn", "gpu", "deep neural network", "ml memes", "deep learning memes", "machine learning memes", "funny", "gpus", "classifier", "hinton", "turing award", "bert", "xlnet", "optimization", "error rate", "culture", "community", "research"], "243": ["ml", "ai", "machine learning", "reinforcement learning", "deep rl", "deepmind", "google", "starcraft", "alphastar", "alphago", "alphazero", "value function", "policy", "vtrace", "upgo", "terran", "protoss", "zerg", "build order", "strategy", "pointer network", "transformer", "league training", "league", "battlenet", "artificial intelligence", "bot", "rl", "deep reinforcement learning", "model-free", "exploiters", "self-play", "ficticious self-play", "rts"], "244": ["ml", "ai", "machine learning", "reinforcement learning", "deep rl", "deepmind", "google", "alphago", "alphazero", "value function", "policy", "artificial intelligence", "rl", "deep reinforcement learning", "model-free", "model-based", "environment model", "hidden representation", "latent state", "transition", "chess", "shogi", "go", "atari"], "245": ["rl", "reinforcement learning", "ai", "artificial intelligence", "udrl", "schmidhuber", "policy", "value", "reward"], "246": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "memory", "lsh", "locality sensitive hashing", "reversible", "revertible", "flow", "long sequence"], "247": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "special"], "248": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "ebm", "energy function", "gradient descent", "relational neural network", "latent", "attention", "entities", "spatial relation", "inference time", "reasoning", "demonstration"], "249": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "facebook", "fair", "fb", "facebook ai", "object detection", "coco", "bounding boxes", "hungarian", "matching", "bipartite", "cnn", "transformer", "attention", "encoder", "decoder", "images", "vision", "pixels", "segmentation", "classes", "stuff", "things", "attention mechanism", "squared", "unrolled", "overlap", "threshold", "rcnn"], "250": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ml", "reading", "papers", "understanding", "quickly", "quick", "fast", "ultralearning", "research", "facebook", "detr", "object detection", "transformers", "how to"], "251": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "funny", "meme", "memes", "meme review", "gpt-3", "google", "deepmind", "haha", "deep neural networks", "christmas", "sunglasses", "transformers", "neurips", "gathertown", "pytorch", "tensorflow", "paddlepaddle", "review", "rebuttal", "proof", "theory", "analysis", "is all you need", "captcha", "stock market", "state of the art", "attention"], "252": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt", "gpt-3", "visual transformer", "transformer", "transformers", "attention mechanism", "vqvae", "vq vae", "vq-vae", "codebook", "relaxation", "gumbel", "text", "images", "nlp", "natural language processing", "autoregressive", "grid", "encoder", "decoder", "gpt3", "avocado chair", "porcupine sphere", "animations", "fisheye", "text to image", "image captioning", "openai", "sutskever", "dali", "dalle", "walle", "vector quantized", "hierarchical", "gan", "generative", "likelihood"], "253": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "funny", "meme", "memes", "meme review", "gpt-3", "google", "deepmind", "haha", "deep neural networks", "christmas", "sunglasses", "transformers", "neurips", "gathertown", "pytorch", "tensorflow", "paddlepaddle", "review", "rebuttal", "proof", "theory", "analysis", "is all you need", "captcha", "stock market", "state of the art", "attention"], "254": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "geoff hinton", "geoff hinton capsule networks", "geoff hinton neural networks", "geoffrey hinton", "geoffrey hinton deep learning", "geoffrey hinton glom", "hinton glom", "glom model", "deep learning tutorial", "introduction to deep learning", "capsule networks", "computer vision", "capsule networks explained", "google brain", "google ai", "schmidhuber", "transformer", "attention mechanism", "consensus algorithm", "column"], "255": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "deep learning fails", "deep learning failures", "openai clip", "openai clip paper", "openai clip adversarial", "clip adversarial", "adversarial attack", "apple ipod", "adversarial textural attack", "language model", "gpt-3", "dall-e model", "shorts", "yannic kilcher", "experiment reproduce", "adversarial attacks"], "256": ["machine learning phd", "how to do a phd in machine learning", "phd advice", "machine learning phd thesis topics", "machine learning phd topics", "how to machine learning phd", "how to select a thesis topic", "how to machine learning conferences", "how to write a machine learning paper", "advice for phd students", "advice for new phd students", "how to survive a phd", "what to do in a machine learning phd", "deep learning phd advice", "machine learning phd thesis", "machine learning phd thesis topic"], "257": ["deep learning", "machine learning", "neural networks", "ai", "artificial intelligence", "minecraft", "neural networks explained", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "deep learning in minecraft", "minecraft machine learning", "redstone neural network", "minecraft redstone neural network", "gaming neural network", "neural network explained", "machine learning in minecraft", "vanilla minecraft computer", "minecraft vanilla redstone computer", "minecraft backpropagation"], "258": ["deep learning", "machine learning", "neural networks", "artificial intelligence", "deep learning tutorial", "introduction to deep learning", "cooking by ai", "can ai cook", "ai recipe", "ai recipe generator", "gpt 3", "gpt 3 recipe", "gpt-3", "gpt-3 recipe", "can gpt-3 cook", "can gpt-3 generate recipes", "can ai generate recipes", "ai kitchen", "ai in the kichen", "yannic gpt-3", "kilcher cooking", "gpt-3 cooking", "ai generated recipe", "language model recipe", "can ai be creative", "machine learning recipe"], "259": ["deep learning", "machine learning", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "ai generated music video", "ai music video", "deep learning music video", "ai music video generator", "music video generator", "openai clip", "openai clip music video", "biggan music video", "clip biggan", "biggan clip", "stylegan clip", "imagenet song", "imagenet classes lyrics", "stylegan music", "gan interpolation", "be my weasel"], "260": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "sbb", "cff", "sncf", "swiss train", "swiss train system", "intercity train", "intercity 1", "durchmesserlinie", "geneva", "lausanne", "bern", "zurich", "st gallen", "train seat", "2nd class", "switzerland train", "schwerizerische bundesbahnen", "seat review", "train seat review", "travel review", "train travel", "travel switzerland"], "261": null, "262": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "wandb", "huggingface", "hugging face", "is hugging face dangerous", "is ai dangerous", "ai exploit", "pickle exploit", "pytorch exploit", "is hugging face safe", "reduce", "python pickle", "python pickletools", "python pickle exploit", "pytorch pickle exploit", "ai model backdoor", "arbitrary code execution", "pickle code injection", "pytorch danger", "pytorch load danger", "is pytorch safe", "is pytorch dangerous"], "263": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "deepmind alphatensor", "alpha tensor", "deepmind math", "google deep mind", "google deepmind", "matrix multiplication", "ai matrix multiplication", "matrix multiplication reinforcement learning", "alphazero", "alpha zero", "alphazero math", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "alphatensor explained", "alpha tensor explained"], "264": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chatgpt", "chat gpt", "openai chat gpt", "openai chatbot gpt", "openai chatbot", "gpt-3 chatbot", "gpt-4", "gpt 3 chatbot", "ml news", "mlnews", "ai news", "what is deep learning", "deep learning tutorial", "chatgpt jailbreak"], "265": ["gpt-4", "gpt 4", "gpt 4chan", "gpt-4chan", "gpt4chan", "gpt4", "4chan ai", "chatgpt 4chan", "chatgpt biased", "4chan language model", "llm 4chan", "seychelles", "seychelles anon", "seychelle anon", "chat gpt 4", "gpt-j", "artificial intelligence", "ai", "machine learning"], "266": null, "267": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chatgpt", "chat gpt", "openai chat gpt", "chat gpt alternative", "chat gpt alternative open source", "chatgpt alternative", "open source chatgpt", "open source language model", "chatgpt github", "openassistant", "open assistant", "open-assistant", "open assistant laion", "open assistant yannic", "chatgpt free", "chat gpt free", "openai", "instructgpt"], "268": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chatgpt", "open source chatgpt", "open source chatbot alternatives", "chatgpt open source", "llama", "open assistant", "open assistent", "openassistant", "open-assistant", "laion", "laion 5b", "rlhf"], "269": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "what is deep learning", "open source ai", "open source chatgpt", "laion", "laion chatgpt", "yannic chatgpt", "open source intelligence", "open source gpt", "open source gpt 4", "dolly", "alpaca", "llama", "pythia", "vincuna", "huggingface", "streaming"], "270": null, "271": null, "272": ["machine learning", "deep learning", "neural networks", "batch normalization", "batchnorm", "whitening", "data", "internal covariate shift", "deep neural networks", "deep nets", "mini-batch", "training"], "273": ["NeurIPS2018", "NIPS2018", "NLP", "deep learning", "RNN"], "274": ["ai", "deep learning", "variational", "autoencoders", "vae", "disentanglement", "representation learning", "machine learning", "unsupervised", "arxiv", "google", "google ai", "mpi", "eth", "eth zurich", "ethz"], "275": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "tensor2tensor", "rnn", "recurrent", "seq2seq"], "276": ["deep learning", "neural networks", "adversarial examples", "machine learning", "bengio", "classification", "smooth", "flat representations", "ai", "artificial intelligence", "supervised learning", "regluarization", "regularizer", "hidden representations", "overconfidence"], "277": ["machine learning", "deep learning", "research", "attention", "attention sampling", "attention model", "attention distribution", "megapixel images", "large images", "artificial intelligence", "megapixel mnist", "street sign dataset", "monte carlo", "speed", "memory", "cnn", "convolutional neural networks", "limited resources", "ai", "image recognition", "image classifier"], "278": ["machine learning", "deep learning", "artificial intelligence", "ai", "data science", "convolution", "convolutional neural networks", "cnn", "manifolds", "curvature", "parallel transport", "gauge", "gauge transformation", "icosahedron", "weight sharing", "coordinate frame", "invariant", "coordinate system", "equivariance", "sphere", "spherical"], "279": ["machine learning", "deep learning", "capsules", "capsule networks", "google brain", "hinton", "jeff hinton", "geoff hinton", "routing", "neural networks", "convolution", "convolutional neural networks", "deep neural networks", "cnns", "mnist", "multimnist", "disentanglement", "architecture", "reconstruction", "alternative", "dnn", "ml", "ai", "artificial intelligence", "brain", "visual system", "classifier", "image", "nonlinearity", "entities", "objects", "capsule", "network"], "280": ["ml", "ai", "machine learning", "artificial ingelligence", "gan", "generative", "image processing", "deep learning", "image editing", "deep dream", "style transfer", "convolutional neural networks", "generative adversarial networks", "photoshop"], "281": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "google", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "memory", "lsh", "locality sensitive hashing", "reversible", "revertible", "flow", "long sequence"], "282": ["machine learning", "deep learning", "cellular automata", "game of life", "conway", "google", "distill", "interactive", "colab", "local", "global", "update"], "283": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "attention mechanism", "attention", "transformer", "rnn", "recurrent", "seq2seq", "facebook", "fair", "research", "math", "integral", "ode"], "284": ["deep learning", "machine learning", "arxiv", "google", "attention mechanism", "attention", "transformer", "rnn", "recurrent", "weather", "long-range", "layers", "convolutions", "cnns", "rain", "physics"], "285": ["deep learning", "machine learning", "arxiv", "evolution", "reinforcement learning", "neat", "open-ended", "never ending", "population", "bipedal walker"], "286": ["deep learning", "machine learning", "cnn", "resnet", "residual", "efficientnet", "mobilenet", "cifar10", "imagenet", "batch normalization", "batchnorm", "relu", "sigmoid", "evolution", "architecture", "transfer", "image classification", "supervised learning", "population", "activation", "normalization", "google", "deepmind"], "287": ["deep learning", "machine learning", "neural networks", "pruning", "distillation", "quantization", "size", "weights", "optimization", "training", "generalization", "overparameterization", "winning ticket", "winning lottery ticket", "arxiv"], "288": ["deep learning", "machine learning", "arxiv", "google", "semi-supervised", "unlabeled", "augmentation", "research", "randaugment"], "289": ["deep learning", "machine learning", "imagenet", "visualization", "features", "intermediate", "hidden layers", "activations", "patterns", "openai", "google", "interactive", "explanation"], "290": ["deep learning", "machine learning", "nlp", "natural language processing", "machine translation", "arxiv", "attention mechanism", "attention", "transformer", "bert", "roberta", "mlm", "convolution", "memory", "linear", "sliding", "dilated", "sparse"], "291": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "music", "vae", "vq-vae", "latent codes", "quantization", "sound", "lyrics", "sinatra", "kanye", "transformer", "openai"], "292": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "brain", "cnn", "convolutional neural network", "resnet", "residual network", "pretraining", "finetuning", "vtab", "imagenet", "cifar", "state of the art", "pretrained", "computer vision"], "293": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "batchnorm", "groupnorm", "layer norm", "group norm", "batch norm", "instance norm", "fair", "normalization", "mean", "standard deviation", "minibatch", "batch statistics", "kernel", "cnn", "convolutional neural network"], "294": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "normalize", "batchnorm", "groupnorm", "layernorm", "mean", "center", "std", "standardize", "backpropagation", "convergence", "gradients", "norm", "convolution", "cnn", "convolutional neural networks", "filters", "kernel", "channel", "architecture"], "295": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "facebook", "fair", "fb", "facebook ai", "object detection", "coco", "bounding boxes", "hungarian", "matching", "bipartite", "cnn", "transformer", "attention", "encoder", "decoder", "images", "vision", "pixels", "segmentation", "classes", "stuff", "things", "attention mechanism", "squared", "unrolled", "overlap", "threshold", "rcnn"], "296": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "machine translation", "google", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "memory", "lsh", "locality sensitive hashing", "reversible", "revertible", "flow", "long sequence"], "297": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ethz", "clustering", "self-supervision", "self-labeling", "entropy", "dot product", "representation learning", "cnns", "convolutional neural network", "deep cluster", "nce", "noise contrastive estimation", "unsupervised", "overcluster", "imagenet", "cifar10", "nearest neighbors"], "298": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nas", "nao", "uber", "openai", "architecture search", "neural architecture search", "inner loop", "inner optimization", "small", "abstract", "turing", "performance", "evolutionary algorithm", "outer loop", "mlp", "sigmoid", "ptb", "rnn", "cell", "meta-learning"], "299": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "tts", "text-to-speech", "aligner", "convolutions", "spectrogram", "mel", "alignment", "phonemes", "deepmind", "deep mind", "dynamic time warping", "gaussian kernel", "adversarial", "gan", "discriminator", "tokens", "sound wave", "speech"], "300": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "facebook", "linear", "quadratic", "transformer", "attention", "self-attention", "multi-head attention", "t2t", "vasvani", "bert", "devlin", "roberta", "glue", "language modeling", "perplexity", "dot product", "johnson", "lindenstrauss", "random projection"], "301": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cnn", "visual", "resnet", "caption", "nlp", "transformer", "vasvani", "attention", "text", "coco", "imagenet", "convolutional neural network", "adaptation", "transfer learning", "quality", "unsupervised", "self-supervised"], "302": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "initialization", "lottery ticket hypothesis", "pruning", "training", "magnitude", "snip", "grasp", "init", "xavier", "glorot", "he", "flow", "layer collapse", "iterative", "recompute", "stepwise", "memory", "fast", "prune", "weights", "feedforward", "layer", "neural network"], "303": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gru", "lstm", "schmidhuber", "bistable", "bistability", "neurons", "biological", "spiking", "tanh", "stable", "attractor", "fixed points", "memory", "memorize", "sparse", "long sequence", "history", "storage", "remember", "rnn", "recurrent neural network", "gated recurrent unit", "forget", "backpropagation", "biologically inspired"], "304": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "image translation", "style transfer", "unsupervised", "clustering", "self-supervised", "cnn", "convolutional neural networks", "gan", "generative adversarial network", "generator", "encoder", "discriminator", "conditional", "style", "pseudo-label", "augmentation", "cropping"], "305": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "implicit", "nerf", "neural processes", "optimization", "curve fitting", "audio", "signal processing", "surfaces", "point clouds", "oriented", "signed distance function", "mlp", "layers", "hypernetworks", "representation", "function", "sin", "sinus", "sinusoid", "fourier", "initialization", "relu", "nonlinearity", "derivative", "gradient", "laplacian", "wave"], "306": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "counting", "self-similarity", "temporal", "frames", "video", "repeating", "lines", "transformer", "attention", "cnn", "convolutional neural network", "repetitions", "periodicity", "period", "repeat", "actions", "kinetics", "countix"], "307": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "graph networks", "graph neural networks", "gnn", "physics", "newtonian", "hamiltonian", "dynamics", "cosmology", "dark matter", "symbolic regression", "edge", "vertex", "regularization"], "308": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gnn", "transformer", "graph", "biology", "neurons", "axon", "dendrites", "plausible", "biologically plausible", "backprop", "backpropagation", "dfa", "feedback alignment", "random projections"], "309": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "cnn", "convolutional neural network", "coco", "object detection", "region of interest", "rcnn", "r-cnn", "attention", "attention mechanism", "google", "caltech", "gazelle", "wildlife", "wild trap", "traffic", "object", "car", "bus", "vehicle", "lighting", "time", "sampling", "frames", "memory", "long-term", "query"], "310": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "sets", "images", "cnn", "convolutional neural network", "gan", "generator", "encoder", "discriminator", "prior", "mean", "made", "latent", "binary", "conditional", "noise", "distribution", "probability", "energy-based", "energy", "apple", "research", "sdn", "variational", "elbo"], "311": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "ethz", "vision", "objects", "slots", "attention mechanism", "gru", "lstm", "routing", "capsules", "permutation invariant", "encoder", "set", "detr", "embeddings", "transformer", "weight sharing", "disentanglement", "render", "tetris", "clevr", "cnn", "convolutional neural network", "attention"], "312": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "billion", "parameters", "float32", "attention mechanism", "transformer", "scale", "gpt-3", "google", "gshard", "xla", "sharding", "parallelism", "mixture of experts", "trillion", "tpus", "distributed", "m4", "multilingual translation", "natural language processing"], "313": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "recognition", "localization", "resnet", "resnet50", "fpn", "backbone", "permuation", "upsampling", "stride", "convolution", "convolutional neural network", "google", "spine", "spine net", "imagenet", "coco", "segmentation", "bounding box", "skip connections", "residual", "bottleneck"], "314": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "attention", "attention mechanism", "linear", "linear transformer", "linformer", "reformer", "idiap", "epfl", "queries", "keys", "softmax", "kernel", "routing", "inner product", "rnn", "recurrent neural network", "transformer", "bert", "autoregressive", "dimensions", "topic modeling", "language model"], "315": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "supsup", "supermasks", "lottery ticket", "lottery ticket hypothesis", "gradient", "entropy", "surplus", "superfluous neurons", "lifelong learning", "multitask learning", "catastrophic forgetting", "continuous learning", "binary mask", "random network", "optimization", "hopfield network", "gradient descent", "superposition"], "316": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "supsup", "supermasks", "lottery ticket", "lottery ticket hypothesis", "gradient", "entropy", "surplus", "superfluous neurons", "lifelong learning", "multitask learning", "catastrophic forgetting", "continuous learning", "binary mask", "random network", "optimization", "hopfield network", "gradient descent", "superposition"], "317": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gan", "vae", "kl", "elbo", "autoencoder", "variational", "latent", "sampling", "hierarchical", "scales", "faces", "mnist", "cifar10", "swish", "batch norm", "generative", "nvidia", "mixed precision", "memory", "deep", "layers", "depthwise convolutions", "cnn", "convolutional", "generation", "generative model"], "318": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gon", "gradient", "negative gradient", "implicit", "implicit representation", "siren", "sirens", "deep neural networks", "convolutional neural network", "dnns", "mnist", "cifar10", "fashion mnist", "gradient descent", "sgd", "inner loop", "backpropagation", "live code", "code", "machine learning code", "research", "research paper"], "319": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "classic", "alexnet", "hinton", "geoff hinton", "imagenet", "convolution", "convolutional neural network", "architecture", "dropout", "data augmentation", "cnns", "computer vision", "image classification", "object recognition", "classifier", "max pool", "pretraining", "deep neural networks"], "320": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gan", "generator", "discriminator", "convolution", "deconvolution", "goodfellow", "bengio", "convolutional neural network", "mnist", "cifar10", "generative", "generative model", "image generation", "face model", "latent space", "interpolation", "minmax", "nash equilibrium", "game theory"], "321": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "computer vision", "kaiming he", "google", "resnet", "resnet50", "resnet151", "deep neural network", "imagenet", "residual", "identity function", "very deep", "convolutional neural network", "bottleneck", "overfitting"], "322": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nas", "nas-bench", "architecture search", "initialization", "untrained", "cifar10", "imagenet", "neural architecture search", "controller", "rnn", "correlation", "gradient", "jacobian", "linearization"], "323": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "google research", "bigbird", "big bird", "bert", "attention", "attention is all you need", "longformer", "random attention", "quadratic attention", "attention mechanism", "qa", "natural questions", "hotpot qa", "genomics", "nlp", "natural language processing", "transformer", "transformers", "fully connected", "sparse attention", "graph", "star graph", "turing complete", "universal approximation", "window attention", "convolution"], "324": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "schmidhuber", "hochreiter", "lstm", "gru", "rnn", "hopfield", "attention", "attention is all you need", "transformer", "bert", "query", "key", "value", "routing", "pattern", "retrieval", "store", "error", "exponental", "binary", "continuous", "hopfield network", "lse", "energy function", "update rule", "metastable", "separation"], "325": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "cnn", "resnet", "big bird", "bigbird", "attention", "attention mechanism", "attention for images", "transformer for images", "transformer", "bert", "convolutions", "window", "neighbors", "axial attention", "position embeddings", "positional encodings", "quadratic", "memory", "panoptic segmentation", "coco", "imagenet", "cityscapes", "softmax", "routing"], "326": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention mechanism", "convolutional neural network", "data science", "cnn", "transformer", "attention is all you need", "vaswani", "beyer", "google", "google brain", "google research", "tpu", "tpu v3", "iclr", "iclr 2021", "peer review", "anonymous", "karpathy", "andrej karpathy", "twitter", "review", "under submission", "big transfer", "bit", "vit", "vision transformer", "visual transformer", "transformer images", "transformer computer vision"], "327": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention", "attention mechanism", "lambda", "lambdaresnet", "residual networks", "local attention", "quadratic", "memory", "transformer", "transformers", "keys", "values", "queries", "architecture", "input size", "iclr", "lambdanet", "lambdanets", "lambdaresnets", "efficientnet", "tradeoff", "routing", "linear function", "functional programming"], "328": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "natural language understanding", "data science", "transformer", "attention", "attention mechanism", "transformers", "attention is all you need", "gpus", "tpu", "linformer", "reformer", "explanation", "imagenet64", "kernels", "gaussian kernel", "softmax", "softmax kernel", "approximation", "random features", "random positive features", "random fourier features", "google", "favor", "machine translation"], "329": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "berkeley", "purdue", "mc hammer", "mchammer", "mit", "technology review", "pde", "partial differential equation", "navier stokes", "darcy flow", "burgers", "convolutions", "fft", "dfft", "fourier transform", "fourier neural operator", "neural operator", "fast fourier transform", "fourier modes", "flow", "turbulent flow", "fluid dynamics", "residual", "aerodynamics", "wind tunnel", "neural network", "layers", "numerical", "discretization"], "330": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention", "transformer", "attention mechanism", "google", "google brain", "shazeer", "trillion", "trillion parameter", "language model", "gpt3", "gpt-3", "gpt 3", "t5", "sharding", "mesh", "mtf", "mesh tensorflow", "query", "key", "value", "feed forward", "experts", "routing", "mixture of experts", "sparse", "sparse experts", "data parallelism", "model parallelism", "expert parallelism", "trillion parameters", "perplexity", "scaling", "flops", "bfloat16"], "331": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformer", "rnn", "lstm", "seq2seq", "gpt3", "gpt-3", "nlp", "natural language processing", "language modelling", "feedback transformers", "memory", "attention", "attention mechanism", "attention is all you need", "facebook ai", "fair", "long range", "complex", "reasoning", "bert", "autoregressive", "reinforcement learning", "abstraction", "representation", "higher layers", "attention matrix", "recurrent neural networks"], "332": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "machine learning tutorial", "machine learning explained", "batch normalization", "jax", "layer normalization", "gradient clipping", "weight standardization", "normalizer-free", "nfnets", "nfnet", "nfresnet", "deepmind", "deep mind", "best neural network", "imagenet", "best imagenet model", "distributed training", "mean shift", "batch norm", "batchnorm", "nfnets code", "deep learning code", "ml code"], "333": ["deep learning", "machine learning", "arxiv", "neural networks", "ai", "artificial intelligence", "attention neural networks", "attention is all you need", "transformer gan", "transformer gans", "transformer generative adversarial network", "generative adversarial network", "attention mechanism", "self attention", "vision transformer", "pixelshuffle", "superresolution", "local attention", "multihead attention", "transformer generator", "google", "machine learning explained", "deep learning explained", "paper explained", "transgan"], "334": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "fast weights", "fast weights hinton", "fast weights neural network", "schmidhuber", "j\u00fcrgen schmidhuber", "juergen schmidhuber", "lstm transformer", "performers", "transformer performer", "linear transformer", "linear attention", "linear attention transformer", "autoregressive model", "autoregressive transformer", "transformer kernel", "kernels transformer", "favor performer", "favor algorithm", "deep learning tutorial"], "335": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "geoff hinton", "geoff hinton capsule networks", "geoff hinton neural networks", "geoffrey hinton", "geoffrey hinton deep learning", "geoffrey hinton glom", "hinton glom", "glom model", "deep learning tutorial", "introduction to deep learning", "capsule networks", "computer vision", "capsule networks explained", "google brain", "google ai", "schmidhuber", "transformer", "attention mechanism", "consensus algorithm", "column"], "336": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "deepmind", "perceiver", "cross attention", "attention mechanism", "attention is all you need", "google deepmind", "deepmind perceiver", "perceiver model", "perciever model", "perciever", "self attention", "rnn", "recurrent neural network", "weight sharing", "computer vision", "natural language processing", "fourier features"], "337": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "google mixer", "google ai mixer", "vit", "bit", "mlp mixer", "mlpmixer", "imagenet mixer", "imagenet only feedforward", "no convolutions", "imagenet without convolutions", "image patches", "attention mechanism", "multilayer perceptron", "transfer learning", "linear classifier", "state of the art", "tradeoff"], "338": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "computer vision", "convolutional neural network", "convolutions alternative", "cnn attention", "self attention", "attention mechanism for vision", "weight sharing neural networks", "convolutions vision", "cnn vision", "involution vision", "image segmentation", "rednet", "resnet", "residual neural networks", "bytedance ai"], "339": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "expire span", "facebook ai", "transformers", "long sequence models", "transformers long sequence", "large context language models", "language model sequence length", "transformer xl", "learning to forget", "lstm", "schmidhuber", "learning to remember", "not all memories are created equal", "linear attention", "attention mechanism", "linear attention mechanism", "transformer memory", "deep learning tutorial"], "340": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention mechanism", "attention is all you need", "fastformer", "fast former", "nlp", "natural language processing", "linear attention", "linear transformer", "query key value", "additive attention", "elementwise product", "fast transformer", "faster transformer", "transformer memory", "attention quadratic memory", "fastformer explained"], "341": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "diffusion models", "autoregressive models", "generative models", "nlp", "natural language processing", "gpt", "image-gpt", "gpt-3", "gpt-2", "order agnostic", "order agnostic diffusion", "generative diffusion models", "bert", "autoregressive bert", "bert text generation", "character level language model", "upscaling", "dynamic programming", "pixelwise sampling"], "342": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "terraformer", "scaling transformers", "nli", "nlp", "natural language processing", "transformers memory", "deep learning memory", "fast transformer", "fast transformers", "attention", "attention mechanism", "attention is all you need", "bert", "gpt-3", "google research", "reversible layers", "reformer", "sparse attention", "sparse feedforward", "low-rank"], "343": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "clearml", "nuwa", "n\u00fcwa", "visual pretraining", "pretraining vision models", "igpt", "image gpt", "autoregressive", "autoregressive image gpt", "autoregressive image generation", "nearby self-attention", "3dna", "3d nearby self-attention", "transformer", "transformer for videos", "deep learning on videos", "deep learning video generation", "video manipulation", "text to image", "text to video", "microsoft"], "344": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "noether networks", "noether's theroem", "noether theorem", "symmetries", "neural network bias", "neural network symmetries", "inductive biases", "conserved quantities", "pendulum", "neural network physics", "deep learning physics", "deep learning symmetries", "group convolutions", "with the authors", "paper explained", "deep learning prediction", "test time optimization", "tailoring", "neural network tailoring"], "345": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "neural interpreters", "dynamic inference", "neural programming", "neural functions", "recurrent networks", "yoshua bengio", "mila", "schoelkopf", "attention", "modlin", "modulated linear layer", "weight sharing", "recurrent modules", "function modules", "sparse neural networks", "interview", "first author interview", "with the authors", "dynamic inference with neural interpreters", "deep neural interpreters"], "346": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "metalearning", "meta learning", "neural network", "unsupervised learning", "few shot learning", "google", "google research", "google ai", "transformer", "meta transformer", "hypertransformer", "hyper transformer", "generate the weights of a neural network", "privacy", "personalization", "interview", "paper explained", "semi-supervised learning"], "347": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cm3", "facebook ai", "fair", "meta ai", "language model", "language modelling", "gpt-3", "gpt 3", "gpt3", "dall-e", "ru-dalle", "text to image", "ai image generation", "ai internet", "language model html", "transformer html", "large language models", "transformer", "autoregressive", "causal masking", "causally masked language model", "bidirectional", "bert", "masked language modelling"], "348": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "zeta alpha", "blip", "language vision pre training", "language vision pre-training", "deep learning pre-training", "clip pre-training", "blip pretraining", "parameter sharing", "sequence to sequence", "image captioning", "vqa", "visual question answering", "fine-tuning", "vit", "vision transformer", "salesforce"], "349": ["Pewdiepie", "MrBeast", "Markiplier", "Jackscepticeye", "Turkey Tom", "Keemstar", "r/pewdiepie submissions", "pewdiepie obese", "FuturisticHub: The World's Worst YouTuber"], "350": null, "351": null, "352": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt4", "gpt 4", "recurrent neural network", "rwkv", "rvkw", "transformer alternative", "gpt-4", "deep learning tutorial", "what is deep learning"], "353": ["deep learning", "machine learning", "coding", "research", "engineering", "ipython", "colab", "notebook", "locals"], "354": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "code", "pytorch", "bert", "pretrained", "lightning", "live", "tutorial", "pip", "nlp", "transformers", "tokenizers", "sequence", "sentiment", "imdb", "dataset", "full", "github"], "355": ["deep learning", "machine learning", "imagenet", "cifar10", "cifar10.1", "generalization", "overfitting", "mturk", "arxiv", "vision", "models", "research", "hardness", "accuracy", "classifier", "resnet"], "356": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "music", "vae", "vq-vae", "latent codes", "quantization", "sound", "lyrics", "sinatra", "kanye", "transformer", "openai"], "357": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "bert", "nlp", "natural language processing", "wikitables", "sql", "tabular", "aggregations", "structured", "google"], "358": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "sac", "ppo", "deep rl", "deep reinforcement learning", "dreamer", "curl", "pixel", "pretraining", "deepmind", "openai", "berkeley"], "359": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "deep rl", "planning", "alphago", "alphazero", "alpha go", "alpha zero", "mcts", "monte carlo", "tree search", "subdivision", "recursive", "training data", "hindsight experience replay"], "360": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "brain", "cnn", "convolutional neural network", "resnet", "residual network", "pretraining", "finetuning", "vtab", "imagenet", "cifar", "state of the art", "pretrained", "computer vision"], "361": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "ebm", "energy function", "gradient descent", "relational neural network", "latent", "attention", "entities", "spatial relation", "inference time", "reasoning", "demonstration"], "362": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "batchnorm", "groupnorm", "layer norm", "group norm", "batch norm", "instance norm", "fair", "normalization", "mean", "standard deviation", "minibatch", "batch statistics", "kernel", "cnn", "convolutional neural network"], "363": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "brain", "pipeline", "bottleneck", "speed", "gpu", "tpu", "idle", "network", "distributed", "preprocessing", "augmentation"], "364": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "normalize", "batchnorm", "groupnorm", "layernorm", "mean", "center", "std", "standardize", "backpropagation", "convergence", "gradients", "norm", "convolution", "cnn", "convolutional neural networks", "filters", "kernel", "channel", "architecture"], "365": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "investigation", "linear probes", "usefulness", "representations", "intermediate", "hidden layers", "self-supervised", "rotnet", "crop", "augmentation", "color jitter", "dataset"], "366": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "367": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "classifier", "dnn", "cnn", "high dimensions", "class boundaries", "mixing", "interpolation", "latent", "beta", "regularizer", "regularization", "generalization", "adversarial examples", "smooth"], "368": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "deep rl", "deep reinforcement learning", "novelty", "curiosity", "intrinsic reward", "dreamer", "planet", "control", "walker", "run forward", "imaginary", "imagination", "planning", "google", "neural network", "actor", "critic", "uncertainty", "information gain", "mutual information", "model"], "369": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "fmri", "mind reading", "thoughts", "visual cortex", "vc", "v1", "v4", "vgg", "reconstruction", "iterative", "deep dream", "microscope", "activity", "imagine", "visualize", "introspection", "human", "telepathy"], "370": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "deep rl", "human", "prior", "objects", "game", "video game", "key", "visuals", "enemy", "ladder", "gravity", "ablation"], "371": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "bert", "nlp", "lottery ticket", "good", "bad", "winning", "pruning", "weights", "attention", "transformer", "heads", "multi-head", "fine-tuning", "glue", "benchmark"], "372": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "model predictive control", "dae", "denoising autoencoders", "trajectory", "trajectory optimization", "planning", "adversarial attack", "errors", "open loop", "closed loop", "joint", "probability", "derivative", "gaussian", "experience", "learned model", "world model", "model predictive", "mpc"], "373": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "facebook", "fair", "fb", "facebook ai", "object detection", "coco", "bounding boxes", "hungarian", "matching", "bipartite", "cnn", "transformer", "attention", "encoder", "decoder", "images", "vision", "pixels", "segmentation", "classes", "stuff", "things", "attention mechanism", "squared", "unrolled", "overlap", "threshold", "rcnn"], "374": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformers", "attention", "nlp", "natural language processing", "gpt3", "gpt-3", "gpt2", "gpt-2", "openai", "language model", "mlm", "autoregressive", "heads", "bert", "turing", "microsoft", "question answering", "news", "glue", "superglue", "sota", "preplexity", "corpus", "common crawl", "wikipedia", "natural questions", "boolq", "math", "strings", "context", "deep language", "zero shot", "few shot", "training data"], "375": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "machine translation", "google", "attention mechanism", "attention", "transformer", "seq2seq", "bert", "memory", "lsh", "locality sensitive hashing", "reversible", "revertible", "flow", "long sequence"], "376": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "deep rl", "control", "planning", "world model", "dads", "skills", "latent", "high level", "unsupervised", "tree search", "deep reinforcement learning", "mujoco", "ant", "google"], "377": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "378": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ethz", "clustering", "self-supervision", "self-labeling", "entropy", "dot product", "representation learning", "cnns", "convolutional neural network", "deep cluster", "nce", "noise contrastive estimation", "unsupervised", "overcluster", "imagenet", "cifar10", "nearest neighbors"], "379": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "corner", "top left", "bottom right", "corners", "cv", "computer vision", "vision", "object detection", "detr", "bounding box", "center", "anchor", "pooling", "local", "cnn", "convolutions", "convolutional neural network", "hourglass", "skip connection", "heatmap", "embedding", "push", "pull", "loss", "overlap", "filters", "channels"], "380": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nas", "nao", "uber", "openai", "architecture search", "neural architecture search", "inner loop", "inner optimization", "small", "abstract", "turing", "performance", "evolutionary algorithm", "outer loop", "mlp", "sigmoid", "ptb", "rnn", "cell", "meta-learning"], "381": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "mt", "machine translation", "transformer", "bert", "lstm", "attention", "wmt", "wikipedia", "backtranslation", "bleu", "rouge", "ngrams", "score", "metric", "comparison", "human raters", "google", "google research", "automatic", "overlap", "distribution shift"], "382": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "383": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "tts", "text-to-speech", "aligner", "convolutions", "spectrogram", "mel", "alignment", "phonemes", "deepmind", "deep mind", "dynamic time warping", "gaussian kernel", "adversarial", "gan", "discriminator", "tokens", "sound wave", "speech"], "384": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "facebook", "linear", "quadratic", "transformer", "attention", "self-attention", "multi-head attention", "t2t", "vasvani", "bert", "devlin", "roberta", "glue", "language modeling", "perplexity", "dot product", "johnson", "lindenstrauss", "random projection"], "385": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cnn", "visual", "resnet", "caption", "nlp", "transformer", "vasvani", "attention", "text", "coco", "imagenet", "convolutional neural network", "adaptation", "transfer learning", "quality", "unsupervised", "self-supervised"], "386": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "387": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "388": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "chollet", "keras", "google", "francois", "intelligence", "iq", "iq test", "deep neural networks", "prior", "skill", "performance", "measurement", "measure", "test", "number", "intelligent", "smart", "learning", "generalization", "ability", "experience", "humans", "evolution", "nature", "nurture", "psychometrics", "range", "adaptability", "arc", "kaggle", "difficulty", "entropy", "core knowledge", "objectness", "navigation", "contact", "agent", "goal"], "389": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "reinforcement learning", "level design", "game design", "video game", "sobokan", "sokoban", "zelda", "maze", "agent", "turtle", "observation", "reward", "action", "space", "deep rl", "deep reinforcement learning", "content", "minecraft"], "390": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "math", "derivative", "ode", "pde", "solution", "integral", "gradient", "jacobian", "mathematics", "language model", "transformer", "symbolic", "numeric", "stability", "equilibrium", "attention", "tokens", "dataset", "abstract"], "391": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "initialization", "lottery ticket hypothesis", "pruning", "training", "magnitude", "snip", "grasp", "init", "xavier", "glorot", "he", "flow", "layer collapse", "iterative", "recompute", "stepwise", "memory", "fast", "prune", "weights", "feedforward", "layer", "neural network"], "392": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gru", "lstm", "schmidhuber", "bistable", "bistability", "neurons", "biological", "spiking", "tanh", "stable", "attractor", "fixed points", "memory", "memorize", "sparse", "long sequence", "history", "storage", "remember", "rnn", "recurrent neural network", "gated recurrent unit", "forget", "backpropagation", "biologically inspired"], "393": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "image translation", "style transfer", "unsupervised", "clustering", "self-supervised", "cnn", "convolutional neural networks", "gan", "generative adversarial network", "generator", "encoder", "discriminator", "conditional", "style", "pseudo-label", "augmentation", "cropping"], "394": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "ucl", "representation", "moco", "momentum contrast", "simclr", "encoder", "augmentation", "mixup", "randaugment", "crop", "random crop", "jitter", "flip", "unsupervised", "self-supervised", "cnn", "resnet", "latent", "contrastive", "online", "target", "exponential moving average", "negatives"], "395": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "gpt2", "gpt3", "bert", "transformer", "attention is all you need", "attention mechanism", "multi-head attention", "pixel rnn", "pixel cnn", "pretraining", "representation", "linear probe", "fine-tuning", "cifar10", "cifar100", "imagenet", "cnn", "convolutional neural network", "autoregressive"], "396": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cnn", "resnet", "simclr", "simclr2", "simclrv2", "simclr v2", "v2", "hinton", "geoff", "brain", "wide", "deep", "convolutional", "convolutions", "self-supervised", "contrastive", "moco", "momentum", "projection", "semi-supervised", "unsupervised", "distillation", "teacher", "student"], "397": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "implicit", "nerf", "neural processes", "optimization", "curve fitting", "audio", "signal processing", "surfaces", "point clouds", "oriented", "signed distance function", "mlp", "layers", "hypernetworks", "representation", "function", "sin", "sinus", "sinusoid", "fourier", "initialization", "relu", "nonlinearity", "derivative", "gradient", "laplacian", "wave"], "398": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "counting", "self-similarity", "temporal", "frames", "video", "repeating", "lines", "transformer", "attention", "cnn", "convolutional neural network", "repetitions", "periodicity", "period", "repeat", "actions", "kinetics", "countix"], "399": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ml", "reading", "papers", "understanding", "quickly", "quick", "fast", "ultralearning", "research", "facebook", "detr", "object detection", "transformers", "how to"], "400": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "graph networks", "graph neural networks", "gnn", "physics", "newtonian", "hamiltonian", "dynamics", "cosmology", "dark matter", "symbolic regression", "edge", "vertex", "regularization"], "401": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gnn", "transformer", "graph", "biology", "neurons", "axon", "dendrites", "plausible", "biologically plausible", "backprop", "backpropagation", "dfa", "feedback alignment", "random projections"], "402": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "cnn", "convolutional neural network", "coco", "object detection", "region of interest", "rcnn", "r-cnn", "attention", "attention mechanism", "google", "caltech", "gazelle", "wildlife", "wild trap", "traffic", "object", "car", "bus", "vehicle", "lighting", "time", "sampling", "frames", "memory", "long-term", "query"], "403": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "sets", "images", "cnn", "convolutional neural network", "gan", "generator", "encoder", "discriminator", "prior", "mean", "made", "latent", "binary", "conditional", "noise", "distribution", "probability", "energy-based", "energy", "apple", "research", "sdn", "variational", "elbo"], "404": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ssl", "semi-supervised", "transfer learning", "cnn", "resnet", "efficientnet", "noise", "augmentation", "data augmentation", "randaugment", "dropout", "stochastic depth", "google", "distillation", "self-training", "knowledge distillation", "imagenet", "unsupervised", "unlabeled", "unlabelled", "jft"], "405": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "prune", "pruning", "transfer learning", "weights", "magnitude", "gradient", "moving", "small", "importance", "huggingface", "nlp", "natural language processing", "squad", "mnli", "bert", "transformer", "attention", "cnn", "distillation", "teacher", "sparse", "sparsity", "question answering", "mobile", "edge", "tune", "fine-tune"], "406": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "ethz", "vision", "objects", "slots", "attention mechanism", "gru", "lstm", "routing", "capsules", "permutation invariant", "encoder", "set", "detr", "embeddings", "transformer", "weight sharing", "disentanglement", "render", "tetris", "clevr", "cnn", "convolutional neural network", "attention"], "407": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "billion", "parameters", "float32", "attention mechanism", "transformer", "scale", "gpt-3", "google", "gshard", "xla", "sharding", "parallelism", "mixture of experts", "trillion", "tpus", "distributed", "m4", "multilingual translation", "natural language processing"], "408": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "bert", "transformer", "mlm", "language model", "masked language modeling", "proteins", "protein", "amino acid", "primary", "secondary", "tertiary", "structure", "helix", "strand", "band", "sheet", "turn", "binding site", "contact map", "dna", "rna", "amino acids", "proline", "phenylalanine"], "409": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "recognition", "localization", "resnet", "resnet50", "fpn", "backbone", "permuation", "upsampling", "stride", "convolution", "convolutional neural network", "google", "spine", "spine net", "imagenet", "coco", "segmentation", "bounding box", "skip connections", "residual", "bottleneck"], "410": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "attention", "attention mechanism", "linear", "linear transformer", "linformer", "reformer", "idiap", "epfl", "queries", "keys", "softmax", "kernel", "routing", "inner product", "rnn", "recurrent neural network", "transformer", "bert", "autoregressive", "dimensions", "topic modeling", "language model"], "411": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "supsup", "supermasks", "lottery ticket", "lottery ticket hypothesis", "gradient", "entropy", "surplus", "superfluous neurons", "lifelong learning", "multitask learning", "catastrophic forgetting", "continuous learning", "binary mask", "random network", "optimization", "hopfield network", "gradient descent", "superposition"], "412": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "supsup", "supermasks", "lottery ticket", "lottery ticket hypothesis", "gradient", "entropy", "surplus", "superfluous neurons", "lifelong learning", "multitask learning", "catastrophic forgetting", "continuous learning", "binary mask", "random network", "optimization", "hopfield network", "gradient descent", "superposition"], "413": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gan", "vae", "kl", "elbo", "autoencoder", "variational", "latent", "sampling", "hierarchical", "scales", "faces", "mnist", "cifar10", "swish", "batch norm", "generative", "nvidia", "mixed precision", "memory", "deep", "layers", "depthwise convolutions", "cnn", "convolutional", "generation", "generative model"], "414": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gon", "gradient", "negative gradient", "implicit", "implicit representation", "siren", "sirens", "deep neural networks", "convolutional neural network", "dnns", "mnist", "cifar10", "fashion mnist", "gradient descent", "sgd", "inner loop", "backpropagation", "live code", "code", "machine learning code", "research", "research paper"], "415": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "ensembles", "bayesian", "modes", "loss function", "nonconvex", "google", "deepmind", "stan fort", "foundational", "weight space", "labels", "agreement", "minima", "loss landscape", "trajectory", "local minima", "optimization"], "416": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "dqn", "deep q learning", "deep q networks", "q learning", "qlearning", "rl", "drl", "deep rl", "deep reinforcement learning", "deepmind", "david silver", "atari", "pong", "breakout", "space invaders", "agent", "cnn", "convolutional neural network", "bellman"], "417": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "classic", "alexnet", "hinton", "geoff hinton", "imagenet", "convolution", "convolutional neural network", "architecture", "dropout", "data augmentation", "cnns", "computer vision", "image classification", "object recognition", "classifier", "max pool", "pretraining", "deep neural networks"], "418": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gan", "generator", "discriminator", "convolution", "deconvolution", "goodfellow", "bengio", "convolutional neural network", "mnist", "cifar10", "generative", "generative model", "image generation", "face model", "latent space", "interpolation", "minmax", "nash equilibrium", "game theory"], "419": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "jeff dean", "mikolov", "word2vec", "word vectors", "word representations", "nlp", "natural language processing", "sentiment classification", "king", "queen", "man", "woman", "arithmetic", "latent space", "distributed", "country", "capital", "semantic", "synonyms", "skip gram", "negative sampling", "nce", "noise contrastive estimation"], "420": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vision", "computer vision", "kaiming he", "google", "resnet", "resnet50", "resnet151", "deep neural network", "imagenet", "residual", "identity function", "very deep", "convolutional neural network", "bottleneck", "overfitting"], "421": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nas", "nas-bench", "architecture search", "initialization", "untrained", "cifar10", "imagenet", "neural architecture search", "controller", "rnn", "correlation", "gradient", "jacobian", "linearization"], "422": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "rl", "deep rl", "deep reinforcement learning", "on-policy", "on policy", "off policy", "replay buffer", "normalization", "initialization", "control", "continuous control", "deep neural networks", "agent", "environment", "mujoco", "hyperparameters", "learning rate", "optimizer", "adam", "entropy", "regularization", "grid search"], "423": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "schmidhuber", "hochreiter", "lstm", "gru", "rnn", "hopfield", "attention", "attention is all you need", "transformer", "bert", "query", "key", "value", "routing", "pattern", "retrieval", "store", "error", "exponental", "binary", "continuous", "hopfield network", "lse", "energy function", "update rule", "metastable", "separation"], "424": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "hebbian", "vision", "car", "ant", "quadruped", "neuroplasticity", "fire together wire together", "reinforcement learning", "deep rl", "deep reinforcement learning", "policy network", "policy gradient", "evolutionary methods", "evolution step", "population", "correlation", "gradient", "episode", "random", "adaptive", "reconfigure", "damage", "injury", "agent"], "425": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "orqa", "qa", "question answering", "google", "kenton", "wikipedia", "mlm", "bert", "masked language modeling", "realm", "t5", "transformer", "inner product", "mips", "index", "pretraining", "ict", "inverse cloze task", "google ai", "search", "retrieval", "documents", "natural questions", "open domain", "attention", "salient", "masking", "encoder"], "426": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "rl", "deep rl", "q learning", "deep reinforcement learning", "q learning machine learning", "deep q learning", "successor features", "deep mind", "zero shot", "environment", "agent", "task", "linear", "regression", "reward", "mila", "neural network", "reinforcement learning", "value function", "state value function", "state value"], "427": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cnn", "imagenet", "resnet", "radioactive", "fake", "feature", "feature space", "feature extractor", "facebook ai", "fair", "deep neural networks", "classifier", "classes", "backpropagation", "black box", "white box", "detect", "features", "privacy", "adversarial examples", "tagging", "inria"], "428": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "cnn", "resnet", "big bird", "bigbird", "attention", "attention mechanism", "attention for images", "transformer for images", "transformer", "bert", "convolutions", "window", "neighbors", "axial attention", "position embeddings", "positional encodings", "quadratic", "memory", "panoptic segmentation", "coco", "imagenet", "cityscapes", "softmax", "routing"], "429": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "biology", "biological", "alive", "living", "message passing", "global state", "local state", "information", "cellular automata", "neural cellular automata", "neural ca", "convolution", "recurrent", "rnn", "pixels", "cell state", "latent state", "distill", "distill pub", "mnist", "neural network", "digit classification"], "430": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "nlp", "transformer", "gpt", "gpt3", "gpt-3", "gpt-2", "natural language processing", "summarization", "extractive", "reddit", "attention mechanism", "language model", "natural language understanding", "human feedback", "human in the loop", "active learning", "reward", "reward model", "reinforcement learning", "deep reinforcement learning", "deep rl", "ppo", "proximal policy optimization", "adversarial example", "broader impact"], "431": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "hardware", "gpus", "tpus", "gpu", "tpu", "convolutional neural networks", "yann lecun", "history", "historic", "ai winter", "expert systems", "babbage", "google", "accelerators", "cuda", "nvidia", "flops", "von neumann architecture", "bottleneck", "parallelize", "research", "funding", "society", "cost", "competition", "general purpose", "fpga"], "432": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "optimization", "lstm", "taskset", "google", "google research", "compute", "outer optimization", "adam", "adamw", "sgd", "momentum", "learning rate", "gradient", "learned optimizer", "second moment", "cnn", "rnn", "paper explained", "neural network", "gradient descent", "hyper parameters", "grid search", "mnist", "cifar10", "imagenet"], "433": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention mechanism", "convolutional neural network", "data science", "cnn", "transformer", "attention is all you need", "vaswani", "beyer", "google", "google brain", "google research", "tpu", "tpu v3", "iclr", "iclr 2021", "peer review", "anonymous", "karpathy", "andrej karpathy", "twitter", "review", "under submission", "big transfer", "bit", "vit", "vision transformer", "visual transformer", "transformer images", "transformer computer vision"], "434": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "optimization", "polyak", "nesterov", "benchmark", "cnn", "cifar", "mnist", "adam", "adagrad", "adadelta", "momentum", "sgd", "gradient", "learning rate", "tuning", "budget", "default parameters", "comparison", "grid search", "random search", "random seed", "vae", "learning rate schedule", "cosine decay", "trapezoid", "improvement", "best optimizer", "best optimizer for deep learning", "stochastic gradient descent"], "435": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention", "attention mechanism", "lambda", "lambdaresnet", "residual networks", "local attention", "quadratic", "memory", "transformer", "transformers", "keys", "values", "queries", "architecture", "input size", "iclr", "lambdanet", "lambdanets", "lambdaresnets", "efficientnet", "tradeoff", "routing", "linear function", "functional programming"], "436": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "natural language understanding", "data science", "transformer", "attention", "attention mechanism", "transformers", "attention is all you need", "gpus", "tpu", "linformer", "reformer", "explanation", "imagenet64", "kernels", "gaussian kernel", "softmax", "softmax kernel", "approximation", "random features", "random positive features", "random fourier features", "google", "favor", "machine translation"], "437": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nlp", "natural language processing", "bert", "gpt", "gpt2", "gpt-2", "gpt3", "gpt-3", "gpt 2", "gpt 3", "knowledge graph", "knowledge base", "language", "natural language understanding", "berkeley", "uc berkeley", "dawn song", "unsupervised", "extraction", "corpus", "wikidata", "wikipedia", "entity linking", "entity recognition", "spacy", "attention", "attention matrix", "beam search", "viterbi", "causal attention", "language model", "autoregressive"], "438": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "pipeline", "ml pipeline", "deep networks", "epidemiology", "theoretical", "underspecification", "overparameterization", "overfitting", "generalization", "out of distribution", "bert", "gender", "stereotypes", "distribution shift", "analysis", "performance", "bias", "correlation", "problems", "quality assurance"], "439": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "berkeley", "purdue", "mc hammer", "mchammer", "mit", "technology review", "pde", "partial differential equation", "navier stokes", "darcy flow", "burgers", "convolutions", "fft", "dfft", "fourier transform", "fourier neural operator", "neural operator", "fast fourier transform", "fourier modes", "flow", "turbulent flow", "fluid dynamics", "residual", "aerodynamics", "wind tunnel", "neural network", "layers", "numerical", "discretization"], "440": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "backpropagation", "computation", "autograph", "tensorflow", "pytorch", "torch", "autodiff", "differentiation", "backprop", "biologically plausible", "neurons", "error signal", "predictive coding", "variational", "gaussian", "iterative", "local updates", "distributed", "inner loop", "brain", "neuroscience", "deep neural networks", "analyzed", "hand drawing", "cnn", "rnn", "lstm", "convolutional neural network", "recurrent neural network", "hebian"], "441": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "deepmind", "deep mind", "alphago", "alphazero", "alphafold", "protein", "dna", "rna", "folding", "casp", "casp14", "alphafold 2", "blog", "hassabis", "biology", "translation", "amino acid", "transformer", "convolution", "residual", "spatial graph", "refine", "gradient descent", "van der waals", "torsion angles", "google ai", "google brain", "nobel prize", "msa", "multiple sequence alignment", "covariation", "evolution", "contact prediction", "distogram"], "442": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "poker", "deep neural networks", "facebook", "facebook ai", "rebel", "holdem", "texas holdem", "rock paper scissors", "liars dice", "liar dice", "self play", "nash equilibrium", "alpha go", "alphazero", "zero sum", "policy", "cfr", "counterfactual regret minimization", "tree search", "monte carlo tree search", "mcts", "public belief state", "infostate", "value function", "supergradient", "strategy", "actor critic", "imperfect information"], "443": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "google", "apple", "openai", "berkeley", "stanford", "carlini", "dawn song", "google ai", "nlp", "natural language processing", "gpt", "gpt2", "gpt-2", "gpt3", "gpt-3", "gpt 2", "gpt 3", "bert", "transformers", "attention", "training data", "security", "leak", "privacy", "data protection", "ethics", "broader impact", "likelihood", "perplexity", "entropy", "url", "uuid", "personal information", "address", "private", "user data", "gdpr", "adversarial", "zlib"], "444": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt", "gpt-3", "visual transformer", "transformer", "transformers", "attention mechanism", "vqvae", "vq vae", "vq-vae", "codebook", "relaxation", "gumbel", "text", "images", "nlp", "natural language processing", "autoregressive", "grid", "encoder", "decoder", "gpt3", "avocado chair", "porcupine sphere", "animations", "fisheye", "text to image", "image captioning", "openai", "sutskever", "dali", "dalle", "walle", "vector quantized", "hierarchical", "gan", "generative", "likelihood"], "445": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "sutskever", "radford", "meme", "dalle", "dall-e", "images", "vision", "text", "nlp", "natural language processing", "resnet", "vision transformer", "transformer", "visual transformer", "sota", "state of the art", "zero shot", "zero-shot", "few shot", "few-shot", "unsupervised", "contrastive", "simclr", "efficientnet", "noisy student", "representation", "embedding", "latent", "natural language", "prompt engineering", "bias", "scale", "distribution shift"], "446": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention", "transformer", "attention mechanism", "google", "google brain", "shazeer", "trillion", "trillion parameter", "language model", "gpt3", "gpt-3", "gpt 3", "t5", "sharding", "mesh", "mtf", "mesh tensorflow", "query", "key", "value", "feed forward", "experts", "routing", "mixture of experts", "sparse", "sparse experts", "data parallelism", "model parallelism", "expert parallelism", "trillion parameters", "perplexity", "scaling", "flops", "bfloat16"], "447": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "singularity", "singularitynet", "agi", "ben goertzel", "goertzel", "hanson", "hanson robotics", "sophia", "network", "api", "offercoin", "offernetworks", "offer networks", "offer coin", "agi token", "erc20", "ethereum", "cardano", "governance", "benefit", "reputation", "reputation system", "liquid rank", "liquidrank", "deoldify", "inflation", "ico", "matchmaking", "graph", "opencog", "open cog", "tontoni phi", "intelligence", "artificial general intelligence", "blockchain"], "448": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformer", "rnn", "lstm", "seq2seq", "gpt3", "gpt-3", "nlp", "natural language processing", "language modelling", "feedback transformers", "memory", "attention", "attention mechanism", "attention is all you need", "facebook ai", "fair", "long range", "complex", "reasoning", "bert", "autoregressive", "reinforcement learning", "abstraction", "representation", "higher layers", "attention matrix", "recurrent neural networks"], "449": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep neural networks", "neural networks gradient descent", "kernel machines", "kernel trick", "svm", "support vector machine", "sgd", "stochastic gradient descent", "machine learning theory", "pedro domingos", "linear regression", "nearest neighbor", "representations", "data representations", "representation learning", "proof", "math proof", "learning theory", "representer theorem"], "450": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "machine learning explained", "transformers explained", "nystrom", "nystromformer", "nystromer", "nystrom approximation", "self attention", "attention mechanism", "attention is all you need", "transformer", "linear transformer", "linformer", "linear attention", "machine learning tutorial", "quadratic attention", "matrix approximation", "low rank", "landmark points", "landmarks", "matrix reconstruction", "fast attention"], "451": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "machine learning tutorial", "machine learning explained", "batch normalization", "jax", "layer normalization", "gradient clipping", "weight standardization", "normalizer-free", "nfnets", "nfnet", "nfresnet", "deepmind", "deep mind", "best neural network", "imagenet", "best imagenet model", "distributed training", "mean shift", "batch norm", "batchnorm", "nfnets code", "deep learning code", "ml code"], "452": ["deep learning", "machine learning", "arxiv", "neural networks", "ai", "artificial intelligence", "attention neural networks", "attention is all you need", "transformer gan", "transformer gans", "transformer generative adversarial network", "generative adversarial network", "attention mechanism", "self attention", "vision transformer", "pixelshuffle", "superresolution", "local attention", "multihead attention", "transformer generator", "google", "machine learning explained", "deep learning explained", "paper explained", "transgan"], "453": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "reinforcement learning", "deep reinforcement learning", "dreamer", "dreamer v2", "dreamer rl", "dreamer reinforcement learning", "google reinforcement learning", "deepmind reinforcement learning", "google ai", "world model", "world model reinforcement learning", "google deepmind world model", "google deepmind reinforcement learning", "atari reinforcement learning", "atari world model", "rainbow", "muzero"], "454": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "huggingface", "huggingface transformers", "microsoft", "microsoft research", "bert", "roberta", "deberta", "nlp", "natural language processing", "glue", "superglue", "state of the art", "transformers", "attention", "attention mechanism", "disentanglement", "disentangled representation", "positional encodings", "position embeddings", "masked language modelling", "pretraining", "open source"], "455": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "fast weights", "fast weights hinton", "fast weights neural network", "schmidhuber", "j\u00fcrgen schmidhuber", "juergen schmidhuber", "lstm transformer", "performers", "transformer performer", "linear transformer", "linear attention", "linear attention transformer", "autoregressive model", "autoregressive transformer", "transformer kernel", "kernels transformer", "favor performer", "favor algorithm", "deep learning tutorial"], "456": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "geoff hinton", "geoff hinton capsule networks", "geoff hinton neural networks", "geoffrey hinton", "geoffrey hinton deep learning", "geoffrey hinton glom", "hinton glom", "glom model", "deep learning tutorial", "introduction to deep learning", "capsule networks", "computer vision", "capsule networks explained", "google brain", "google ai", "schmidhuber", "transformer", "attention mechanism", "consensus algorithm", "column"], "457": ["deep learning", "machine learning", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "openai emotions", "openai dalle", "openai clip", "openai microscope", "openai clip microscope", "alec radford", "emotion neuron", "deep learning emotion", "chris olah", "chris olah openai", "neural network feature visualization", "multimodal neural network", "what does a neural network learn", "what do neural networks learn", "how do neural networks work", "what does openai do", "faceted visualization"], "458": ["deep learning", "machine learning", "explained", "neural networks", "artificial intelligence", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "what is self supervised learning", "self supervised learning", "self-supervised learning", "self-supervised learning yann lecun", "yann lecun", "yann lecun energy based models", "energy based models", "energy based machine learning", "energy based models deep learning", "byol", "contrastive learning", "bert", "noise contrastive estimation"], "459": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "berkeley", "google brain", "facebook ai research", "pretrained transformers", "gpt-3", "huggingface", "language model", "fine-tuning", "finetuning", "out of domain generalization", "universal computation", "can transformers solve xor", "transformer mnist", "transformer cifar10", "fine tuning transformer", "gpt-2", "pretrained language model"], "460": ["Kubernetes"], "461": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "deepmind", "perceiver", "cross attention", "attention mechanism", "attention is all you need", "google deepmind", "deepmind perceiver", "perceiver model", "perciever model", "perciever", "self attention", "rnn", "recurrent neural network", "weight sharing", "computer vision", "natural language processing", "fourier features"], "462": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "artificial intelligence", "wake sleep algorithm", "program synthesis", "ai program synthesis", "program synthesis deep learning", "dreamcoder", "dream coder", "mit dream coder", "bayesian program search", "neural guided search", "learning to sort a list", "neural networks learn sorting", "deep learning physical laws", "deep learning symbolic reasoning", "symbolic machine learning", "symbolic artificial intelligence", "deep learning tutorial"], "463": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "nerf network", "nerf neural network", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "deep learning explanation", "nerf network explanation", "neural rendering", "differentiable rendering", "differentiable neural rendering", "volume rendering", "nerf view synthesis", "view synthesis", "view synthesis nerf", "view synthesis neural", "novel view synthesis", "nerf"], "464": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "ai winter", "ai spring", "why is ai hard", "can machines think", "can machines be conscious", "alan turing", "elon musk artificial intelligence", "self driving cars", "marvin minsky", "expert systems", "deep learning artificial intelligence", "are neural networks artificial intelligence", "why is deep learning important"], "465": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "facebook", "facebook ai", "fair", "byol", "swav", "self supervised learning", "unsupervised feature learning", "unsupervised machine learning", "feature engineering", "stop gradient", "dino", "self distillation", "self-distillation", "segmentation maps", "visual transformer", "visual transformer self supervised", "imagenet"], "466": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "computer vision", "convolutional neural network", "convolutions alternative", "cnn attention", "self attention", "attention mechanism for vision", "weight sharing neural networks", "convolutions vision", "cnn vision", "involution vision", "image segmentation", "rednet", "resnet", "residual neural networks", "bytedance ai"], "467": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "diffusion models", "diffusion model", "ddpm", "ddim", "denoising autoencoders", "generative models", "generative models deep learning", "gan alternatives", "alternatives to gans", "computer vision generative", "machine learning image generation", "openai diffusion", "openai gan", "variational autoencoder", "log likelihood", "variational lower bound"], "468": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "expire span", "facebook ai", "transformers", "long sequence models", "transformers long sequence", "large context language models", "language model sequence length", "transformer xl", "learning to forget", "lstm", "schmidhuber", "learning to remember", "not all memories are created equal", "linear attention", "attention mechanism", "linear attention mechanism", "transformer memory", "deep learning tutorial"], "469": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "recurrent independent mechanisms", "metarim", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "machine learning paper", "deep reinforcement learning", "reinforcement learning meta learning", "yoshua bengio", "bentio mila", "grid world", "fast and slow learning", "reinforcement learning attention", "catastrophic forgetting", "lifelong learning", "multitask learning"], "470": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "how to achieve agi", "artificial general intelligence", "how to create intelligence", "reward maximisation", "reward maximization", "reinforcement learning", "is alphago intelligence", "is gpt 3 self aware", "is gpt 3 intelligent", "how to create ai", "how to achieve ai", "general ai", "agent environment", "deepmind"], "471": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "decisiontransformer", "decision transformer", "berkeley", "uc berkeley", "facebook ai language", "fair", "deep learning tutorial", "what is deep learning", "introduction to deep learning", "transformers for reinforcement learning", "transformers for rl", "transformer reinforcement learning", "sequence modeling", "sequence modelling", "sequence modeling reinforcement learning", "reinforcement learning with transformers"], "472": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "implicit differentiation", "implicit function theorem", "imaml", "inner optimization", "inner optimization procedure", "how to backpropagate through sgd", "backpropagate through optimizer", "outer optimization loop", "bi-level optimization", "implicit graident", "gradient of optimizer", "dictionary learning", "dataset distillation", "google research", "what is deep learning", "deep learning tutorial"], "473": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "reinforcement learning", "imitation learning", "uc berkeley", "sergey levine", "sergey levine reinforcement learning", "pieter abbeel", "pieter abbeel reinforcement learning", "walk and punch", "learning from demonstration", "amp", "adversarial motion priors", "physics based reinforcement learning", "3d reinforcement learning"], "474": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "xcit", "facebook ai", "fair", "transformer", "transformer neural network", "transformer computer vision", "vision transformer", "deit", "self-supervised learning", "imagenet", "attention mechanism", "linear attention mechanism", "deep learning computer vision", "state of the art", "transpose attention", "linear attention", "linear attention transformer", "convolutional neural network", "what is deep learning", "dino"], "475": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "adversarial examples", "goodfellow", "goodfellow adversarial attacks", "adversarial attacks on neural networks", "features not bugs", "madry", "dimpled manifold", "why do adversarial examples exist", "adversarial examples explanation", "adversarial attacks explanation", "computer vision", "decision boundary", "data manifold", "low dimensional manifold", "what are adversarial examples", "what is deep learning"], "476": ["neural networks", "artificial intelligence", "what is deep learning", "introduction to deep learning", "deep learning tutorial", "neuralhash", "neural hash", "apple privacy", "icloud privacy", "icloud encryption", "icloud illegal", "apple illegal", "apple scan", "apple scan illegal material", "icloud illegal material", "blinding step", "hash function", "private set intersection", "adversarial attack", "threshold secret sharing", "icloud", "csam", "csam apple", "csam apple scanning", "csam detection", "explained"], "477": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "pondernet", "deepmind", "pondernet learning to ponder", "deepmind pondernet", "pondernet explained", "dynamic computation", "deep learning classic algorithms", "halting probability", "deep learning recurrent computation", "dynamic recurrent network", "broader impact", "deep network learning to stop"], "478": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "attention mechanism", "attention is all you need", "fastformer", "fast former", "nlp", "natural language processing", "linear attention", "linear transformer", "query key value", "additive attention", "elementwise product", "fast transformer", "faster transformer", "transformer memory", "attention quadratic memory", "fastformer explained"], "479": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "alibi", "transformer", "position encoding", "position embeddings", "fair", "google", "attention is all you need", "causal masking", "causal attention", "attentin matrix", "attention matrix", "vasvani", "sinusoidal position encodings", "learned position embeddings", "train short test long", "alibi position encodings", "transformer position encodings", "transformer position embeddings", "transformer long sequences"], "480": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "inftyformer", "infinityformer", "infty former", "infinity former", "transformer", "transformers", "transformer linear", "linear attention", "unbounded memory transformer", "continuous attention", "attention mechanism", "continuous attention mechanism", "radial basis function", "radial basis functions", "ridge regression", "long term memory", "long term memory explained"], "481": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "vae", "variational", "bayesian", "variational methods", "variational autoencoder", "max welling", "elbo", "prior", "student t", "reparameterization trick", "log likelihood", "encoder decoder"], "482": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt-3", "truthful", "truthfulqa", "conspiracy", "conspiracy theories", "large language models", "ezra klein", "inverse scaling", "openai", "gpt-j", "gpt-neo", "imitative falsehoods", "adversarial", "informativeness", "evaluation", "trustworthy", "ml bias", "are language models biased", "is gpt-3 truthful", "question answering", "harmful prompt", "helpful prompt"], "483": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "neurips", "nips", "nips experiment", "peer reviw", "conference review", "reviewer", "machine learning reviewer", "ml conference review", "subjectivity in peer review", "reviewer opinions", "science review", "science peer review", "peer review fail"], "484": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "scale", "co2", "gpt-3", "bert", "language models", "environment", "large scale", "large language models", "deep neural networks", "transformers", "imagenet", "datasets", "language modeling", "training cost", "openai", "microsoft", "google", "google ai", "facebook research", "transfer learning", "meta learning", "exponential scale", "overparameterization"], "485": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "grokking", "openai", "double descent", "belkin", "overfitting", "bias variance", "steps", "training", "binary tables", "binary operations", "binary operation", "multiplication table", "algorithmic datasets", "groups", "s5 group", "deep learning algorithmic", "deep learning generalization", "generalization research", "why do neural networks generalize"], "486": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt-3", "knowledge distillation", "teacher", "student", "nlp", "natural language processing", "gpt3", "prompt engineering", "symbolic knowledge", "symbolic reasoning", "symbolic nlp", "knowledge graphs", "triples", "what does gpt-3 know", "does gpt-3 understand"], "487": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "muzero", "alphazero", "berkeley", "pieter abbeel", "dreamer", "dreamerv2", "atari", "reinforcement learning", "deep reinforcement learning", "world model", "learned world model", "latent world model", "alphago", "deep rl", "model-based reinforcement learning", "how does muzero work", "efficientzero", "efficientzero model", "atari 100k", "sample-efficient reinforcement learning"], "488": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "diffusion models", "autoregressive models", "generative models", "nlp", "natural language processing", "gpt", "image-gpt", "gpt-3", "gpt-2", "order agnostic", "order agnostic diffusion", "generative diffusion models", "bert", "autoregressive bert", "bert text generation", "character level language model", "upscaling", "dynamic programming", "pixelwise sampling"], "489": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "backpropagation", "all you need", "gradients", "machine learning gradients", "differentiable environment", "differentiable physics", "differentiable simulation", "when to use gradients", "when not to use gradients", "when to avoid gradients", "google research", "google ai"], "490": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "grafting", "learning rate", "deep learning learning rate", "neural network learning rate", "adaptive learning rate", "adaptive optimizer", "learning rate grafting", "optimizer grafting", "adam", "sgd", "adagrad", "lars", "lamb", "openreview", "reviewer", "automatic learning rate", "learning rate decay", "learning rate warmup"], "491": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "imle", "implicit mle", "maximum likelihood", "backpropagation through algorithms", "deep learning discrete", "discrete deep learning", "discrete backpropagation", "gradient discrete", "gradient of an algorithm"], "492": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "terraformer", "scaling transformers", "nli", "nlp", "natural language processing", "transformers memory", "deep learning memory", "fast transformer", "fast transformers", "attention", "attention mechanism", "attention is all you need", "bert", "gpt-3", "google research", "reversible layers", "reformer", "sparse attention", "sparse feedforward", "low-rank"], "493": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "clearml", "nuwa", "n\u00fcwa", "visual pretraining", "pretraining vision models", "igpt", "image gpt", "autoregressive", "autoregressive image gpt", "autoregressive image generation", "nearby self-attention", "3dna", "3d nearby self-attention", "transformer", "transformer for videos", "deep learning on videos", "deep learning video generation", "video manipulation", "text to image", "text to video", "microsoft"], "494": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "lama", "inpainting", "gan", "adversarial", "loss function", "fourier transform", "fft", "fast fourier transform", "fourier convolution", "fast fourier convolution", "fourier convolution layer", "global information", "generative model", "periodic strucutre", "best inpainting", "ai inpainting", "first author interview", "lama inpainting", "mask filling", "large mask inpainting", "remove from picture", "ai image editing"], "495": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "glide", "diffusion", "clip-guided diffusion", "diffusion models", "clip-guided diffusion models", "generative models", "image to text", "generate image from text", "ai text to image", "machine learning text to image", "text 2 image", "classifier-free guidance", "noise process", "posterior", "variational lower bound", "log likelihood", "dalle", "dall-e", "ai drawing", "ai images"], "496": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "reinforcement learning", "ai for go", "ai go", "ai chess", "chess ai", "stockfish", "alphazero", "alpha zero", "muzero", "player of games", "pog", "deepmind", "deepmind games", "imperfect information games", "ai for poker", "perfect vs imperfect information", "public state", "scotland yard", "ai for scotland yard", "reinforcement learning poker", "ai no limit holdem", "counterfactual regret minimization", "tree search"], "497": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "minecraft", "minerl", "minerl basalt", "minecraft machine learning", "minecraft ai", "human-like ai", "minecraft bot", "minecraft ai challenge", "minecraft reinforcement learning", "behavior cloning", "kairos", "minecraft kairos", "minerl kairos", "minerl winners", "interview", "with the authors", "minecraft deep learning", "minecraft behavior cloning", "gail", "generative adversarial imitation learning", "state machine"], "498": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "noether networks", "noether's theroem", "noether theorem", "symmetries", "neural network bias", "neural network symmetries", "inductive biases", "conserved quantities", "pendulum", "neural network physics", "deep learning physics", "deep learning symmetries", "group convolutions", "with the authors", "paper explained", "deep learning prediction", "test time optimization", "tailoring", "neural network tailoring"], "499": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "neural interpreters", "dynamic inference", "neural programming", "neural functions", "recurrent networks", "yoshua bengio", "mila", "schoelkopf", "attention", "modlin", "modulated linear layer", "weight sharing", "recurrent modules", "function modules", "sparse neural networks", "interview", "first author interview", "with the authors", "dynamic inference with neural interpreters", "deep neural interpreters"], "500": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "research", "symbolic", "symbolic regression", "neuro symbolic computation", "integer sequences", "oeis", "number sequences", "ai number sequences", "machine learning sequences", "integer sequence rules", "embedding space", "transformers", "attention mechanism", "sequence generation", "learning number sequences", "predicting number sequences", "facebook ai", "meta ai", "beam search", "symbolic vs numeric"], "501": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "xcorr", "patrick mineault", "unsupervised models", "neuroscience", "neuroscience and deep learning", "deep learning brain", "machine learning brain", "brain models", "how does the brain work", "deep learning and neuroscience", "self-supervised models", "representation learning", "does the brain do representation learning", "does the brain work like a deep neural network", "neurips"], "502": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "natural language processing", "training data", "deep learning tutorial", "nlp", "gpt3", "gpt 3", "codex", "openai codex", "large language models", "gpt 3 planning", "zero-shot planning", "zero shot learning", "virtualhome", "virtual home", "bert", "bert model", "bert translation", "bert embedding", "pieter abbeel", "reinforcement learning", "human language learning"], "503": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "metalearning", "meta learning", "neural network", "unsupervised learning", "few shot learning", "google", "google research", "google ai", "transformer", "meta transformer", "hypertransformer", "hyper transformer", "generate the weights of a neural network", "privacy", "personalization", "interview", "paper explained", "semi-supervised learning"], "504": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "security", "machine learning in security", "ai security", "ai network security", "deep learning censorship", "ai censorship", "internet censorship", "geneva", "vpn", "genetic algorithms", "genetic algorithm", "genetic algorithm example", "real world genetic algorithm", "ai in the real world", "firewall", "evolution", "evolutionary search", "maryland", "breakerspace", "encryption", "amplification"], "505": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "cm3", "facebook ai", "fair", "meta ai", "language model", "language modelling", "gpt-3", "gpt 3", "gpt3", "dall-e", "ru-dalle", "text to image", "ai image generation", "ai internet", "language model html", "transformer html", "large language models", "transformer", "autoregressive", "causal masking", "causally masked language model", "bidirectional", "bert", "masked language modelling"], "506": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "507": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "alphacode", "alpha code", "deepmind", "deepmind code", "deepmind alphacode", "alphacoder", "codex", "copilot", "ai code", "ai programmer", "ai competitive programming", "ai leetcode", "machine learning leetcode", "deepmind leetcode", "codeforces", "large scale sampling", "language models", "language models for code", "ai python programmer", "deep mind", "fuzzing", "google deepmind", "competitive programming ai"], "508": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "openai", "formal math", "ai math", "ai math prover", "machine learning for math", "ml math", "artificial intelligence math", "ai mathematics", "automated proof search", "mini f2f", "ai imo", "ai math olympiad", "openai mathematics", "openai formal math", "language models formal math", "lean", "lean prover", "lean proof", "lean math", "ai lean environment", "ai proves theorems", "ai theorem prover"], "509": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "ml and society", "ai and society", "sociology and machine learning", "machine learning for sociology", "machine learning for economics", "ai microeconomics", "reinforcement learning economics", "society simulations", "silly rules", "social norms", "social norms enforcement", "why do social norms exist", "why do silly rules exist", "deep mind society"], "510": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "paper explained", "virtual outliers", "how to detect outliers", "deep learning outliers", "deep learning outlier detection", "vos", "deep learning energy", "latent space outliers", "density estimation", "classification boundaries", "generative models"], "511": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "paper explained", "virtual outliers", "how to detect outliers", "deep learning outliers", "deep learning outlier detection", "vos", "deep learning energy", "latent space outliers", "density estimation", "classification boundaries", "generative models"], "512": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "active dendrites", "neurons dendrites", "biological deep learning", "deep learning biology", "numenta", "numenta research", "numenta deep learning", "dendrites deep learning", "deep learning tutorial", "hierarchical temporal memory", "computational neuroscience", "reinforcement learning", "robotics", "multi task learning", "continuous learning", "continual learning", "permuted mnist"], "513": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "zeta alpha", "blip", "language vision pre training", "language vision pre-training", "deep learning pre-training", "clip pre-training", "blip pretraining", "parameter sharing", "sequence to sequence", "image captioning", "vqa", "visual question answering", "fine-tuning", "vit", "vision transformer", "salesforce"], "514": null, "515": null, "516": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "machine learning news", "ml paper", "machine learning paper", "language", "nlp", "natural language processing", "stanford", "reinforcement learning", "data science", "deep learning tutorial", "deep learning paper", "language in reinforcement learning", "rl nlp", "nlp rl", "nlp reinforcement learning", "exploration exploitation", "rl exploration"], "517": null, "518": null, "519": null, "520": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "diffusion models", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "generative models", "parti", "google parti", "google party", "google pathways", "google imagen", "image", "dalle", "dalle2", "dalle 2", "dall e 2", "dall e 2 vs graphic designer", "anubis"], "521": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "minerl", "minecraft ai", "diamond pickaxe", "ai diamond pickaxe", "openai minecraft", "deep learning projects", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "gpt 3", "gpt-3", "vpt", "video pretraining", "video pre-training", "openai vpt", "vpt minecraft", "minecarft"], "522": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "jepa", "h-jepa", "yann lecun", "lecun", "agi", "artificial general intelligence", "openreview"], "523": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "deepmind", "deep mind", "deepmind alphatensor", "alpha tensor", "deepmind math", "google deep mind", "google deepmind", "matrix multiplication", "ai matrix multiplication", "matrix multiplication reinforcement learning", "alphazero", "alpha zero", "alphazero math", "deep learning tutorial", "introduction to deep learning", "what is deep learning", "alphatensor explained", "alpha tensor explained"], "524": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "525": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "526": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "galactica", "meta", "meta ai", "facebook ai", "ai science", "galactica ai", "galactica model", "yann lecun", "research", "fair", "deep learning tutorial", "what is deep learning", "introduction to deep learning"], "527": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "introduction to deep learning", "deep learning tutorial", "meta", "meta ai", "meta cicero", "cicero ai", "meta cicero ai", "diplomacy ai", "web diplomacy", "facebook ai", "fair ai", "language model", "politics ai", "geopolitics ai", "ai online game"], "528": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "introduction to deep learning", "meta ai", "meta llama", "llama llm", "gpt-3", "large language models", "transformers", "chatgpt", "instruction tuning", "llama-i", "llama paper"], "529": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "openai", "gpt 4", "gpt-4", "gpt4", "chatgpt4", "chatgpt 4", "gpt 4 paper"], "530": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "transformers", "longformer", "transformer long documents", "paper explained", "recurrent transformer", "transformer xl", "what is deep learning", "mit deep learning", "deep learning basics"], "531": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper"], "532": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "gpt4", "gpt 4", "recurrent neural network", "rwkv", "rvkw", "transformer alternative", "gpt-4", "deep learning tutorial", "what is deep learning"], "533": ["deep learning", "machine learning", "arxiv", "explained", "neural networks", "ai", "artificial intelligence", "paper", "what is deep learning", "deep learning tutorial", "watermarks", "watermarking", "ai watermarking", "dalle", "stable diffusion", "diffusion explained", "stable diffusion explained"], "534": null, "535": null, "536": null, "537": null, "538": null, "539": null, "540": null, "541": null, "542": null, "543": null, "544": null, "545": null, "546": null, "547": null, "548": null, "549": null, "550": null, "551": null, "552": null, "553": null, "554": null, "555": null, "556": null, "557": null, "558": null, "559": null, "560": null, "561": null, "562": null, "563": null, "564": null, "565": null, "566": null, "567": null, "568": null, "569": null, "570": null, "571": null, "572": null, "573": null, "574": null, "575": null, "576": null, "577": null, "578": null, "579": null, "580": null, "581": null, "582": null, "583": null, "584": null, "585": null, "586": null, "587": null, "588": null, "589": null, "590": null, "591": null, "592": null, "593": null, "594": null, "595": null, "596": null, "597": null, "598": null, "599": null, "600": null, "601": null, "602": null, "603": null, "604": null, "605": null, "606": null, "607": null, "608": null, "609": null, "610": null, "611": null, "612": null, "613": null, "614": null, "615": null, "616": null, "617": null, "618": null, "619": null, "620": null, "621": null, "622": null, "623": null, "624": null, "625": null, "626": null, "627": null, "628": null, "629": null, "630": null, "631": null, "632": null, "633": null, "634": null, "635": null, "636": null, "637": null, "638": null, "639": null, "640": null, "641": null, "642": null, "643": null, "644": null, "645": null, "646": null, "647": null, "648": null, "649": null, "650": null, "651": null, "652": null, "653": null, "654": null, "655": null, "656": null, "657": null, "658": null, "659": null, "660": null, "661": null, "662": null, "663": null, "664": null, "665": null, "666": null, "667": null, "668": null, "669": null, "670": null, "671": null, "672": null, "673": null, "674": null, "675": null, "676": ["p vs np", "probability", "machine learning", "ai", "neural networks", "data science", "programming", "statistics", "math", "mathematics", "number theory", "pi", "terry tao", "algebra", "calculus", "lecture", "analysis", "abstract algebra", "computer science", "professor", "harvard", "MIT", "stanford", "yale", "prime", "prime numbers", "fields institute", "hinton", "deep learning", "nips", "CLVR", "computer vision", "AI", "talk", "LSTM", "sutton", "bengio", "facebook", "google", "google brain", "alpha go", "ml", "cousera", "andrew ng", "geoffrey hinton", "toronto", "goodfellow"], "677": null, "678": null, "679": null, "680": null, "681": null, "682": ["singularity", "ai", "artificial intelligence", "deep learning", "machine learning", "deepmind", "robots", "robotics", "self-driving cars", "driverless cars", "Andrew Ng", "Coursera", "Stanford University"], "683": ["andrew ng", "deep learning", "machine learning", "coursera", "deeplearning.ai", "landing.ai", "stanford", "introducation to machine learning", "artificial intelligence", "agi", "ai", "ai podcast", "artificial intelligence podcast", "lex fridman", "lex podcast", "lex mit", "lex ai", "lex jre", "mit ai"], "684": null, "685": null, "686": null, "687": null, "688": null, "689": null, "690": null, "691": null, "692": null, "693": null, "694": null, "695": null, "696": null, "697": null, "698": null, "699": null, "700": null, "701": null, "702": null, "703": null, "704": null, "705": null, "706": null, "707": null, "708": null, "709": null, "710": null, "711": null, "712": null, "713": null, "714": null, "715": null, "716": null, "717": null, "718": null, "719": null, "720": null, "721": null, "722": null, "723": null, "724": null, "725": null, "726": null, "727": null, "728": null, "729": null, "730": null, "731": null, "732": null, "733": null, "734": null, "735": null, "736": null, "737": null, "738": null, "739": null, "740": null, "741": null, "742": null, "743": ["AI", "Machine learning", "NLP", "Andrew Ng", "Chris Manning", "GPT-3", "GloVe"], "744": ["NLP", "Deep Learning", "Machine Learning", "AI", "Andrew Ng"], "745": ["AI", "Machine Learning", "Data", "AI for Good", "Andrew Ng", "AI2"], "746": ["AI", "Machine Learning", "Google", "Chatbot", "Andrew Ng", "Quoc Le", "NLP"], "747": null, "748": null, "749": null, "750": null, "751": null, "752": null, "753": null, "754": null, "755": null, "756": null, "757": null, "758": null, "759": null, "760": null, "761": null, "762": null, "763": null, "764": null, "765": null, "766": null, "767": null, "768": null, "769": null, "770": null, "771": null, "772": null, "773": null, "774": null, "775": null, "776": null, "777": null, "778": null, "779": null, "780": null, "781": null, "782": null, "783": null, "784": null, "785": null, "786": null}, "publishedAt": {"0": "2020-07-26T13:00:23Z", "1": "2020-07-23T13:00:13Z", "2": "2020-07-19T13:00:18Z", "3": "2020-07-16T13:00:30Z", "4": "2020-07-14T13:00:04Z", "5": "2021-03-30T14:09:13Z", "6": "2021-12-18T12:09:06Z", "7": "2022-01-02T12:34:43Z", "8": "2022-01-11T21:17:53Z", "9": "2022-01-19T14:58:40Z", "10": "2022-01-21T16:20:45Z", "11": "2022-01-29T13:28:10Z", "12": "2022-02-04T16:49:40Z", "13": "2022-02-06T12:10:28Z", "14": "2022-02-08T21:13:05Z", "15": "2022-02-16T00:21:02Z", "16": "2022-02-16T14:23:01Z", "17": "2022-02-17T23:04:45Z", "18": "2022-02-20T16:48:36Z", "19": "2022-02-26T14:02:37Z", "20": "2022-02-28T15:24:13Z", "21": "2022-03-02T23:00:01Z", "22": "2022-03-06T16:27:24Z", "23": "2022-03-08T16:24:37Z", "24": "2022-03-14T14:01:38Z", "25": "2022-03-20T09:23:17Z", "26": "2022-03-24T21:27:22Z", "27": "2022-03-26T15:59:28Z", "28": "2022-03-29T15:09:06Z", "29": "2022-04-02T22:38:55Z", "30": "2022-04-04T13:13:08Z", "31": "2022-04-17T22:14:08Z", "32": "2022-04-21T08:40:35Z", "33": "2022-04-22T13:00:44Z", "34": "2022-04-26T20:23:52Z", "35": "2022-05-02T13:12:13Z", "36": "2022-08-26T18:25:39Z", "37": "2022-09-13T21:33:39Z", "38": "2021-12-28T22:31:56Z", "39": "2022-06-23T22:38:33Z", "40": "2022-08-07T12:54:12Z", "41": "2022-08-13T10:52:40Z", "42": "2023-06-06T19:55:30Z", "43": "2018-04-07T13:36:31Z", "44": "2018-03-18T12:07:52Z", "45": "2017-08-28T06:53:30Z", "46": "2017-08-09T06:02:41Z", "47": "2017-08-04T07:15:56Z", "48": "2019-05-10T12:47:57Z", "49": "2019-08-08T08:56:43Z", "50": "2019-10-15T16:34:32Z", "51": "2019-11-01T13:08:17Z", "52": "2019-11-02T11:23:04Z", "53": "2019-11-07T10:15:19Z", "54": "2019-11-21T12:23:05Z", "55": "2019-12-11T02:45:34Z", "56": "2020-01-10T12:36:40Z", "57": "2020-03-31T15:49:49Z", "58": "2020-04-03T12:46:59Z", "59": "2020-04-06T14:33:52Z", "60": "2020-04-10T10:11:09Z", "61": "2020-04-11T11:04:57Z", "62": "2020-04-16T18:07:00Z", "63": "2020-04-23T13:26:07Z", "64": "2020-04-30T16:17:55Z", "65": "2020-05-04T13:29:02Z", "66": "2020-05-06T13:02:50Z", "67": "2020-05-08T13:10:29Z", "68": "2020-05-17T14:01:30Z", "69": "2020-05-20T13:52:49Z", "70": "2020-05-24T15:31:04Z", "71": "2020-06-01T15:47:00Z", "72": "2020-08-04T12:00:15Z", "73": "2020-07-26T13:00:23Z", "74": "2020-08-20T09:46:35Z", "75": "2020-08-12T09:20:25Z", "76": "2020-08-23T13:06:02Z", "77": "2020-09-07T11:56:46Z", "78": "2017-04-15T13:02:08Z", "79": "2020-09-13T19:56:31Z", "80": "2020-12-16T13:55:47Z", "81": "2021-02-19T16:11:18Z", "82": "2021-05-29T16:35:08Z", "83": "2021-06-05T16:54:02Z", "84": "2021-06-19T15:07:08Z", "85": "2021-11-03T15:54:39Z", "86": "2022-01-02T12:34:43Z", "87": "2022-02-26T14:02:37Z", "88": "2022-02-28T15:24:13Z", "89": "2022-03-08T16:24:37Z", "90": "2022-04-01T18:51:58Z", "91": "2022-04-02T22:38:55Z", "92": "2022-04-25T18:40:16Z", "93": "2022-04-26T20:23:52Z", "94": "2022-04-30T16:26:42Z", "95": "2022-05-02T13:12:13Z", "96": "2022-10-07T22:45:51Z", "97": "2022-11-25T23:23:37Z", "98": "2019-02-18T16:11:42Z", "99": "2019-01-30T08:51:15Z", "100": "2018-12-21T13:02:52Z", "101": "2017-11-28T08:04:38Z", "102": "2019-07-03T10:51:44Z", "103": "2019-09-03T15:05:28Z", "104": "2019-10-15T16:34:32Z", "105": "2020-01-22T10:01:26Z", "106": "2020-02-11T14:56:20Z", "107": "2020-02-24T16:55:45Z", "108": "2020-04-07T11:40:47Z", "109": "2020-04-14T12:47:57Z", "110": "2020-04-20T14:07:56Z", "111": "2020-05-03T15:02:03Z", "112": "2020-05-05T13:41:35Z", "113": "2020-05-18T13:26:18Z", "114": "2020-05-21T16:34:09Z", "115": "2020-05-22T17:45:44Z", "116": "2020-05-29T15:13:36Z", "117": "2020-05-31T14:27:04Z", "118": "2020-06-04T20:06:40Z", "119": "2020-06-07T14:11:39Z", "120": "2020-06-09T14:07:27Z", "121": "2020-06-11T13:50:50Z", "122": "2020-06-12T17:28:15Z", "123": "2020-06-13T14:52:28Z", "124": "2020-07-01T14:21:03Z", "125": "2020-07-02T13:27:53Z", "126": "2020-07-04T12:39:13Z", "127": "2020-07-16T13:00:30Z", "128": "2020-08-02T07:49:12Z", "129": "2020-08-09T15:51:38Z", "130": "2020-08-14T15:50:20Z", "131": "2020-09-07T11:56:46Z", "132": "2020-10-26T16:57:35Z", "133": "2020-11-02T14:02:08Z", "134": "2020-12-26T19:42:56Z", "135": "2021-01-06T13:49:24Z", "136": "2021-01-12T14:52:03Z", "137": "2021-01-22T19:38:10Z", "138": "2021-02-02T15:36:12Z", "139": "2021-02-11T10:54:06Z", "140": "2021-02-25T17:26:39Z", "141": "2021-09-02T22:35:08Z", "142": "2021-09-06T12:07:08Z", "143": "2021-09-21T13:43:28Z", "144": "2021-10-24T20:59:32Z", "145": "2021-12-02T01:21:29Z", "146": "2022-02-04T16:49:40Z", "147": "2022-02-08T21:13:05Z", "148": "2022-02-17T23:04:45Z", "149": "2022-03-01T13:15:04Z", "150": "2022-03-25T20:08:04Z", "151": "2022-03-26T15:59:28Z", "152": "2022-03-28T13:48:00Z", "153": "2022-03-29T15:09:06Z", "154": "2022-04-16T15:40:13Z", "155": "2022-04-17T22:14:08Z", "156": "2022-04-21T08:40:35Z", "157": "2022-04-30T16:26:42Z", "158": "2022-05-02T13:12:13Z", "159": "2022-06-15T22:16:05Z", "160": "2022-11-04T16:34:43Z", "161": "2022-11-19T15:44:28Z", "162": "2022-12-07T22:54:46Z", "163": "2023-02-04T14:08:23Z", "164": "2023-03-02T21:30:56Z", "165": "2023-03-15T09:26:25Z", "166": "2023-04-27T21:37:31Z", "167": "2023-05-23T11:21:44Z", "168": "2023-06-02T22:38:25Z", "169": "2019-05-14T13:45:57Z", "170": "2020-04-07T11:40:47Z", "171": "2020-04-18T13:29:14Z", "172": "2020-12-26T19:42:56Z", "173": "2021-06-27T12:06:18Z", "174": "2019-06-12T22:33:17Z", "175": "2019-07-05T08:04:42Z", "176": "2019-08-05T10:08:25Z", "177": "2019-10-07T12:35:36Z", "178": "2019-10-31T18:04:16Z", "179": "2020-04-13T14:51:36Z", "180": "2020-04-15T18:01:56Z", "181": "2020-04-21T12:19:54Z", "182": "2020-04-17T09:47:23Z", "183": "2020-04-18T13:29:14Z", "184": "2020-04-20T13:40:41Z", "185": "2020-04-24T11:59:41Z", "186": "2020-04-27T12:42:47Z", "187": "2020-04-29T14:04:08Z", "188": "2020-05-09T13:28:25Z", "189": "2020-05-11T13:59:56Z", "190": "2020-05-13T13:56:56Z", "191": "2020-05-26T14:18:55Z", "192": "2020-05-19T13:37:21Z", "193": "2020-05-27T14:13:12Z", "194": "2020-06-02T12:43:55Z", "195": "2020-06-19T13:32:01Z", "196": "2020-07-03T14:23:17Z", "197": "2020-06-26T13:49:27Z", "198": "2020-06-27T16:04:14Z", "199": "2020-07-29T14:29:26Z", "200": "2020-07-01T14:21:03Z", "201": "2020-07-06T13:23:54Z", "202": "2020-07-07T17:16:53Z", "203": "2020-07-08T13:36:15Z", "204": "2020-07-11T12:57:17Z", "205": "2020-07-21T13:00:01Z", "206": "2020-08-09T15:51:38Z", "207": "2020-08-26T12:31:39Z", "208": "2020-09-02T09:29:27Z", "209": "2020-10-03T16:06:32Z", "210": "2020-10-11T12:46:39Z", "211": "2020-11-10T21:48:01Z", "212": "2020-11-29T14:04:54Z", "213": "2021-02-04T14:36:15Z", "214": "2021-03-05T13:18:21Z", "215": "2021-03-16T18:15:38Z", "216": "2021-06-11T21:14:12Z", "217": "2021-08-23T12:40:24Z", "218": "2021-10-06T20:12:52Z", "219": "2021-11-16T00:36:11Z", "220": "2021-11-20T15:47:46Z", "221": "2021-11-27T17:34:17Z", "222": "2022-01-29T13:28:10Z", "223": "2022-03-13T12:50:48Z", "224": "2022-03-14T14:01:38Z", "225": "2022-03-18T12:14:08Z", "226": "2022-03-20T09:23:17Z", "227": "2022-04-22T13:00:44Z", "228": "2022-07-06T22:57:03Z", "229": "2022-09-17T12:19:00Z", "230": "2022-10-07T22:45:51Z", "231": "2022-10-21T11:28:27Z", "232": "2019-02-19T05:12:20Z", "233": "2019-02-18T16:11:42Z", "234": "2019-01-30T08:51:15Z", "235": "2019-01-09T13:33:22Z", "236": "2017-11-28T08:04:38Z", "237": "2017-08-04T07:15:56Z", "238": "2019-05-14T13:45:57Z", "239": "2019-07-03T10:51:44Z", "240": "2019-08-09T11:04:40Z", "241": "2019-09-04T14:02:31Z", "242": "2019-09-05T14:30:00Z", "243": "2019-11-02T11:23:04Z", "244": "2019-11-21T12:23:05Z", "245": "2019-12-11T02:45:34Z", "246": "2020-01-22T10:01:26Z", "247": "2020-05-07T14:18:26Z", "248": "2020-05-11T13:59:56Z", "249": "2020-05-28T15:09:01Z", "250": "2020-06-24T13:48:00Z", "251": "2020-12-24T23:18:43Z", "252": "2021-01-06T13:49:24Z", "253": "2021-01-17T09:55:02Z", "254": "2021-02-27T15:47:03Z", "255": "2021-03-06T16:13:09Z", "256": "2021-03-30T14:09:13Z", "257": "2021-04-14T13:33:43Z", "258": "2021-04-27T14:34:28Z", "259": "2021-05-18T14:47:54Z", "260": "2021-10-21T13:58:08Z", "261": "2022-05-05T19:17:09Z", "262": "2022-09-02T20:16:53Z", "263": "2022-10-07T22:45:51Z", "264": "2022-12-07T22:54:46Z", "265": "2022-06-03T15:25:58Z", "266": "2022-06-15T22:16:05Z", "267": "2023-02-04T14:08:23Z", "268": "2023-04-06T22:58:51Z", "269": "2023-04-15T16:47:58Z", "270": "2019-02-19T05:12:20Z", "271": "2019-02-20T05:12:21Z", "272": "2019-02-02T15:16:38Z", "273": "2018-12-21T13:02:52Z", "274": "2018-12-18T09:18:47Z", "275": "2017-11-28T08:04:38Z", "276": "2019-08-09T11:04:40Z", "277": "2019-08-12T13:44:37Z", "278": "2019-08-13T14:03:03Z", "279": "2019-09-04T14:02:31Z", "280": "2019-11-03T13:23:12Z", "281": "2020-01-22T10:01:26Z", "282": "2020-02-12T08:36:06Z", "283": "2020-02-24T16:55:45Z", "284": "2020-03-30T12:40:26Z", "285": "2020-04-06T14:33:52Z", "286": "2020-04-09T11:10:44Z", "287": "2020-04-13T14:51:36Z", "288": "2020-04-15T18:01:56Z", "289": "2020-04-17T09:47:23Z", "290": "2020-04-20T14:07:56Z", "291": "2020-05-02T11:06:26Z", "292": "2020-05-09T13:28:25Z", "293": "2020-05-12T14:05:56Z", "294": "2020-05-15T14:11:14Z", "295": "2020-05-28T15:09:01Z", "296": "2020-05-31T14:27:04Z", "297": "2020-06-03T13:22:28Z", "298": "2020-06-06T13:56:12Z", "299": "2020-06-10T14:14:52Z", "300": "2020-06-11T13:50:50Z", "301": "2020-06-12T17:28:15Z", "302": "2020-06-14T13:48:08Z", "303": "2020-06-15T14:13:34Z", "304": "2020-06-16T14:16:24Z", "305": "2020-06-21T13:09:30Z", "306": "2020-06-23T13:54:48Z", "307": "2020-06-25T14:09:09Z", "308": "2020-06-27T16:04:14Z", "309": "2020-06-28T15:09:19Z", "310": "2020-06-29T12:58:05Z", "311": "2020-06-30T17:10:35Z", "312": "2020-07-01T14:21:03Z", "313": "2020-07-05T15:25:42Z", "314": "2020-07-04T12:39:13Z", "315": "2020-07-07T17:16:53Z", "316": "2020-07-08T13:36:15Z", "317": "2020-07-09T16:52:06Z", "318": "2020-07-10T13:01:46Z", "319": "2020-07-23T13:00:13Z", "320": "2020-07-19T13:00:18Z", "321": "2020-07-14T13:00:04Z", "322": "2020-07-21T13:00:01Z", "323": "2020-08-02T07:49:12Z", "324": "2020-08-09T15:51:38Z", "325": "2020-08-28T13:06:20Z", "326": "2020-10-04T11:22:34Z", "327": "2020-10-17T13:31:53Z", "328": "2020-10-26T16:57:35Z", "329": "2020-11-22T13:50:36Z", "330": "2021-01-22T19:38:10Z", "331": "2021-02-02T15:36:12Z", "332": "2021-02-14T16:51:10Z", "333": "2021-02-17T16:02:54Z", "334": "2021-02-26T16:12:24Z", "335": "2021-02-27T15:47:03Z", "336": "2021-03-22T17:26:39Z", "337": "2021-05-06T11:53:49Z", "338": "2021-05-08T10:17:15Z", "339": "2021-05-24T12:24:30Z", "340": "2021-08-26T13:32:01Z", "341": "2021-11-10T13:24:24Z", "342": "2021-12-02T01:21:29Z", "343": "2021-12-08T18:54:58Z", "344": "2022-01-19T14:58:40Z", "345": "2022-01-21T16:20:45Z", "346": "2022-02-16T00:21:02Z", "347": "2022-02-17T23:04:45Z", "348": "2022-03-23T19:44:37Z", "349": "2021-08-08T18:06:01Z", "350": "2022-03-24T21:27:22Z", "351": "2022-04-21T08:40:35Z", "352": "2023-06-02T22:38:25Z", "353": "2020-05-01T17:22:06Z", "354": "2020-05-18T13:26:18Z", "355": "2020-04-27T12:42:47Z", "356": "2020-05-02T11:06:26Z", "357": "2020-05-05T13:41:35Z", "358": "2020-05-06T13:02:50Z", "359": "2020-05-08T13:10:29Z", "360": "2020-05-09T13:28:25Z", "361": "2020-05-11T13:59:56Z", "362": "2020-05-12T14:05:56Z", "363": "2020-05-13T13:56:56Z", "364": "2020-05-15T14:11:14Z", "365": "2020-05-26T14:18:55Z", "366": "2020-05-19T13:37:21Z", "367": "2020-05-27T14:13:12Z", "368": "2020-05-17T14:01:30Z", "369": "2020-05-25T15:38:17Z", "370": "2020-05-20T13:52:49Z", "371": "2020-05-22T17:45:44Z", "372": "2020-05-24T15:31:04Z", "373": "2020-05-28T15:09:01Z", "374": "2020-05-29T15:13:36Z", "375": "2020-05-31T14:27:04Z", "376": "2020-06-01T15:47:00Z", "377": "2020-06-02T12:43:55Z", "378": "2020-06-03T13:22:28Z", "379": "2020-06-05T13:31:46Z", "380": "2020-06-06T13:56:12Z", "381": "2020-06-07T14:11:39Z", "382": "2020-06-09T14:07:27Z", "383": "2020-06-10T14:14:52Z", "384": "2020-06-11T13:50:50Z", "385": "2020-06-12T17:28:15Z", "386": "2020-06-19T13:32:01Z", "387": "2020-07-03T14:23:17Z", "388": "2020-06-26T13:49:27Z", "389": "2020-08-04T12:00:15Z", "390": "2020-06-13T14:52:28Z", "391": "2020-06-14T13:48:08Z", "392": "2020-06-15T14:13:34Z", "393": "2020-06-16T14:16:24Z", "394": "2020-06-17T13:48:43Z", "395": "2020-06-18T13:24:24Z", "396": "2020-06-20T14:03:33Z", "397": "2020-06-21T13:09:30Z", "398": "2020-06-23T13:54:48Z", "399": "2020-06-24T13:48:00Z", "400": "2020-06-25T14:09:09Z", "401": "2020-06-27T16:04:14Z", "402": "2020-06-28T15:09:19Z", "403": "2020-06-29T12:58:05Z", "404": "2020-07-29T14:29:26Z", "405": "2020-06-04T20:06:40Z", "406": "2020-06-30T17:10:35Z", "407": "2020-07-01T14:21:03Z", "408": "2020-07-02T13:27:53Z", "409": "2020-07-05T15:25:42Z", "410": "2020-07-04T12:39:13Z", "411": "2020-07-07T17:16:53Z", "412": "2020-07-08T13:36:15Z", "413": "2020-07-09T16:52:06Z", "414": "2020-07-10T13:01:46Z", "415": "2020-07-11T12:57:17Z", "416": "2020-07-26T13:00:23Z", "417": "2020-07-23T13:00:13Z", "418": "2020-07-19T13:00:18Z", "419": "2020-07-16T13:00:30Z", "420": "2020-07-14T13:00:04Z", "421": "2020-07-21T13:00:01Z", "422": "2020-08-20T09:46:35Z", "423": "2020-08-09T15:51:38Z", "424": "2020-08-12T09:20:25Z", "425": "2020-08-14T15:50:20Z", "426": "2020-08-23T13:06:02Z", "427": "2020-08-26T12:31:39Z", "428": "2020-08-28T13:06:20Z", "429": "2020-09-02T09:29:27Z", "430": "2020-09-07T11:56:46Z", "431": "2020-09-18T20:27:55Z", "432": "2020-10-03T16:06:32Z", "433": "2020-10-04T11:22:34Z", "434": "2020-10-11T12:46:39Z", "435": "2020-10-17T13:31:53Z", "436": "2020-10-26T16:57:35Z", "437": "2020-11-02T14:02:08Z", "438": "2020-11-10T21:48:01Z", "439": "2020-11-22T13:50:36Z", "440": "2020-11-29T14:04:54Z", "441": "2020-12-01T15:31:42Z", "442": "2020-12-16T13:55:47Z", "443": "2020-12-26T19:42:56Z", "444": "2021-01-06T13:49:24Z", "445": "2021-01-12T14:52:03Z", "446": "2021-01-22T19:38:10Z", "447": "2021-01-29T21:13:21Z", "448": "2021-02-02T15:36:12Z", "449": "2021-02-04T14:36:15Z", "450": "2021-02-11T10:54:06Z", "451": "2021-02-14T16:51:10Z", "452": "2021-02-17T16:02:54Z", "453": "2021-02-19T16:11:18Z", "454": "2021-02-25T17:26:39Z", "455": "2021-02-26T16:12:24Z", "456": "2021-02-27T15:47:03Z", "457": "2021-03-05T13:18:21Z", "458": "2021-03-11T17:13:56Z", "459": "2021-03-16T18:15:38Z", "460": "2020-08-26T22:38:15Z", "461": "2021-03-22T17:26:39Z", "462": "2021-04-11T14:00:49Z", "463": "2021-04-19T15:21:29Z", "464": "2021-04-30T16:02:42Z", "465": "2021-05-01T19:53:03Z", "466": "2021-05-08T10:17:15Z", "467": "2021-05-15T12:59:43Z", "468": "2021-05-24T12:24:30Z", "469": "2021-05-29T16:35:08Z", "470": "2021-05-31T13:27:21Z", "471": "2021-06-05T16:54:02Z", "472": "2021-06-11T21:14:12Z", "473": "2021-06-19T15:07:08Z", "474": "2021-06-23T12:25:53Z", "475": "2021-06-27T12:06:18Z", "476": "2021-08-16T11:34:59Z", "477": "2021-08-23T12:40:24Z", "478": "2021-08-26T13:32:01Z", "479": "2021-09-02T22:35:08Z", "480": "2021-09-06T12:07:08Z", "481": "2021-09-20T20:01:59Z", "482": "2021-09-21T13:43:28Z", "483": "2021-09-27T13:22:14Z", "484": "2021-10-02T14:24:54Z", "485": "2021-10-06T20:12:52Z", "486": "2021-10-24T20:59:32Z", "487": "2021-11-03T15:54:39Z", "488": "2021-11-10T13:24:24Z", "489": "2021-11-16T00:36:11Z", "490": "2021-11-20T15:47:46Z", "491": "2021-11-27T17:34:17Z", "492": "2021-12-02T01:21:29Z", "493": "2021-12-08T18:54:58Z", "494": "2021-12-18T12:09:06Z", "495": "2021-12-28T22:31:56Z", "496": "2022-01-02T12:34:43Z", "497": "2022-01-11T21:17:53Z", "498": "2022-01-19T14:58:40Z", "499": "2022-01-21T16:20:45Z", "500": "2022-01-29T13:28:10Z", "501": "2022-02-06T12:10:28Z", "502": "2022-02-08T21:13:05Z", "503": "2022-02-16T00:21:02Z", "504": "2022-02-16T14:23:01Z", "505": "2022-02-17T23:04:45Z", "506": "2022-02-26T14:02:37Z", "507": "2022-03-01T13:15:04Z", "508": "2022-03-05T14:58:01Z", "509": "2022-03-08T16:24:37Z", "510": "2022-03-13T12:50:48Z", "511": "2022-03-14T14:01:38Z", "512": "2022-03-18T12:14:08Z", "513": "2022-03-23T19:44:37Z", "514": "2022-03-25T20:08:04Z", "515": "2022-03-28T13:48:00Z", "516": "2022-04-01T18:51:58Z", "517": "2022-04-16T15:40:13Z", "518": "2022-04-25T18:40:16Z", "519": "2022-04-30T16:26:42Z", "520": "2022-06-23T22:38:33Z", "521": "2022-06-26T21:58:34Z", "522": "2022-07-06T22:57:03Z", "523": "2022-10-07T22:45:51Z", "524": "2022-10-21T11:28:27Z", "525": "2022-11-04T16:34:43Z", "526": "2022-11-19T15:44:28Z", "527": "2022-11-25T23:23:37Z", "528": "2023-03-02T21:30:56Z", "529": "2023-03-15T09:26:25Z", "530": "2023-04-27T21:37:31Z", "531": "2023-05-23T11:21:44Z", "532": "2023-06-02T22:38:25Z", "533": "2023-06-06T19:55:30Z", "534": "2017-08-25T20:22:46Z", "535": "2017-08-25T20:22:48Z", "536": "2017-08-25T20:22:48Z", "537": "2017-08-25T20:22:49Z", "538": "2017-08-25T20:22:49Z", "539": "2017-08-25T20:22:47Z", "540": "2017-08-25T20:22:45Z", "541": "2017-08-25T20:22:46Z", "542": "2017-08-25T20:22:46Z", "543": "2017-08-25T20:22:47Z", "544": "2017-08-25T20:22:48Z", "545": "2017-08-25T20:22:48Z", "546": "2017-08-25T20:22:29Z", "547": "2017-08-25T20:22:30Z", "548": "2017-08-25T20:22:31Z", "549": "2017-08-25T20:22:31Z", "550": "2017-08-25T20:22:31Z", "551": "2017-08-25T20:22:32Z", "552": "2017-08-25T20:22:32Z", "553": "2017-08-25T20:22:33Z", "554": "2017-08-25T20:22:34Z", "555": "2017-08-25T20:22:34Z", "556": "2017-08-25T20:22:30Z", "557": "2017-08-25T20:22:32Z", "558": "2017-08-25T20:22:24Z", "559": "2017-08-25T20:22:33Z", "560": "2017-08-25T20:22:33Z", "561": "2017-08-25T20:22:34Z", "562": "2017-08-25T20:22:35Z", "563": "2017-08-25T20:22:41Z", "564": "2017-08-25T20:22:42Z", "565": "2017-08-25T20:22:43Z", "566": "2017-08-25T20:22:31Z", "567": "2017-08-25T20:22:44Z", "568": "2017-08-25T20:22:56Z", "569": "2017-08-25T20:22:57Z", "570": "2017-08-25T20:23:04Z", "571": "2017-08-25T20:23:05Z", "572": "2017-08-25T20:23:05Z", "573": "2017-08-25T20:22:55Z", "574": "2017-08-25T20:23:06Z", "575": "2017-08-25T20:23:07Z", "576": "2017-08-25T20:23:07Z", "577": "2017-08-25T20:23:07Z", "578": "2017-08-25T20:23:08Z", "579": "2017-08-25T20:23:06Z", "580": "2017-08-25T20:23:07Z", "581": "2017-08-25T20:23:08Z", "582": "2017-08-25T20:23:04Z", "583": "2017-08-25T20:23:05Z", "584": "2017-08-25T20:23:05Z", "585": "2017-08-25T20:23:06Z", "586": "2017-08-25T20:23:08Z", "587": "2017-08-25T20:23:09Z", "588": "2017-08-25T20:23:05Z", "589": "2017-08-25T20:23:05Z", "590": "2017-08-25T20:23:06Z", "591": "2017-08-25T20:23:07Z", "592": "2017-08-25T20:22:43Z", "593": "2017-08-25T20:22:44Z", "594": "2017-08-25T20:22:45Z", "595": "2017-08-25T20:22:46Z", "596": "2017-08-25T20:22:44Z", "597": "2017-08-25T20:22:44Z", "598": "2017-08-25T20:22:45Z", "599": "2017-08-25T20:22:46Z", "600": "2017-08-25T20:22:44Z", "601": "2017-08-25T20:22:45Z", "602": "2017-08-25T20:22:46Z", "603": "2017-08-25T20:22:46Z", "604": "2017-08-25T20:22:48Z", "605": "2017-08-25T20:22:44Z", "606": "2017-08-25T20:22:44Z", "607": "2017-08-25T20:22:45Z", "608": "2017-08-25T20:22:19Z", "609": "2017-08-25T20:22:46Z", "610": "2017-08-25T20:22:47Z", "611": "2017-08-25T20:22:46Z", "612": "2017-08-25T20:22:48Z", "613": "2017-08-25T20:22:48Z", "614": "2017-08-25T20:22:49Z", "615": "2017-08-25T20:22:49Z", "616": "2017-08-25T20:22:47Z", "617": "2017-08-25T20:22:45Z", "618": "2017-08-25T20:22:46Z", "619": "2017-08-25T20:22:46Z", "620": "2017-08-25T20:22:47Z", "621": "2017-08-25T20:22:48Z", "622": "2017-08-25T20:22:48Z", "623": "2017-08-25T20:22:29Z", "624": "2017-08-25T20:22:30Z", "625": "2017-08-25T20:22:31Z", "626": "2017-08-25T20:22:31Z", "627": "2017-08-25T20:22:31Z", "628": "2017-08-25T20:22:32Z", "629": "2017-08-25T20:22:32Z", "630": "2017-08-25T20:22:33Z", "631": "2017-08-25T20:22:34Z", "632": "2017-08-25T20:22:34Z", "633": "2017-08-25T20:22:30Z", "634": "2017-08-25T20:22:32Z", "635": "2017-08-25T20:22:24Z", "636": "2017-08-25T20:22:33Z", "637": "2017-08-25T20:22:33Z", "638": "2017-08-25T20:22:34Z", "639": "2017-08-25T20:22:35Z", "640": "2017-08-25T20:22:41Z", "641": "2017-08-25T20:22:42Z", "642": "2017-08-25T20:22:43Z", "643": "2017-08-25T20:22:31Z", "644": "2017-08-25T20:22:44Z", "645": "2017-08-25T20:23:10Z", "646": "2017-08-25T20:23:11Z", "647": "2017-08-25T20:23:11Z", "648": "2017-08-25T20:23:12Z", "649": "2017-08-25T20:23:12Z", "650": "2017-08-25T20:23:13Z", "651": "2017-08-25T20:23:13Z", "652": "2017-08-25T20:23:14Z", "653": "2017-08-25T20:23:15Z", "654": "2017-08-25T20:23:15Z", "655": "2017-08-25T20:23:16Z", "656": "2017-08-25T20:23:16Z", "657": "2017-08-25T20:23:18Z", "658": "2017-08-25T20:23:18Z", "659": "2017-08-25T20:23:19Z", "660": "2017-08-25T20:23:19Z", "661": "2017-08-25T20:22:52Z", "662": "2017-08-25T20:22:53Z", "663": "2017-08-25T20:22:53Z", "664": "2017-08-25T20:22:54Z", "665": "2017-08-25T20:22:54Z", "666": "2017-08-25T20:22:55Z", "667": "2017-08-25T20:22:21Z", "668": "2017-08-25T20:22:20Z", "669": "2017-08-25T20:22:22Z", "670": "2017-08-25T20:22:21Z", "671": "2017-08-25T20:22:22Z", "672": "2017-08-25T20:22:23Z", "673": "2018-04-04T23:40:12Z", "674": "2018-09-28T20:06:40Z", "675": "2020-02-03T20:04:32Z", "676": "2017-08-08T23:06:44Z", "677": "2020-02-24T19:31:41Z", "678": "2019-10-14T22:37:25Z", "679": "2019-09-16T19:47:19Z", "680": "2019-09-09T23:43:39Z", "681": "2019-07-31T01:26:47Z", "682": "2017-12-15T22:00:01Z", "683": "2020-02-20T17:16:01Z", "684": "2020-01-29T23:37:45Z", "685": "2020-01-28T01:02:39Z", "686": "2020-01-20T19:07:11Z", "687": "2019-09-24T23:29:46Z", "688": "2020-02-10T19:48:13Z", "689": "2020-02-12T19:12:55Z", "690": "2020-03-09T18:54:11Z", "691": "2020-03-31T18:00:16Z", "692": "2020-04-07T18:35:05Z", "693": "2020-04-09T18:38:45Z", "694": "2020-04-14T18:23:26Z", "695": "2018-02-05T19:50:14Z", "696": "2018-02-05T19:50:14Z", "697": "2018-02-05T19:50:14Z", "698": "2018-02-05T19:50:14Z", "699": "2018-02-05T19:50:16Z", "700": "2018-02-05T19:50:15Z", "701": "2017-11-07T19:14:40Z", "702": "2017-11-07T19:14:40Z", "703": "2017-11-07T19:14:40Z", "704": "2017-11-07T19:14:40Z", "705": "2017-11-07T19:14:40Z", "706": "2017-11-07T19:14:40Z", "707": "2017-11-07T19:14:40Z", "708": "2017-11-07T19:14:40Z", "709": "2017-11-07T19:14:40Z", "710": "2017-11-07T19:14:40Z", "711": "2017-11-07T19:14:40Z", "712": "2017-11-07T19:26:10Z", "713": "2017-11-07T19:26:10Z", "714": "2017-11-07T19:26:10Z", "715": "2017-11-07T19:26:10Z", "716": "2017-11-07T19:26:10Z", "717": "2017-11-07T19:26:10Z", "718": "2017-11-07T19:26:10Z", "719": "2017-11-07T19:26:10Z", "720": "2017-11-07T19:26:10Z", "721": "2017-11-07T19:26:10Z", "722": "2017-11-07T19:26:10Z", "723": "2017-11-07T19:34:58Z", "724": "2017-11-07T19:34:58Z", "725": "2017-11-07T19:34:58Z", "726": "2017-11-07T19:34:58Z", "727": "2017-11-07T19:34:58Z", "728": "2017-11-07T19:34:58Z", "729": "2017-11-07T19:34:58Z", "730": "2017-11-07T19:34:58Z", "731": "2017-11-07T19:34:58Z", "732": "2017-11-07T19:46:13Z", "733": "2017-11-07T19:46:13Z", "734": "2017-11-07T19:46:13Z", "735": "2017-11-07T19:46:13Z", "736": "2017-11-07T19:46:13Z", "737": "2017-11-07T19:46:13Z", "738": "2017-11-07T19:46:13Z", "739": "2017-11-07T19:46:13Z", "740": "2017-11-07T19:46:13Z", "741": "2017-11-07T19:46:13Z", "742": "2017-11-07T19:46:13Z", "743": "2020-10-14T06:43:47Z", "744": "2020-10-13T16:08:53Z", "745": "2020-10-13T23:48:15Z", "746": "2020-10-13T20:35:40Z", "747": "2022-04-20T23:59:50Z", "748": "2022-04-20T23:59:53Z", "749": "2022-04-20T23:59:57Z", "750": "2022-04-21T00:00:04Z", "751": "2022-04-21T00:00:07Z", "752": "2022-04-21T00:00:10Z", "753": "2022-04-21T00:00:17Z", "754": "2022-04-21T00:00:22Z", "755": "2022-04-21T00:00:24Z", "756": "2022-04-21T00:00:29Z", "757": "2022-04-21T00:03:26Z", "758": "2022-04-21T00:03:23Z", "759": "2022-04-21T00:03:20Z", "760": "2022-04-21T00:03:18Z", "761": "2022-04-21T00:03:13Z", "762": "2022-04-21T00:03:09Z", "763": "2022-04-21T00:03:05Z", "764": "2022-04-21T00:03:02Z", "765": "2022-04-21T00:03:00Z", "766": "2022-04-21T00:02:53Z", "767": "2022-04-21T00:02:56Z", "768": "2022-04-21T00:02:51Z", "769": "2022-04-21T00:02:47Z", "770": "2022-04-21T00:02:44Z", "771": "2022-04-21T00:02:40Z", "772": "2022-04-21T00:02:34Z", "773": "2022-04-21T00:02:30Z", "774": "2022-04-21T00:02:27Z", "775": "2022-04-21T00:02:22Z", "776": "2022-04-21T00:02:19Z", "777": "2022-04-21T00:02:17Z", "778": "2022-04-21T00:02:14Z", "779": "2022-04-21T00:02:09Z", "780": "2022-04-21T00:02:06Z", "781": "2022-04-21T00:02:03Z", "782": "2022-04-21T00:01:58Z", "783": "2022-04-21T00:01:55Z", "784": "2022-04-21T00:00:57Z", "785": "2022-04-21T00:00:53Z", "786": "2022-04-21T00:00:50Z"}, "viewCount": {"0": 35348, "1": 32031, "2": 49478, "3": 19370, "4": 57751, "5": 68754, "6": 16426, "7": 17615, "8": 12825, "9": 11394, "10": 14538, "11": 18093, "12": 20775, "13": 27127, "14": 15465, "15": 16838, "16": 7592, "17": 13128, "18": 61745, "19": 11319, "20": 4133, "21": 11862, "22": 7530, "23": 6953, "24": 6477, "25": 6922, "26": 4529, "27": 8753, "28": 2954, "29": 4109, "30": 5659, "31": 7025, "32": 11462, "33": 17550, "34": 3952, "35": 5817, "36": 38517, "37": 19357, "38": 31515, "39": 23624, "40": 27755, "41": 140538, "42": 18828, "43": 11107, "44": 7400, "45": 3696, "46": 4329, "47": 7896, "48": 3441, "49": 3149, "50": 2205, "51": 5600, "52": 17554, "53": 2899, "54": 22292, "55": 9115, "56": 3161, "57": 8367, "58": 6366, "59": 2976, "60": 1728, "61": 11080, "62": 4532, "63": 2849, "64": 21421, "65": 10344, "66": 6612, "67": 4158, "68": 5927, "69": 2473, "70": 5171, "71": 7168, "72": 7351, "73": 35348, "74": 8694, "75": 11474, "76": 10284, "77": 17976, "78": 29126794, "79": 5984, "80": 32640, "81": 22150, "82": 9172, "83": 46392, "84": 8478, "85": 21931, "86": 17615, "87": 11319, "88": 4133, "89": 6953, "90": 9097, "91": 4109, "92": 10089, "93": 3952, "94": 12000, "95": 5817, "96": 174289, "97": 36727, "98": 25495, "99": 92113, "100": 3079, "101": 503578, "102": 21125, "103": 21176, "104": 2205, "105": 18491, "106": 10735, "107": 5273, "108": 2229, "109": 1797, "110": 18336, "111": 15125, "112": 7780, "113": 38349, "114": 91042, "115": 29641, "116": 204187, "117": 16003, "118": 4550, "119": 6464, "120": 144924, "121": 27893, "122": 5958, "123": 6775, "124": 14945, "125": 13628, "126": 23779, "127": 19370, "128": 21782, "129": 77040, "130": 11185, "131": 17976, "132": 51536, "133": 29589, "134": 18389, "135": 101511, "136": 85693, "137": 26056, "138": 14815, "139": 16036, "140": 16146, "141": 15293, "142": 29740, "143": 19107, "144": 22488, "145": 20742, "146": 20775, "147": 15465, "148": 13128, "149": 10334, "150": 17715, "151": 8753, "152": 9579, "153": 2954, "154": 23253, "155": 7025, "156": 11462, "157": 12000, "158": 5817, "159": 884870, "160": 31868, "161": 42773, "162": 433642, "163": 122955, "164": 75345, "165": 105673, "166": 50621, "167": 73167, "168": 44345, "169": 10551, "170": 2229, "171": 9857, "172": 18389, "173": 14915, "174": 4285, "175": 1173, "176": 10239, "177": 2477, "178": 1316, "179": 17435, "180": 15617, "181": 7242, "182": 9283, "183": 9857, "184": 14001, "185": 49556, "186": 20591, "187": 12878, "188": 9446, "189": 25175, "190": 7001, "191": 10153, "192": 20176, "193": 10731, "194": 14079, "195": 7314, "196": 4663, "197": 5519, "198": 16186, "199": 15880, "200": 14945, "201": 12123, "202": 7749, "203": 2715, "204": 20719, "205": 25068, "206": 77040, "207": 4658, "208": 12768, "209": 18578, "210": 13616, "211": 18834, "212": 24578, "213": 55309, "214": 20214, "215": 22153, "216": 17255, "217": 21886, "218": 61913, "219": 34959, "220": 14861, "221": 19588, "222": 18093, "223": 13272, "224": 6477, "225": 17379, "226": 6922, "227": 17550, "228": 47027, "229": 47698, "230": 174289, "231": 52972, "232": 40619, "233": 25495, "234": 92114, "235": 2596, "236": 503578, "237": 7896, "238": 10551, "239": 21125, "240": 13300, "241": 8842, "242": 31644, "243": 17554, "244": 22292, "245": 9115, "246": 18491, "247": 3483, "248": 25175, "249": 125574, "250": 39211, "251": 22348, "252": 101511, "253": 13102, "254": 43400, "255": 24168, "256": 68754, "257": 30805, "258": 12263, "259": 93393, "260": 9998, "261": 44339, "262": 48955, "263": 174289, "264": 433642, "265": 947652, "266": 884870, "267": 122955, "268": 71159, "269": 133628, "270": 40619, "271": 1558, "272": 22315, "273": 3079, "274": 8363, "275": 503578, "276": 13300, "277": 3304, "278": 6096, "279": 8842, "280": 7991, "281": 18491, "282": 17187, "283": 5273, "284": 2997, "285": 2976, "286": 2601, "287": 17435, "288": 15617, "289": 9283, "290": 18336, "291": 21821, "292": 9446, "293": 25179, "294": 9421, "295": 125574, "296": 16003, "297": 43796, "298": 4758, "299": 12313, "300": 27893, "301": 5958, "302": 16299, "303": 7965, "304": 11277, "305": 39543, "306": 9371, "307": 41469, "308": 16186, "309": 12099, "310": 5181, "311": 14502, "312": 14945, "313": 10200, "314": 23779, "315": 7749, "316": 2715, "317": 31805, "318": 10002, "319": 32031, "320": 49478, "321": 57751, "322": 25068, "323": 21782, "324": 77040, "325": 14395, "326": 270500, "327": 46986, "328": 51536, "329": 50159, "330": 26056, "331": 14815, "332": 36987, "333": 30912, "334": 17260, "335": 43400, "336": 50347, "337": 42412, "338": 23690, "339": 10477, "340": 26709, "341": 23669, "342": 20742, "343": 15116, "344": 11394, "345": 14538, "346": 16838, "347": 13128, "348": 17295, "349": 653074, "350": 4529, "351": 11462, "352": 44345, "353": 4980, "354": 38349, "355": 20591, "356": 21821, "357": 7780, "358": 6612, "359": 4158, "360": 9446, "361": 25175, "362": 25179, "363": 7001, "364": 9421, "365": 10153, "366": 20176, "367": 10731, "368": 5927, "369": 14907, "370": 2473, "371": 29641, "372": 5171, "373": 125574, "374": 204187, "375": 16003, "376": 7168, "377": 14079, "378": 43796, "379": 11474, "380": 4758, "381": 6464, "382": 144924, "383": 12313, "384": 27893, "385": 5958, "386": 7314, "387": 4663, "388": 5519, "389": 7351, "390": 6775, "391": 16299, "392": 7965, "393": 11277, "394": 50607, "395": 28822, "396": 31106, "397": 39543, "398": 9371, "399": 39211, "400": 41469, "401": 16186, "402": 12099, "403": 5181, "404": 15880, "405": 4550, "406": 14502, "407": 14945, "408": 13628, "409": 10200, "410": 23779, "411": 7749, "412": 2715, "413": 31805, "414": 10002, "415": 20719, "416": 35348, "417": 32031, "418": 49478, "419": 19370, "420": 57751, "421": 25068, "422": 8694, "423": 77040, "424": 11474, "425": 11185, "426": 10284, "427": 4658, "428": 14395, "429": 12768, "430": 17976, "431": 10314, "432": 18578, "433": 270500, "434": 13616, "435": 46986, "436": 51536, "437": 29589, "438": 18834, "439": 50159, "440": 24578, "441": 215012, "442": 32640, "443": 18389, "444": 101511, "445": 85693, "446": 26056, "447": 17138, "448": 14815, "449": 55309, "450": 16036, "451": 36987, "452": 30912, "453": 22150, "454": 16146, "455": 17260, "456": 43400, "457": 20214, "458": 92677, "459": 22153, "460": 14975, "461": 50347, "462": 26641, "463": 112833, "464": 46760, "465": 91462, "466": 23690, "467": 120371, "468": 10477, "469": 9172, "470": 28064, "471": 46392, "472": 17255, "473": 8478, "474": 17617, "475": 14915, "476": 17285, "477": 21886, "478": 26709, "479": 15293, "480": 29740, "481": 17774, "482": 19107, "483": 15890, "484": 22566, "485": 61913, "486": 22488, "487": 21931, "488": 23669, "489": 34959, "490": 14861, "491": 19588, "492": 20742, "493": 15116, "494": 16426, "495": 31515, "496": 17615, "497": 12825, "498": 11394, "499": 14538, "500": 18093, "501": 27127, "502": 15465, "503": 16838, "504": 7592, "505": 13128, "506": 11319, "507": 10334, "508": 9107, "509": 6953, "510": 13272, "511": 6477, "512": 17379, "513": 17295, "514": 17715, "515": 9579, "516": 9097, "517": 23253, "518": 10089, "519": 12000, "520": 23624, "521": 38536, "522": 47027, "523": 174289, "524": 52972, "525": 31868, "526": 42773, "527": 36727, "528": 75345, "529": 105673, "530": 50621, "531": 73167, "532": 44345, "533": 18828, "534": 234581, "535": 125642, "536": 61288, "537": 100763, "538": 78464, "539": 91728, "540": 130187, "541": 46916, "542": 74952, "543": 103335, "544": 75345, "545": 37327, "546": 35592, "547": 27183, "548": 135301, "549": 56075, "550": 74679, "551": 59831, "552": 48492, "553": 148497, "554": 98961, "555": 190751, "556": 63080, "557": 41459, "558": 31241, "559": 69889, "560": 99880, "561": 75106, "562": 179699, "563": 54170, "564": 140474, "565": 66371, "566": 33452, "567": 36437, "568": 533522, "569": 163466, "570": 124842, "571": 102339, "572": 71250, "573": 67507, "574": 180983, "575": 119548, "576": 114473, "577": 120486, "578": 83170, "579": 64325, "580": 83538, "581": 89884, "582": 103254, "583": 79230, "584": 78325, "585": 59327, "586": 59655, "587": 57937, "588": 72323, "589": 47096, "590": 37032, "591": 53229, "592": 75883, "593": 59280, "594": 63262, "595": 52151, "596": 50243, "597": 82316, "598": 77283, "599": 55054, "600": 69909, "601": 80717, "602": 42833, "603": 45828, "604": 62055, "605": 37738, "606": 40766, "607": 39329, "608": 67363, "609": 58882, "610": 31547, "611": 234581, "612": 125642, "613": 61288, "614": 100763, "615": 78464, "616": 91728, "617": 130187, "618": 46916, "619": 74952, "620": 103335, "621": 75345, "622": 37327, "623": 35592, "624": 27183, "625": 135301, "626": 56075, "627": 74679, "628": 59831, "629": 48492, "630": 148497, "631": 98961, "632": 190751, "633": 63080, "634": 41459, "635": 31241, "636": 69889, "637": 99880, "638": 75106, "639": 179699, "640": 54170, "641": 140474, "642": 66371, "643": 33452, "644": 36437, "645": 113756, "646": 34707, "647": 24563, "648": 21819, "649": 27879, "650": 18910, "651": 18657, "652": 16285, "653": 16944, "654": 16329, "655": 13509, "656": 17217, "657": 20830, "658": 18662, "659": 13195, "660": 19323, "661": 17949, "662": 14571, "663": 121141, "664": 65806, "665": 38655, "666": 18840, "667": 27887, "668": 6008, "669": 10166, "670": 4792, "671": 3173, "672": 13732, "673": 20204, "674": 10611, "675": 3616, "676": 145279, "677": 4163, "678": 5024, "679": 4770, "680": 87942, "681": 6042, "682": 419562, "683": 468301, "684": 8932, "685": 3981, "686": 3499, "687": 13244, "688": 7314, "689": 3477, "690": 2878, "691": 11468, "692": 4519, "693": 4349, "694": 4763, "695": 101580, "696": 30694, "697": 104579, "698": 264572, "699": 148131, "700": 21700, "701": 472159, "702": 276943, "703": 145430, "704": 134002, "705": 124401, "706": 196489, "707": 140506, "708": 159124, "709": 137490, "710": 105398, "711": 90232, "712": 54386, "713": 86958, "714": 162079, "715": 123497, "716": 87179, "717": 107366, "718": 89193, "719": 38895, "720": 54692, "721": 55246, "722": 42858, "723": 128309, "724": 74705, "725": 80311, "726": 100762, "727": 67841, "728": 78661, "729": 119721, "730": 182308, "731": 75079, "732": 46021, "733": 91014, "734": 116176, "735": 110375, "736": 47761, "737": 38789, "738": 34676, "739": 28960, "740": 23949, "741": 26218, "742": 24555, "743": 14693, "744": 4243, "745": 2846, "746": 7135, "747": 61257, "748": 16742, "749": 14668, "750": 9721, "751": 10868, "752": 9022, "753": 7624, "754": 7324, "755": 5469, "756": 5126, "757": 9429, "758": 6099, "759": 5077, "760": 4548, "761": 4044, "762": 4063, "763": 3736, "764": 3171, "765": 3212, "766": 3339, "767": 3358, "768": 3139, "769": 2700, "770": 2573, "771": 2810, "772": 2681, "773": 2680, "774": 2547, "775": 2294, "776": 2343, "777": 2339, "778": 2327, "779": 2198, "780": 2148, "781": 2092, "782": 2269, "783": 2689, "784": 3208, "785": 2957, "786": 4344}, "likeCount": {"0": 1102, "1": 1084, "2": 1974, "3": 881, "4": 2567, "5": 3488, "6": 517, "7": 577, "8": 307, "9": 312, "10": 377, "11": 533, "12": 695, "13": 808, "14": 393, "15": 389, "16": 265, "17": 382, "18": 1431, "19": 341, "20": 151, "21": 409, "22": 227, "23": 170, "24": 215, "25": 181, "26": 119, "27": 267, "28": 84, "29": 122, "30": 217, "31": 204, "32": 313, "33": 497, "34": 83, "35": 145, "36": 1170, "37": 469, "38": 847, "39": 897, "40": 1254, "41": 5321, "42": 613, "43": 251, "44": 264, "45": 84, "46": 87, "47": 157, "48": 108, "49": 99, "50": 49, "51": 222, "52": 607, "53": 96, "54": 697, "55": 329, "56": 129, "57": 266, "58": 214, "59": 105, "60": 55, "61": 361, "62": 141, "63": 66, "64": 785, "65": 342, "66": 211, "67": 163, "68": 227, "69": 112, "70": 188, "71": 281, "72": 272, "73": 1102, "74": 366, "75": 470, "76": 349, "77": 643, "78": 219341, "79": 200, "80": 991, "81": 828, "82": 264, "83": 1196, "84": 258, "85": 776, "86": 577, "87": 341, "88": 151, "89": 170, "90": 238, "91": 122, "92": 227, "93": 83, "94": 374, "95": 145, "96": 4824, "97": 966, "98": 469, "99": 2088, "100": 61, "101": 11632, "102": 660, "103": 805, "104": 49, "105": 580, "106": 313, "107": 159, "108": 68, "109": 53, "110": 668, "111": 507, "112": 204, "113": 1077, "114": 2174, "115": 308, "116": 5622, "117": 476, "118": 192, "119": 274, "120": 2012, "121": 928, "122": 267, "123": 277, "124": 471, "125": 522, "126": 818, "127": 881, "128": 787, "129": 2223, "130": 404, "131": 643, "132": 1703, "133": 1038, "134": 638, "135": 2257, "136": 2255, "137": 797, "138": 521, "139": 529, "140": 505, "141": 526, "142": 823, "143": 1184, "144": 766, "145": 515, "146": 695, "147": 393, "148": 382, "149": 392, "150": 569, "151": 267, "152": 219, "153": 84, "154": 623, "155": 204, "156": 313, "157": 374, "158": 145, "159": 22594, "160": 990, "161": 1803, "162": 15298, "163": 6691, "164": 2392, "165": 4362, "166": 1733, "167": 2084, "168": 1453, "169": 357, "170": 68, "171": 231, "172": 638, "173": 357, "174": 62, "175": 18, "176": 417, "177": 120, "178": 43, "179": 558, "180": 483, "181": 233, "182": 324, "183": 231, "184": 515, "185": 1581, "186": 392, "187": 285, "188": 304, "189": 903, "190": 195, "191": 317, "192": 648, "193": 524, "194": 623, "195": 243, "196": 149, "197": 211, "198": 448, "199": 648, "200": 471, "201": 511, "202": 258, "203": 87, "204": 853, "205": 1069, "206": 2223, "207": 168, "208": 450, "209": 799, "210": 552, "211": 510, "212": 933, "213": 1829, "214": 782, "215": 929, "216": 623, "217": 750, "218": 2322, "219": 1048, "220": 438, "221": 496, "222": 533, "223": 422, "224": 215, "225": 535, "226": 181, "227": 497, "228": 1519, "229": 1646, "230": 4824, "231": 1551, "232": 924, "233": 469, "234": 2088, "235": 34, "236": 11632, "237": 157, "238": 357, "239": 660, "240": 589, "241": 304, "242": 871, "243": 607, "244": 697, "245": 329, "246": 580, "247": 280, "248": 903, "249": 4650, "250": 1662, "251": 531, "252": 2257, "253": 613, "254": 1337, "255": 851, "256": 3488, "257": 1762, "258": 710, "259": 3277, "260": 506, "261": 1933, "262": 2141, "263": 4824, "264": 15298, "265": 46551, "266": 22594, "267": 6691, "268": 3963, "269": 7513, "270": 924, "271": 51, "272": 814, "273": 61, "274": 204, "275": 11632, "276": 589, "277": 159, "278": 248, "279": 304, "280": 258, "281": 580, "282": 806, "283": 159, "284": 113, "285": 105, "286": 105, "287": 558, "288": 483, "289": 324, "290": 668, "291": 704, "292": 304, "293": 967, "294": 335, "295": 4650, "296": 476, "297": 1565, "298": 214, "299": 418, "300": 928, "301": 267, "302": 666, "303": 418, "304": 383, "305": 1305, "306": 351, "307": 1574, "308": 448, "309": 401, "310": 154, "311": 539, "312": 471, "313": 471, "314": 818, "315": 258, "316": 87, "317": 1020, "318": 380, "319": 1084, "320": 1974, "321": 2567, "322": 1069, "323": 787, "324": 2223, "325": 444, "326": 7625, "327": 1220, "328": 1703, "329": 1884, "330": 797, "331": 521, "332": 1528, "333": 1029, "334": 518, "335": 1337, "336": 1815, "337": 1316, "338": 839, "339": 374, "340": 869, "341": 555, "342": 515, "343": 362, "344": 312, "345": 377, "346": 389, "347": 382, "348": 347, "349": 22104, "350": 119, "351": 313, "352": 1453, "353": 139, "354": 1077, "355": 392, "356": 704, "357": 204, "358": 211, "359": 163, "360": 304, "361": 903, "362": 967, "363": 195, "364": 335, "365": 317, "366": 648, "367": 524, "368": 227, "369": 463, "370": 112, "371": 308, "372": 188, "373": 4650, "374": 5622, "375": 476, "376": 281, "377": 623, "378": 1565, "379": 384, "380": 214, "381": 274, "382": 2012, "383": 418, "384": 928, "385": 267, "386": 243, "387": 149, "388": 211, "389": 272, "390": 277, "391": 666, "392": 418, "393": 383, "394": 1463, "395": 940, "396": 1220, "397": 1305, "398": 351, "399": 1662, "400": 1574, "401": 448, "402": 401, "403": 154, "404": 648, "405": 192, "406": 539, "407": 471, "408": 522, "409": 471, "410": 818, "411": 258, "412": 87, "413": 1020, "414": 380, "415": 853, "416": 1102, "417": 1084, "418": 1974, "419": 881, "420": 2567, "421": 1069, "422": 366, "423": 2223, "424": 470, "425": 404, "426": 349, "427": 168, "428": 444, "429": 450, "430": 643, "431": 331, "432": 799, "433": 7625, "434": 552, "435": 1220, "436": 1703, "437": 1038, "438": 510, "439": 1884, "440": 933, "441": 5796, "442": 991, "443": 638, "444": 2257, "445": 2255, "446": 797, "447": 559, "448": 521, "449": 1829, "450": 529, "451": 1528, "452": 1029, "453": 828, "454": 505, "455": 518, "456": 1337, "457": 782, "458": 2869, "459": 929, "460": 391, "461": 1815, "462": 1066, "463": 2877, "464": 1667, "465": 2893, "466": 839, "467": 2885, "468": 374, "469": 264, "470": 901, "471": 1196, "472": 623, "473": 258, "474": 451, "475": 357, "476": 539, "477": 750, "478": 869, "479": 526, "480": 823, "481": 499, "482": 1184, "483": 402, "484": 893, "485": 2322, "486": 766, "487": 776, "488": 555, "489": 1048, "490": 438, "491": 496, "492": 515, "493": 362, "494": 517, "495": 847, "496": 577, "497": 307, "498": 312, "499": 377, "500": 533, "501": 808, "502": 393, "503": 389, "504": 265, "505": 382, "506": 341, "507": 392, "508": 342, "509": 170, "510": 422, "511": 215, "512": 535, "513": 347, "514": 569, "515": 219, "516": 238, "517": 623, "518": 227, "519": 374, "520": 897, "521": 1392, "522": 1519, "523": 4824, "524": 1551, "525": 990, "526": 1803, "527": 966, "528": 2392, "529": 4362, "530": 1733, "531": 2084, "532": 1453, "533": 613, "534": 961, "535": 1422, "536": 544, "537": 738, "538": 687, "539": 747, "540": 1737, "541": 405, "542": 796, "543": 911, "544": 742, "545": 251, "546": 259, "547": 195, "548": 1610, "549": 569, "550": 784, "551": 589, "552": 421, "553": 1549, "554": 1007, "555": 1761, "556": 689, "557": 421, "558": 301, "559": 613, "560": 1104, "561": 732, "562": 2564, "563": 655, "564": 1693, "565": 722, "566": 470, "567": 420, "568": 2793, "569": 1170, "570": 909, "571": 775, "572": 507, "573": 315, "574": 1540, "575": 656, "576": 764, "577": 1020, "578": 598, "579": 438, "580": 538, "581": 788, "582": 661, "583": 511, "584": 610, "585": 386, "586": 399, "587": 380, "588": 608, "589": 451, "590": 169, "591": 438, "592": 703, "593": 361, "594": 456, "595": 369, "596": 307, "597": 852, "598": 802, "599": 439, "600": 493, "601": 540, "602": 384, "603": 335, "604": 310, "605": 342, "606": 410, "607": 269, "608": 440, "609": 665, "610": 335, "611": 961, "612": 1422, "613": 544, "614": 738, "615": 687, "616": 747, "617": 1737, "618": 405, "619": 796, "620": 911, "621": 742, "622": 251, "623": 259, "624": 195, "625": 1610, "626": 569, "627": 784, "628": 589, "629": 421, "630": 1549, "631": 1007, "632": 1761, "633": 689, "634": 421, "635": 301, "636": 613, "637": 1104, "638": 732, "639": 2564, "640": 655, "641": 1693, "642": 722, "643": 470, "644": 420, "645": 411, "646": 340, "647": 206, "648": 172, "649": 223, "650": 145, "651": 161, "652": 114, "653": 144, "654": 142, "655": 101, "656": 139, "657": 176, "658": 188, "659": 106, "660": 162, "661": 186, "662": 117, "663": 2036, "664": 872, "665": 512, "666": 294, "667": 205, "668": 63, "669": 210, "670": 91, "671": 48, "672": 322, "673": 478, "674": 192, "675": 71, "676": 3096, "677": 72, "678": 44, "679": 76, "680": 2898, "681": 141, "682": 7081, "683": 12184, "684": 75, "685": 54, "686": 49, "687": 439, "688": 111, "689": 60, "690": 40, "691": 422, "692": 75, "693": 102, "694": 83, "695": 398, "696": 217, "697": 755, "698": 3406, "699": 1542, "700": 195, "701": 2381, "702": 2466, "703": 1366, "704": 1292, "705": 1215, "706": 2592, "707": 1498, "708": 1714, "709": 1516, "710": 1202, "711": 990, "712": 446, "713": 904, "714": 2261, "715": 1765, "716": 1027, "717": 1215, "718": 1070, "719": 388, "720": 640, "721": 625, "722": 478, "723": 1379, "724": 714, "725": 577, "726": 979, "727": 672, "728": 847, "729": 1309, "730": 1985, "731": 958, "732": 353, "733": 913, "734": 1165, "735": 1685, "736": 424, "737": 314, "738": 308, "739": 208, "740": 167, "741": 244, "742": 368, "743": 327, "744": 128, "745": 93, "746": 272, "747": 903, "748": 246, "749": 180, "750": 88, "751": 157, "752": 94, "753": 74, "754": 77, "755": 42, "756": 49, "757": 123, "758": 54, "759": 55, "760": 42, "761": 37, "762": 44, "763": 39, "764": 28, "765": 31, "766": 38, "767": 39, "768": 34, "769": 24, "770": 28, "771": 25, "772": 27, "773": 30, "774": 23, "775": 23, "776": 28, "777": 22, "778": 26, "779": 22, "780": 22, "781": 20, "782": 19, "783": 19, "784": 22, "785": 32, "786": 37}, "favouriteCount": {"0": null, "1": null, "2": null, "3": null, "4": null, "5": null, "6": null, "7": null, "8": null, "9": null, "10": null, "11": null, "12": null, "13": null, "14": null, "15": null, "16": null, "17": null, "18": null, "19": null, "20": null, "21": null, "22": null, "23": null, "24": null, "25": null, "26": null, "27": null, "28": null, "29": null, "30": null, "31": null, "32": null, "33": null, "34": null, "35": null, "36": null, "37": null, "38": null, "39": null, "40": null, "41": null, "42": null, "43": null, "44": null, "45": null, "46": null, "47": null, "48": null, "49": null, "50": null, "51": null, "52": null, "53": null, "54": null, "55": null, "56": null, "57": null, "58": null, "59": null, "60": null, "61": null, "62": null, "63": null, "64": null, "65": null, "66": null, "67": null, "68": null, "69": null, "70": null, "71": null, "72": null, "73": null, "74": null, "75": null, "76": null, "77": null, "78": null, "79": null, "80": null, "81": null, "82": null, "83": null, "84": null, "85": null, "86": null, "87": null, "88": null, "89": null, "90": null, "91": null, "92": null, "93": null, "94": null, "95": null, "96": null, "97": null, "98": null, "99": null, "100": null, "101": null, "102": null, "103": null, "104": null, "105": null, "106": null, "107": null, "108": null, "109": null, "110": null, "111": null, "112": null, "113": null, "114": null, "115": null, "116": null, "117": null, "118": null, "119": null, "120": null, "121": null, "122": null, "123": null, "124": null, "125": null, "126": null, "127": null, "128": null, "129": null, "130": null, "131": null, "132": null, "133": null, "134": null, "135": null, "136": null, "137": null, "138": null, "139": null, "140": null, "141": null, "142": null, "143": null, "144": null, "145": null, "146": null, "147": null, "148": null, "149": null, "150": null, "151": null, "152": null, "153": null, "154": null, "155": null, "156": null, "157": null, "158": null, "159": null, "160": null, "161": null, "162": null, "163": null, "164": null, "165": null, "166": null, "167": null, "168": null, "169": null, "170": null, "171": null, "172": null, "173": null, "174": null, "175": null, "176": null, "177": null, "178": null, "179": null, "180": null, "181": null, "182": null, "183": null, "184": null, "185": null, "186": null, "187": null, "188": null, "189": null, "190": null, "191": null, "192": null, "193": null, "194": null, "195": null, "196": null, "197": null, "198": null, "199": null, "200": null, "201": null, "202": null, "203": null, "204": null, "205": null, "206": null, "207": null, "208": null, "209": null, "210": null, "211": null, "212": null, "213": null, "214": null, "215": null, "216": null, "217": null, "218": null, "219": null, "220": null, "221": null, "222": null, "223": null, "224": null, "225": null, "226": null, "227": null, "228": null, "229": null, "230": null, "231": null, "232": null, "233": null, "234": null, "235": null, "236": null, "237": null, "238": null, "239": null, "240": null, "241": null, "242": null, "243": null, "244": null, "245": null, "246": null, "247": null, "248": null, "249": null, "250": null, "251": null, "252": null, "253": null, "254": null, "255": null, "256": null, "257": null, "258": null, "259": null, "260": null, "261": null, "262": null, "263": null, "264": null, "265": null, "266": null, "267": null, "268": null, "269": null, "270": null, "271": null, "272": null, "273": null, "274": null, "275": null, "276": null, "277": null, "278": null, "279": null, "280": null, "281": null, "282": null, "283": null, "284": null, "285": null, "286": null, "287": null, "288": null, "289": null, "290": null, "291": null, "292": null, "293": null, "294": null, "295": null, "296": null, "297": null, "298": null, "299": null, "300": null, "301": null, "302": null, "303": null, "304": null, "305": null, "306": null, "307": null, "308": null, "309": null, "310": null, "311": null, "312": null, "313": null, "314": null, "315": null, "316": null, "317": null, "318": null, "319": null, "320": null, "321": null, "322": null, "323": null, "324": null, "325": null, "326": null, "327": null, "328": null, "329": null, "330": null, "331": null, "332": null, "333": null, "334": null, "335": null, "336": null, "337": null, "338": null, "339": null, "340": null, "341": null, "342": null, "343": null, "344": null, "345": null, "346": null, "347": null, "348": null, "349": null, "350": null, "351": null, "352": null, "353": null, "354": null, "355": null, "356": null, "357": null, "358": null, "359": null, "360": null, "361": null, "362": null, "363": null, "364": null, "365": null, "366": null, "367": null, "368": null, "369": null, "370": null, "371": null, "372": null, "373": null, "374": null, "375": null, "376": null, "377": null, "378": null, "379": null, "380": null, "381": null, "382": null, "383": null, "384": null, "385": null, "386": null, "387": null, "388": null, "389": null, "390": null, "391": null, "392": null, "393": null, "394": null, "395": null, "396": null, "397": null, "398": null, "399": null, "400": null, "401": null, "402": null, "403": null, "404": null, "405": null, "406": null, "407": null, "408": null, "409": null, "410": null, "411": null, "412": null, "413": null, "414": null, "415": null, "416": null, "417": null, "418": null, "419": null, "420": null, "421": null, "422": null, "423": null, "424": null, "425": null, "426": null, "427": null, "428": null, "429": null, "430": null, "431": null, "432": null, "433": null, "434": null, "435": null, "436": null, "437": null, "438": null, "439": null, "440": null, "441": null, "442": null, "443": null, "444": null, "445": null, "446": null, "447": null, "448": null, "449": null, "450": null, "451": null, "452": null, "453": null, "454": null, "455": null, "456": null, "457": null, "458": null, "459": null, "460": null, "461": null, "462": null, "463": null, "464": null, "465": null, "466": null, "467": null, "468": null, "469": null, "470": null, "471": null, "472": null, "473": null, "474": null, "475": null, "476": null, "477": null, "478": null, "479": null, "480": null, "481": null, "482": null, "483": null, "484": null, "485": null, "486": null, "487": null, "488": null, "489": null, "490": null, "491": null, "492": null, "493": null, "494": null, "495": null, "496": null, "497": null, "498": null, "499": null, "500": null, "501": null, "502": null, "503": null, "504": null, "505": null, "506": null, "507": null, "508": null, "509": null, "510": null, "511": null, "512": null, "513": null, "514": null, "515": null, "516": null, "517": null, "518": null, "519": null, "520": null, "521": null, "522": null, "523": null, "524": null, "525": null, "526": null, "527": null, "528": null, "529": null, "530": null, "531": null, "532": null, "533": null, "534": null, "535": null, "536": null, "537": null, "538": null, "539": null, "540": null, "541": null, "542": null, "543": null, "544": null, "545": null, "546": null, "547": null, "548": null, "549": null, "550": null, "551": null, "552": null, "553": null, "554": null, "555": null, "556": null, "557": null, "558": null, "559": null, "560": null, "561": null, "562": null, "563": null, "564": null, "565": null, "566": null, "567": null, "568": null, "569": null, "570": null, "571": null, "572": null, "573": null, "574": null, "575": null, "576": null, "577": null, "578": null, "579": null, "580": null, "581": null, "582": null, "583": null, "584": null, "585": null, "586": null, "587": null, "588": null, "589": null, "590": null, "591": null, "592": null, "593": null, "594": null, "595": null, "596": null, "597": null, "598": null, "599": null, "600": null, "601": null, "602": null, "603": null, "604": null, "605": null, "606": null, "607": null, "608": null, "609": null, "610": null, "611": null, "612": null, "613": null, "614": null, "615": null, "616": null, "617": null, "618": null, "619": null, "620": null, "621": null, "622": null, "623": null, "624": null, "625": null, "626": null, "627": null, "628": null, "629": null, "630": null, "631": null, "632": null, "633": null, "634": null, "635": null, "636": null, "637": null, "638": null, "639": null, "640": null, "641": null, "642": null, "643": null, "644": null, "645": null, "646": null, "647": null, "648": null, "649": null, "650": null, "651": null, "652": null, "653": null, "654": null, "655": null, "656": null, "657": null, "658": null, "659": null, "660": null, "661": null, "662": null, "663": null, "664": null, "665": null, "666": null, "667": null, "668": null, "669": null, "670": null, "671": null, "672": null, "673": null, "674": null, "675": null, "676": null, "677": null, "678": null, "679": null, "680": null, "681": null, "682": null, "683": null, "684": null, "685": null, "686": null, "687": null, "688": null, "689": null, "690": null, "691": null, "692": null, "693": null, "694": null, "695": null, "696": null, "697": null, "698": null, "699": null, "700": null, "701": null, "702": null, "703": null, "704": null, "705": null, "706": null, "707": null, "708": null, "709": null, "710": null, "711": null, "712": null, "713": null, "714": null, "715": null, "716": null, "717": null, "718": null, "719": null, "720": null, "721": null, "722": null, "723": null, "724": null, "725": null, "726": null, "727": null, "728": null, "729": null, "730": null, "731": null, "732": null, "733": null, "734": null, "735": null, "736": null, "737": null, "738": null, "739": null, "740": null, "741": null, "742": null, "743": null, "744": null, "745": null, "746": null, "747": null, "748": null, "749": null, "750": null, "751": null, "752": null, "753": null, "754": null, "755": null, "756": null, "757": null, "758": null, "759": null, "760": null, "761": null, "762": null, "763": null, "764": null, "765": null, "766": null, "767": null, "768": null, "769": null, "770": null, "771": null, "772": null, "773": null, "774": null, "775": null, "776": null, "777": null, "778": null, "779": null, "780": null, "781": null, "782": null, "783": null, "784": null, "785": null, "786": null}, "commentCount": {"0": 44, "1": 62, "2": 67, "3": 53, "4": 141, "5": 175, "6": 45, "7": 29, "8": 31, "9": 29, "10": 34, "11": 65, "12": 48, "13": 41, "14": 41, "15": 40, "16": 33, "17": 41, "18": 65, "19": 39, "20": 13, "21": 31, "22": 18, "23": 34, "24": 22, "25": 26, "26": 12, "27": 29, "28": 4, "29": 5, "30": 17, "31": 13, "32": 23, "33": 63, "34": 9, "35": 10, "36": 102, "37": 73, "38": 76, "39": 88, "40": 71, "41": 610, "42": 116, "43": 7, "44": 10, "45": 3, "46": 1, "47": 7, "48": 4, "49": 5, "50": 9, "51": 8, "52": 75, "53": 12, "54": 44, "55": 31, "56": 8, "57": 32, "58": 14, "59": 7, "60": 4, "61": 33, "62": 4, "63": 2, "64": 135, "65": 23, "66": 24, "67": 11, "68": 19, "69": 7, "70": 28, "71": 38, "72": 30, "73": 44, "74": 32, "75": 64, "76": 33, "77": 49, "78": 5181, "79": 47, "80": 59, "81": 89, "82": 30, "83": 99, "84": 18, "85": 62, "86": 29, "87": 39, "88": 13, "89": 34, "90": 19, "91": 5, "92": 19, "93": 9, "94": 32, "95": 10, "96": 277, "97": 151, "98": 20, "99": 93, "100": 0, "101": 265, "102": 43, "103": 17, "104": 9, "105": 25, "106": 24, "107": 8, "108": 5, "109": 7, "110": 48, "111": 71, "112": 26, "113": 83, "114": 269, "115": 26, "116": 467, "117": 32, "118": 14, "119": 22, "120": 125, "121": 78, "122": 23, "123": 28, "124": 83, "125": 45, "126": 85, "127": 53, "128": 70, "129": 76, "130": 32, "131": 49, "132": 93, "133": 60, "134": 75, "135": 183, "136": 87, "137": 87, "138": 55, "139": 69, "140": 83, "141": 49, "142": 80, "143": 183, "144": 63, "145": 33, "146": 48, "147": 41, "148": 41, "149": 32, "150": 59, "151": 29, "152": 14, "153": 4, "154": 28, "155": 13, "156": 23, "157": 32, "158": 10, "159": 5102, "160": 96, "161": 314, "162": 1508, "163": 807, "164": 192, "165": 578, "166": 134, "167": 151, "168": 110, "169": 12, "170": 5, "171": 52, "172": 75, "173": 55, "174": 8, "175": 3, "176": 26, "177": 21, "178": 8, "179": 34, "180": 20, "181": 18, "182": 33, "183": 52, "184": 36, "185": 72, "186": 20, "187": 32, "188": 31, "189": 48, "190": 27, "191": 32, "192": 50, "193": 49, "194": 67, "195": 48, "196": 37, "197": 27, "198": 73, "199": 46, "200": 83, "201": 114, "202": 36, "203": 18, "204": 104, "205": 70, "206": 76, "207": 33, "208": 33, "209": 65, "210": 37, "211": 56, "212": 89, "213": 169, "214": 127, "215": 76, "216": 49, "217": 57, "218": 213, "219": 52, "220": 61, "221": 34, "222": 65, "223": 46, "224": 22, "225": 69, "226": 26, "227": 63, "228": 149, "229": 171, "230": 277, "231": 152, "232": 39, "233": 20, "234": 93, "235": 3, "236": 265, "237": 7, "238": 12, "239": 43, "240": 31, "241": 14, "242": 59, "243": 75, "244": 44, "245": 31, "246": 25, "247": 55, "248": 48, "249": 170, "250": 85, "251": 30, "252": 183, "253": 43, "254": 113, "255": 50, "256": 175, "257": 165, "258": 91, "259": 254, "260": 117, "261": 107, "262": 113, "263": 277, "264": 1508, "265": 3483, "266": 5102, "267": 807, "268": 358, "269": 899, "270": 39, "271": 7, "272": 35, "273": 0, "274": 11, "275": 265, "276": 31, "277": 21, "278": 13, "279": 14, "280": 19, "281": 25, "282": 34, "283": 8, "284": 9, "285": 7, "286": 16, "287": 34, "288": 20, "289": 33, "290": 48, "291": 49, "292": 31, "293": 62, "294": 24, "295": 170, "296": 32, "297": 100, "298": 18, "299": 35, "300": 78, "301": 23, "302": 66, "303": 67, "304": 44, "305": 116, "306": 61, "307": 88, "308": 73, "309": 48, "310": 23, "311": 38, "312": 83, "313": 61, "314": 85, "315": 36, "316": 18, "317": 79, "318": 44, "319": 62, "320": 67, "321": 141, "322": 70, "323": 70, "324": 76, "325": 33, "326": 334, "327": 81, "328": 93, "329": 128, "330": 87, "331": 55, "332": 74, "333": 89, "334": 47, "335": 113, "336": 126, "337": 104, "338": 41, "339": 40, "340": 66, "341": 29, "342": 33, "343": 39, "344": 29, "345": 34, "346": 40, "347": 41, "348": 21, "349": 1580, "350": 12, "351": 23, "352": 110, "353": 39, "354": 83, "355": 20, "356": 49, "357": 26, "358": 24, "359": 11, "360": 31, "361": 48, "362": 62, "363": 27, "364": 24, "365": 32, "366": 50, "367": 49, "368": 19, "369": 52, "370": 7, "371": 26, "372": 28, "373": 170, "374": 467, "375": 32, "376": 38, "377": 67, "378": 100, "379": 21, "380": 18, "381": 22, "382": 125, "383": 35, "384": 78, "385": 23, "386": 48, "387": 37, "388": 27, "389": 30, "390": 28, "391": 66, "392": 67, "393": 44, "394": 127, "395": 75, "396": 78, "397": 116, "398": 61, "399": 85, "400": 88, "401": 73, "402": 48, "403": 23, "404": 46, "405": 14, "406": 38, "407": 83, "408": 45, "409": 61, "410": 85, "411": 36, "412": 18, "413": 79, "414": 44, "415": 104, "416": 44, "417": 62, "418": 67, "419": 53, "420": 141, "421": 70, "422": 32, "423": 76, "424": 64, "425": 32, "426": 33, "427": 33, "428": 33, "429": 33, "430": 49, "431": 76, "432": 65, "433": 334, "434": 37, "435": 81, "436": 93, "437": 60, "438": 56, "439": 128, "440": 89, "441": 371, "442": 59, "443": 75, "444": 183, "445": 87, "446": 87, "447": 105, "448": 55, "449": 169, "450": 69, "451": 74, "452": 89, "453": 89, "454": 83, "455": 47, "456": 113, "457": 127, "458": 189, "459": 76, "460": 5, "461": 126, "462": 101, "463": 123, "464": 286, "465": 139, "466": 41, "467": 119, "468": 40, "469": 30, "470": 366, "471": 99, "472": 49, "473": 18, "474": 51, "475": 55, "476": 104, "477": 57, "478": 66, "479": 49, "480": 80, "481": 39, "482": 183, "483": 42, "484": 81, "485": 213, "486": 63, "487": 62, "488": 29, "489": 52, "490": 61, "491": 34, "492": 33, "493": 39, "494": 45, "495": 76, "496": 29, "497": 31, "498": 29, "499": 34, "500": 65, "501": 41, "502": 41, "503": 40, "504": 33, "505": 41, "506": 39, "507": 32, "508": 29, "509": 34, "510": 46, "511": 22, "512": 69, "513": 21, "514": 59, "515": 14, "516": 19, "517": 28, "518": 19, "519": 32, "520": 88, "521": 107, "522": 149, "523": 277, "524": 152, "525": 96, "526": 314, "527": 151, "528": 192, "529": 578, "530": 134, "531": 151, "532": 110, "533": 116, "534": 18, "535": 19, "536": 18, "537": 33, "538": 43, "539": 49, "540": 50, "541": 9, "542": 36, "543": 37, "544": 35, "545": 7, "546": 14, "547": 6, "548": 52, "549": 30, "550": 33, "551": 13, "552": 17, "553": 46, "554": 51, "555": 72, "556": 21, "557": 13, "558": 11, "559": 14, "560": 36, "561": 29, "562": 57, "563": 33, "564": 47, "565": 22, "566": 21, "567": 12, "568": 55, "569": 28, "570": 12, "571": 17, "572": 13, "573": 2, "574": 49, "575": 39, "576": 44, "577": 41, "578": 10, "579": 15, "580": 5, "581": 22, "582": 43, "583": 22, "584": 22, "585": 16, "586": 24, "587": 41, "588": 24, "589": 6, "590": 5, "591": 17, "592": 17, "593": 7, "594": 26, "595": 10, "596": 12, "597": 26, "598": 45, "599": 11, "600": 28, "601": 50, "602": 28, "603": 1, "604": 14, "605": 13, "606": 12, "607": 5, "608": 12, "609": 8, "610": 20, "611": 18, "612": 19, "613": 18, "614": 33, "615": 43, "616": 49, "617": 50, "618": 9, "619": 36, "620": 37, "621": 35, "622": 7, "623": 14, "624": 6, "625": 52, "626": 30, "627": 33, "628": 13, "629": 17, "630": 46, "631": 51, "632": 72, "633": 21, "634": 13, "635": 11, "636": 14, "637": 36, "638": 29, "639": 57, "640": 33, "641": 47, "642": 22, "643": 21, "644": 12, "645": 13, "646": 6, "647": 2, "648": 6, "649": 5, "650": 3, "651": 24, "652": 2, "653": 5, "654": 5, "655": 3, "656": 10, "657": 4, "658": 5, "659": 3, "660": 1, "661": 8, "662": 1, "663": 39, "664": 29, "665": 8, "666": 8, "667": 7, "668": 2, "669": 5, "670": 4, "671": 1, "672": 5, "673": 15, "674": 8, "675": 2, "676": 118, "677": 2, "678": 3, "679": 7, "680": 133, "681": 7, "682": 267, "683": 453, "684": 3, "685": 4, "686": 6, "687": 39, "688": 6, "689": 1, "690": 1, "691": 29, "692": 10, "693": 4, "694": 10, "695": 11, "696": 6, "697": 49, "698": 76, "699": 52, "700": 2, "701": 43, "702": 79, "703": 19, "704": 17, "705": 50, "706": 74, "707": 51, "708": 71, "709": 32, "710": 105, "711": 32, "712": 7, "713": 45, "714": 58, "715": 63, "716": 26, "717": 68, "718": 44, "719": 1, "720": 17, "721": 12, "722": 3, "723": 52, "724": 24, "725": 19, "726": 56, "727": 28, "728": 24, "729": 45, "730": 60, "731": 17, "732": 12, "733": 29, "734": 22, "735": 58, "736": 21, "737": 1, "738": 19, "739": 6, "740": 1, "741": 21, "742": 32, "743": 6, "744": 8, "745": 3, "746": 14, "747": 13, "748": 3, "749": 2, "750": 0, "751": 0, "752": 2, "753": 3, "754": 0, "755": 0, "756": 0, "757": 6, "758": 0, "759": 0, "760": 0, "761": 1, "762": 1, "763": 0, "764": 0, "765": 0, "766": 0, "767": 1, "768": 1, "769": 0, "770": 0, "771": 0, "772": 0, "773": 0, "774": 2, "775": 0, "776": 0, "777": 1, "778": 0, "779": 0, "780": 0, "781": 1, "782": 0, "783": 3, "784": 0, "785": 0, "786": 2}, "duration": {"0": "PT39M12S", "1": "PT46M7S", "2": "PT37M4S", "3": "PT31M22S", "4": "PT31M21S", "5": "PT16M27S", "6": "PT54M2S", "7": "PT54M11S", "8": "PT1H23M51S", "9": "PT1H9M5S", "10": "PT1H22M37S", "11": "PT1H11M10S", "12": "PT20M6S", "13": "PT1H21M28S", "14": "PT1H17M5S", "15": "PT1H18M17S", "16": "PT54M57S", "17": "PT1H24M20S", "18": "PT1H2M35S", "19": "PT38M35S", "20": "PT44M47S", "21": "PT53M46S", "22": "PT58M8S", "23": "PT1H36M40S", "24": "PT35M59S", "25": "PT56M33S", "26": "PT48M34S", "27": "PT48M56S", "28": "PT40M38S", "29": "PT49M26S", "30": "PT59M30S", "31": "PT43M4S", "32": "PT58M23S", "33": "PT58M2S", "34": "PT57M46S", "35": "PT58M32S", "36": "PT1H1M49S", "37": "PT1H6M37S", "38": "PT42M17S", "39": "PT34M58S", "40": "PT20M49S", "41": "PT25M42S", "42": "PT35M45S", "43": "PT18M43S", "44": "PT17M35S", "45": "PT11M1S", "46": "PT11M3S", "47": "PT15M22S", "48": "PT24M5S", "49": "PT18M39S", "50": "PT30M22S", "51": "PT20M46S", "52": "PT37M17S", "53": "PT37M6S", "54": "PT18M59S", "55": "PT26M2S", "56": "PT19M3S", "57": "PT32M8S", "58": "PT22M38S", "59": "PT33M49S", "60": "PT15M40S", "61": "PT28M45S", "62": "PT19M55S", "63": "PT29M41S", "64": "PT35M5S", "65": "PT27M27S", "66": "PT22M15S", "67": "PT25M33S", "68": "PT35M22S", "69": "PT11M2S", "70": "PT29M58S", "71": "PT50M2S", "72": "PT24M37S", "73": "PT39M12S", "74": "PT38M29S", "75": "PT39M22S", "76": "PT55M11S", "77": "PT45M30S", "78": "PT6M54S", "79": "PT42M41S", "80": "PT1H12M22S", "81": "PT54M59S", "82": "PT45M6S", "83": "PT56M49S", "84": "PT34M45S", "85": "PT29M25S", "86": "PT54M11S", "87": "PT38M35S", "88": "PT44M47S", "89": "PT1H36M40S", "90": "PT42M26S", "91": "PT49M26S", "92": "PT44M6S", "93": "PT57M46S", "94": "PT28M47S", "95": "PT58M32S", "96": "PT55M7S", "97": "PT1H1M3S", "98": "PT27M33S", "99": "PT40M13S", "100": "PT18M19S", "101": "PT27M7S", "102": "PT30M6S", "103": "PT19M15S", "104": "PT30M22S", "105": "PT29M12S", "106": "PT21M18S", "107": "PT24M16S", "108": "PT18M59S", "109": "PT18M15S", "110": "PT26M36S", "111": "PT11M21S", "112": "PT31M48S", "113": "PT1H2M41S", "114": "PT11M11S", "115": "PT53M35S", "116": "PT1H4M30S", "117": "PT48M21S", "118": "PT30M11S", "119": "PT31M35S", "120": "PT48M38S", "121": "PT50M24S", "122": "PT29M42S", "123": "PT36M49S", "124": "PT1H13M4S", "125": "PT36M50S", "126": "PT48M6S", "127": "PT31M22S", "128": "PT34M30S", "129": "PT1H5M16S", "130": "PT1H41S", "131": "PT45M30S", "132": "PT54M39S", "133": "PT52M16S", "134": "PT1H3M18S", "135": "PT55M46S", "136": "PT48M7S", "137": "PT33M47S", "138": "PT43M51S", "139": "PT48M12S", "140": "PT45M14S", "141": "PT31M22S", "142": "PT36M37S", "143": "PT13M19S", "144": "PT45M22S", "145": "PT57M7S", "146": "PT20M6S", "147": "PT1H17M5S", "148": "PT1H24M20S", "149": "PT45M26S", "150": "PT48M56S", "151": "PT48M56S", "152": "PT36M42S", "153": "PT40M38S", "154": "PT51M52S", "155": "PT43M4S", "156": "PT58M23S", "157": "PT28M47S", "158": "PT58M32S", "159": "PT22M23S", "160": "PT1H4M59S", "161": "PT51M33S", "162": "PT31M55S", "163": "PT35M48S", "164": "PT41M7S", "165": "PT34M10S", "166": "PT24M34S", "167": "PT29M29S", "168": "PT1H2M17S", "169": "PT40M21S", "170": "PT18M59S", "171": "PT49M12S", "172": "PT1H3M18S", "173": "PT1H14M21S", "174": "PT16M45S", "175": "PT44M34S", "176": "PT18M54S", "177": "PT25M10S", "178": "PT21M37S", "179": "PT19M15S", "180": "PT20M12S", "181": "PT32M16S", "182": "PT20M51S", "183": "PT49M12S", "184": "PT32M26S", "185": "PT30M8S", "186": "PT25M37S", "187": "PT35M40S", "188": "PT34M10S", "189": "PT39M29S", "190": "PT39M18S", "191": "PT24M59S", "192": "PT51M19S", "193": "PT13M2S", "194": "PT36M59S", "195": "PT30M50S", "196": "PT33M56S", "197": "PT43M24S", "198": "PT34M58S", "199": "PT40M35S", "200": "PT1H13M4S", "201": "PT1H18M43S", "202": "PT1H7S", "203": "PT48M52S", "204": "PT46M32S", "205": "PT35M6S", "206": "PT1H5M16S", "207": "PT36M2S", "208": "PT30M31S", "209": "PT53M36S", "210": "PT40M59S", "211": "PT59M22S", "212": "PT48M25S", "213": "PT43M4S", "214": "PT51M30S", "215": "PT34M2S", "216": "PT32M47S", "217": "PT44M20S", "218": "PT29M47S", "219": "PT48M30S", "220": "PT39M15S", "221": "PT59M19S", "222": "PT1H11M10S", "223": "PT35M58S", "224": "PT35M59S", "225": "PT1H5M20S", "226": "PT56M33S", "227": "PT58M2S", "228": "PT59M38S", "229": "PT50M20S", "230": "PT55M7S", "231": "PT31M51S", "232": "PT22M19S", "233": "PT27M33S", "234": "PT40M13S", "235": "PT56M11S", "236": "PT27M7S", "237": "PT15M22S", "238": "PT40M21S", "239": "PT30M6S", "240": "PT21M8S", "241": "PT42M7S", "242": "PT11M38S", "243": "PT37M17S", "244": "PT18M59S", "245": "PT26M2S", "246": "PT29M12S", "247": "PT9M59S", "248": "PT39M29S", "249": "PT40M57S", "250": "PT45M24S", "251": "PT14M28S", "252": "PT55M46S", "253": "PT13M54S", "254": "PT1H3M26S", "255": "PT40S", "256": "PT16M27S", "257": "PT21M47S", "258": "PT15M30S", "259": "PT13M53S", "260": "PT4M16S", "261": "PT18M48S", "262": "PT19M43S", "263": "PT55M7S", "264": "PT31M55S", "265": "PT19M20S", "266": "PT22M23S", "267": "PT35M48S", "268": "PT16M53S", "269": "PT21M6S", "270": "PT22M19S", "271": "PT30M26S", "272": "PT25M44S", "273": "PT18M19S", "274": "PT27M45S", "275": "PT27M7S", "276": "PT21M8S", "277": "PT17M12S", "278": "PT21M50S", "279": "PT42M7S", "280": "PT18M19S", "281": "PT29M12S", "282": "PT15M48S", "283": "PT24M16S", "284": "PT32M34S", "285": "PT33M49S", "286": "PT30M38S", "287": "PT19M15S", "288": "PT20M12S", "289": "PT20M51S", "290": "PT26M36S", "291": "PT33M46S", "292": "PT34M10S", "293": "PT29M6S", "294": "PT19M16S", "295": "PT40M57S", "296": "PT48M21S", "297": "PT45M34S", "298": "PT33M29S", "299": "PT40M49S", "300": "PT50M24S", "301": "PT29M42S", "302": "PT44M53S", "303": "PT49M13S", "304": "PT49M41S", "305": "PT56M5S", "306": "PT36M42S", "307": "PT46M13S", "308": "PT34M58S", "309": "PT34M22S", "310": "PT59M18S", "311": "PT42M39S", "312": "PT1H13M4S", "313": "PT35M52S", "314": "PT48M6S", "315": "PT1H7S", "316": "PT48M52S", "317": "PT34M12S", "318": "PT42M16S", "319": "PT46M7S", "320": "PT37M4S", "321": "PT31M21S", "322": "PT35M6S", "323": "PT34M30S", "324": "PT1H5M16S", "325": "PT55M44S", "326": "PT29M56S", "327": "PT59M33S", "328": "PT54M39S", "329": "PT1H5M33S", "330": "PT33M47S", "331": "PT43M51S", "332": "PT34M26S", "333": "PT29M53S", "334": "PT51M38S", "335": "PT1H3M26S", "336": "PT29M36S", "337": "PT28M12S", "338": "PT30M54S", "339": "PT41M45S", "340": "PT35M30S", "341": "PT34M24S", "342": "PT57M7S", "343": "PT52M45S", "344": "PT1H9M5S", "345": "PT1H22M37S", "346": "PT1H18M17S", "347": "PT1H24M20S", "348": "PT46M41S", "349": "PT1H41M", "350": "PT48M34S", "351": "PT58M23S", "352": "PT1H2M17S", "353": "PT7M7S", "354": "PT1H2M41S", "355": "PT25M37S", "356": "PT33M46S", "357": "PT31M48S", "358": "PT22M15S", "359": "PT25M33S", "360": "PT34M10S", "361": "PT39M29S", "362": "PT29M6S", "363": "PT39M18S", "364": "PT19M16S", "365": "PT24M59S", "366": "PT51M19S", "367": "PT13M2S", "368": "PT35M22S", "369": "PT17M24S", "370": "PT11M2S", "371": "PT53M35S", "372": "PT29M58S", "373": "PT40M57S", "374": "PT1H4M30S", "375": "PT48M21S", "376": "PT50M2S", "377": "PT36M59S", "378": "PT45M34S", "379": "PT25M22S", "380": "PT33M29S", "381": "PT31M35S", "382": "PT48M38S", "383": "PT40M49S", "384": "PT50M24S", "385": "PT29M42S", "386": "PT30M50S", "387": "PT33M56S", "388": "PT43M24S", "389": "PT24M37S", "390": "PT36M49S", "391": "PT44M53S", "392": "PT49M13S", "393": "PT49M41S", "394": "PT33M46S", "395": "PT31M47S", "396": "PT37M31S", "397": "PT56M5S", "398": "PT36M42S", "399": "PT45M24S", "400": "PT46M13S", "401": "PT34M58S", "402": "PT34M22S", "403": "PT59M18S", "404": "PT40M35S", "405": "PT30M11S", "406": "PT42M39S", "407": "PT1H13M4S", "408": "PT36M50S", "409": "PT35M52S", "410": "PT48M6S", "411": "PT1H7S", "412": "PT48M52S", "413": "PT34M12S", "414": "PT42M16S", "415": "PT46M32S", "416": "PT39M12S", "417": "PT46M7S", "418": "PT37M4S", "419": "PT31M22S", "420": "PT31M21S", "421": "PT35M6S", "422": "PT38M29S", "423": "PT1H5M16S", "424": "PT39M22S", "425": "PT1H41S", "426": "PT55M11S", "427": "PT36M2S", "428": "PT55M44S", "429": "PT30M31S", "430": "PT45M30S", "431": "PT52M11S", "432": "PT53M36S", "433": "PT29M56S", "434": "PT40M59S", "435": "PT59M33S", "436": "PT54M39S", "437": "PT52M16S", "438": "PT59M22S", "439": "PT1H5M33S", "440": "PT48M25S", "441": "PT54M38S", "442": "PT1H12M22S", "443": "PT1H3M18S", "444": "PT55M46S", "445": "PT48M7S", "446": "PT33M47S", "447": "PT52M9S", "448": "PT43M51S", "449": "PT43M4S", "450": "PT48M12S", "451": "PT34M26S", "452": "PT29M53S", "453": "PT54M59S", "454": "PT45M14S", "455": "PT51M38S", "456": "PT1H3M26S", "457": "PT51M30S", "458": "PT58M37S", "459": "PT34M2S", "460": "PT57M52S", "461": "PT29M36S", "462": "PT48M18S", "463": "PT33M56S", "464": "PT36M41S", "465": "PT39M13S", "466": "PT30M54S", "467": "PT54M34S", "468": "PT41M45S", "469": "PT45M6S", "470": "PT35M50S", "471": "PT56M49S", "472": "PT32M47S", "473": "PT34M45S", "474": "PT35M40S", "475": "PT1H14M21S", "476": "PT50M38S", "477": "PT44M20S", "478": "PT35M30S", "479": "PT31M22S", "480": "PT36M37S", "481": "PT32M4S", "482": "PT13M19S", "483": "PT26M", "484": "PT20M27S", "485": "PT29M47S", "486": "PT45M22S", "487": "PT29M25S", "488": "PT34M24S", "489": "PT48M30S", "490": "PT39M15S", "491": "PT59M19S", "492": "PT57M7S", "493": "PT52M45S", "494": "PT54M2S", "495": "PT42M17S", "496": "PT54M11S", "497": "PT1H23M51S", "498": "PT1H9M5S", "499": "PT1H22M37S", "500": "PT1H11M10S", "501": "PT1H21M28S", "502": "PT1H17M5S", "503": "PT1H18M17S", "504": "PT54M57S", "505": "PT1H24M20S", "506": "PT38M35S", "507": "PT45M26S", "508": "PT50M41S", "509": "PT1H36M40S", "510": "PT35M58S", "511": "PT35M59S", "512": "PT1H5M20S", "513": "PT46M41S", "514": "PT48M56S", "515": "PT36M42S", "516": "PT42M26S", "517": "PT51M52S", "518": "PT44M6S", "519": "PT28M47S", "520": "PT34M58S", "521": "PT32M34S", "522": "PT59M38S", "523": "PT55M7S", "524": "PT31M51S", "525": "PT1H4M59S", "526": "PT51M33S", "527": "PT1H1M3S", "528": "PT41M7S", "529": "PT34M10S", "530": "PT24M34S", "531": "PT29M29S", "532": "PT1H2M17S", "533": "PT35M45S", "534": "PT12M5S", "535": "PT8M47S", "536": "PT6M22S", "537": "PT9M43S", "538": "PT7M10S", "539": "PT9M26S", "540": "PT7M5S", "541": "PT8M24S", "542": "PT5M31S", "543": "PT6M8S", "544": "PT6M12S", "545": "PT6M36S", "546": "PT6M35S", "547": "PT5M19S", "548": "PT11M29S", "549": "PT11M19S", "550": "PT5M59S", "551": "PT9M42S", "552": "PT4M12S", "553": "PT9M21S", "554": "PT7M42S", "555": "PT7M8S", "556": "PT6M45S", "557": "PT7M11S", "558": "PT8M51S", "559": "PT6M52S", "560": "PT8M55S", "561": "PT12M56S", "562": "PT11M40S", "563": "PT5M47S", "564": "PT11M48S", "565": "PT10M8S", "566": "PT5M24S", "567": "PT16M8S", "568": "PT5M32S", "569": "PT7M17S", "570": "PT8M29S", "571": "PT10M22S", "572": "PT2M28S", "573": "PT1M56S", "574": "PT8M24S", "575": "PT6M", "576": "PT8M12S", "577": "PT11M24S", "578": "PT7M11S", "579": "PT10M28S", "580": "PT3M34S", "581": "PT14M34S", "582": "PT6M43S", "583": "PT8M1S", "584": "PT8M5S", "585": "PT6M20S", "586": "PT7M33S", "587": "PT9M38S", "588": "PT11M6S", "589": "PT6M50S", "590": "PT3M44S", "591": "PT7M15S", "592": "PT4M27S", "593": "PT5M15S", "594": "PT9M58S", "595": "PT9M6S", "596": "PT7M38S", "597": "PT10M57S", "598": "PT5M36S", "599": "PT7M58S", "600": "PT9M58S", "601": "PT15M49S", "602": "PT7M58S", "603": "PT5M51S", "604": "PT7M16S", "605": "PT11M10S", "606": "PT10M34S", "607": "PT8M34S", "608": "PT10M30S", "609": "PT7M17S", "610": "PT3M18S", "611": "PT12M5S", "612": "PT8M47S", "613": "PT6M22S", "614": "PT9M43S", "615": "PT7M10S", "616": "PT9M26S", "617": "PT7M5S", "618": "PT8M24S", "619": "PT5M31S", "620": "PT6M8S", "621": "PT6M12S", "622": "PT6M36S", "623": "PT6M35S", "624": "PT5M19S", "625": "PT11M29S", "626": "PT11M19S", "627": "PT5M59S", "628": "PT9M42S", "629": "PT4M12S", "630": "PT9M21S", "631": "PT7M42S", "632": "PT7M8S", "633": "PT6M45S", "634": "PT7M11S", "635": "PT8M51S", "636": "PT6M52S", "637": "PT8M55S", "638": "PT12M56S", "639": "PT11M40S", "640": "PT5M47S", "641": "PT11M48S", "642": "PT10M8S", "643": "PT5M24S", "644": "PT16M8S", "645": "PT2M43S", "646": "PT10M39S", "647": "PT7M17S", "648": "PT5M59S", "649": "PT6M36S", "650": "PT5M40S", "651": "PT11M8S", "652": "PT5M47S", "653": "PT7M", "654": "PT11M12S", "655": "PT6M22S", "656": "PT6M21S", "657": "PT10M32S", "658": "PT13M6S", "659": "PT6M2S", "660": "PT10M56S", "661": "PT18M17S", "662": "PT10M9S", "663": "PT11M18S", "664": "PT13M", "665": "PT11M48S", "666": "PT10M20S", "667": "PT17M9S", "668": "PT13M37S", "669": "PT14M56S", "670": "PT25M49S", "671": "PT16M4S", "672": "PT15M11S", "673": "PT27M49S", "674": "PT37M26S", "675": "PT1M27S", "676": "PT39M46S", "677": "PT1M45S", "678": "PT1H13M37S", "679": "PT37M15S", "680": "PT20M27S", "681": "PT40M54S", "682": "PT29M19S", "683": "PT1H29M10S", "684": "PT40S", "685": "PT37S", "686": "PT1M2S", "687": "PT32S", "688": "PT41S", "689": "PT2M8S", "690": "PT39S", "691": "PT1M29S", "692": "PT1M1S", "693": "PT1M19S", "694": "PT1M13S", "695": "PT6M19S", "696": "PT8M57S", "697": "PT16M26S", "698": "PT9M42S", "699": "PT12M23S", "700": "PT8M54S", "701": "PT5M44S", "702": "PT11M31S", "703": "PT7M58S", "704": "PT9M50S", "705": "PT9M2S", "706": "PT10M45S", "707": "PT16M11S", "708": "PT8M35S", "709": "PT10M30S", "710": "PT11M40S", "711": "PT9M41S", "712": "PT3M8S", "713": "PT18M19S", "714": "PT7M8S", "715": "PT9M13S", "716": "PT6M40S", "717": "PT10M15S", "718": "PT8M46S", "719": "PT4M57S", "720": "PT8M48S", "721": "PT9M32S", "722": "PT12M38S", "723": "PT11M54S", "724": "PT5M57S", "725": "PT5M49S", "726": "PT11M9S", "727": "PT4M19S", "728": "PT8M2S", "729": "PT9M43S", "730": "PT7M1S", "731": "PT6M28S", "732": "PT4M38S", "733": "PT4M45S", "734": "PT4M52S", "735": "PT15M30S", "736": "PT6M6S", "737": "PT2M13S", "738": "PT8M5S", "739": "PT4M", "740": "PT3M38S", "741": "PT17M1S", "742": "PT9M9S", "743": "PT46M32S", "744": "PT35M32S", "745": "PT34M49S", "746": "PT40M39S", "747": "PT9M44S", "748": "PT3M56S", "749": "PT12M4S", "750": "PT2M43S", "751": "PT14M23S", "752": "PT11M53S", "753": "PT10M46S", "754": "PT9M33S", "755": "PT2M41S", "756": "PT5M11S", "757": "PT10M41S", "758": "PT7M41S", "759": "PT6M24S", "760": "PT8M29S", "761": "PT5M51S", "762": "PT12M14S", "763": "PT7M58S", "764": "PT2M40S", "765": "PT5M59S", "766": "PT8M48S", "767": "PT6M17S", "768": "PT8M45S", "769": "PT4M44S", "770": "PT3M31S", "771": "PT4M18S", "772": "PT9M11S", "773": "PT11M18S", "774": "PT8M18S", "775": "PT9M9S", "776": "PT10M29S", "777": "PT9M3S", "778": "PT12M28S", "779": "PT5M47S", "780": "PT9M56S", "781": "PT4M38S", "782": "PT2M33S", "783": "PT6M55S", "784": "PT14M27S", "785": "PT7M25S", "786": "PT2M36S"}, "definition": {"0": "hd", "1": "hd", "2": "hd", "3": "hd", "4": "hd", "5": "hd", "6": "hd", "7": "hd", "8": "hd", "9": "hd", "10": "hd", "11": "hd", "12": "hd", "13": "hd", "14": "hd", "15": "hd", "16": "hd", "17": "hd", "18": "hd", "19": "hd", "20": "hd", "21": "hd", "22": "hd", "23": "hd", "24": "hd", "25": "hd", "26": "hd", "27": "hd", "28": "hd", "29": "hd", "30": "hd", "31": "hd", "32": "hd", "33": "hd", "34": "hd", "35": "hd", "36": "hd", "37": "hd", "38": "hd", "39": "hd", "40": "hd", "41": "hd", "42": "hd", "43": "hd", "44": "hd", "45": "hd", "46": "hd", "47": "hd", "48": "hd", "49": "hd", "50": "hd", "51": "hd", "52": "hd", "53": "hd", "54": "hd", "55": "hd", "56": "hd", "57": "hd", "58": "hd", "59": "hd", "60": "hd", "61": "hd", "62": "hd", "63": "hd", "64": "hd", "65": "hd", "66": "hd", "67": "hd", "68": "hd", "69": "hd", "70": "hd", "71": "hd", "72": "hd", "73": "hd", "74": "hd", "75": "hd", "76": "hd", "77": "hd", "78": "sd", "79": "hd", "80": "hd", "81": "hd", "82": "hd", "83": "hd", "84": "hd", "85": "hd", "86": "hd", "87": "hd", "88": "hd", "89": "hd", "90": "hd", "91": "hd", "92": "hd", "93": "hd", "94": "hd", "95": "hd", "96": "hd", "97": "hd", "98": "hd", "99": "hd", "100": "sd", "101": "hd", "102": "hd", "103": "hd", "104": "hd", "105": "hd", "106": "hd", "107": "hd", "108": "hd", "109": "hd", "110": "hd", "111": "hd", "112": "hd", "113": "sd", "114": "hd", "115": "hd", "116": "hd", "117": "hd", "118": "hd", "119": "hd", "120": "hd", "121": "hd", "122": "hd", "123": "hd", "124": "hd", "125": "hd", "126": "hd", "127": "hd", "128": "hd", "129": "hd", "130": "hd", "131": "hd", "132": "hd", "133": "hd", "134": "hd", "135": "hd", "136": "hd", "137": "hd", "138": "hd", "139": "hd", "140": "hd", "141": "hd", "142": "hd", "143": "hd", "144": "hd", "145": "hd", "146": "hd", "147": "hd", "148": "hd", "149": "hd", "150": "hd", "151": "hd", "152": "hd", "153": "hd", "154": "hd", "155": "hd", "156": "hd", "157": "hd", "158": "hd", "159": "hd", "160": "hd", "161": "hd", "162": "hd", "163": "hd", "164": "hd", "165": "hd", "166": "hd", "167": "hd", "168": "hd", "169": "hd", "170": "hd", "171": "hd", "172": "hd", "173": "hd", "174": "hd", "175": "hd", "176": "hd", "177": "hd", "178": "hd", "179": "hd", "180": "hd", "181": "hd", "182": "hd", "183": "hd", "184": "hd", "185": "hd", "186": "hd", "187": "hd", "188": "hd", "189": "hd", "190": "hd", "191": "hd", "192": "hd", "193": "hd", "194": "hd", "195": "hd", "196": "hd", "197": "hd", "198": "hd", "199": "hd", "200": "hd", "201": "hd", "202": "hd", "203": "hd", "204": "hd", "205": "hd", "206": "hd", "207": "hd", "208": "hd", "209": "hd", "210": "hd", "211": "hd", "212": "hd", "213": "hd", "214": "hd", "215": "hd", "216": "hd", "217": "hd", "218": "hd", "219": "hd", "220": "hd", "221": "hd", "222": "hd", "223": "hd", "224": "hd", "225": "hd", "226": "hd", "227": "hd", "228": "hd", "229": "hd", "230": "hd", "231": "hd", "232": "hd", "233": "hd", "234": "hd", "235": "hd", "236": "hd", "237": "hd", "238": "hd", "239": "hd", "240": "hd", "241": "hd", "242": "hd", "243": "hd", "244": "hd", "245": "hd", "246": "hd", "247": "hd", "248": "hd", "249": "hd", "250": "hd", "251": "hd", "252": "hd", "253": "hd", "254": "hd", "255": "hd", "256": "hd", "257": "hd", "258": "hd", "259": "hd", "260": "hd", "261": "hd", "262": "hd", "263": "hd", "264": "hd", "265": "hd", "266": "hd", "267": "hd", "268": "hd", "269": "hd", "270": "hd", "271": "hd", "272": "hd", "273": "sd", "274": "hd", "275": "hd", "276": "hd", "277": "hd", "278": "hd", "279": "hd", "280": "hd", "281": "hd", "282": "hd", "283": "hd", "284": "hd", "285": "hd", "286": "hd", "287": "hd", "288": "hd", "289": "hd", "290": "hd", "291": "hd", "292": "hd", "293": "hd", "294": "hd", "295": "hd", "296": "hd", "297": "hd", "298": "hd", "299": "hd", "300": "hd", "301": "hd", "302": "hd", "303": "hd", "304": "hd", "305": "hd", "306": "hd", "307": "hd", "308": "hd", "309": "hd", "310": "hd", "311": "hd", "312": "hd", "313": "hd", "314": "hd", "315": "hd", "316": "hd", "317": "hd", "318": "hd", "319": "hd", "320": "hd", "321": "hd", "322": "hd", "323": "hd", "324": "hd", "325": "hd", "326": "hd", "327": "hd", "328": "hd", "329": "hd", "330": "hd", "331": "hd", "332": "hd", "333": "hd", "334": "hd", "335": "hd", "336": "hd", "337": "hd", "338": "hd", "339": "hd", "340": "hd", "341": "hd", "342": "hd", "343": "hd", "344": "hd", "345": "hd", "346": "hd", "347": "hd", "348": "hd", "349": "hd", "350": "hd", "351": "hd", "352": "hd", "353": "hd", "354": "sd", "355": "hd", "356": "hd", "357": "hd", "358": "hd", "359": "hd", "360": "hd", "361": "hd", "362": "hd", "363": "hd", "364": "hd", "365": "hd", "366": "hd", "367": "hd", "368": "hd", "369": "hd", "370": "hd", "371": "hd", "372": "hd", "373": "hd", "374": "hd", "375": "hd", "376": "hd", "377": "hd", "378": "hd", "379": "hd", "380": "hd", "381": "hd", "382": "hd", "383": "hd", "384": "hd", "385": "hd", "386": "hd", "387": "hd", "388": "hd", "389": "hd", "390": "hd", "391": "hd", "392": "hd", "393": "hd", "394": "hd", "395": "hd", "396": "hd", "397": "hd", "398": "hd", "399": "hd", "400": "hd", "401": "hd", "402": "hd", "403": "hd", "404": "hd", "405": "hd", "406": "hd", "407": "hd", "408": "hd", "409": "hd", "410": "hd", "411": "hd", "412": "hd", "413": "hd", "414": "hd", "415": "hd", "416": "hd", "417": "hd", "418": "hd", "419": "hd", "420": "hd", "421": "hd", "422": "hd", "423": "hd", "424": "hd", "425": "hd", "426": "hd", "427": "hd", "428": "hd", "429": "hd", "430": "hd", "431": "hd", "432": "hd", "433": "hd", "434": "hd", "435": "hd", "436": "hd", "437": "hd", "438": "hd", "439": "hd", "440": "hd", "441": "hd", "442": "hd", "443": "hd", "444": "hd", "445": "hd", "446": "hd", "447": "hd", "448": "hd", "449": "hd", "450": "hd", "451": "hd", "452": "hd", "453": "hd", "454": "hd", "455": "hd", "456": "hd", "457": "hd", "458": "hd", "459": "hd", "460": "hd", "461": "hd", "462": "hd", "463": "hd", "464": "hd", "465": "hd", "466": "hd", "467": "hd", "468": "hd", "469": "hd", "470": "hd", "471": "hd", "472": "hd", "473": "hd", "474": "hd", "475": "hd", "476": "hd", "477": "hd", "478": "hd", "479": "hd", "480": "hd", "481": "hd", "482": "hd", "483": "hd", "484": "hd", "485": "hd", "486": "hd", "487": "hd", "488": "hd", "489": "hd", "490": "hd", "491": "hd", "492": "hd", "493": "hd", "494": "hd", "495": "hd", "496": "hd", "497": "hd", "498": "hd", "499": "hd", "500": "hd", "501": "hd", "502": "hd", "503": "hd", "504": "hd", "505": "hd", "506": "hd", "507": "hd", "508": "hd", "509": "hd", "510": "hd", "511": "hd", "512": "hd", "513": "hd", "514": "hd", "515": "hd", "516": "hd", "517": "hd", "518": "hd", "519": "hd", "520": "hd", "521": "hd", "522": "hd", "523": "hd", "524": "hd", "525": "hd", "526": "hd", "527": "hd", "528": "hd", "529": "hd", "530": "hd", "531": "hd", "532": "hd", "533": "hd", "534": "hd", "535": "hd", "536": "hd", "537": "hd", "538": "hd", "539": "hd", "540": "hd", "541": "hd", "542": "hd", "543": "hd", "544": "hd", "545": "hd", "546": "hd", "547": "hd", "548": "hd", "549": "hd", "550": "hd", "551": "hd", "552": "hd", "553": "hd", "554": "hd", "555": "hd", "556": "hd", "557": "hd", "558": "hd", "559": "hd", "560": "hd", "561": "hd", "562": "hd", "563": "sd", "564": "hd", "565": "hd", "566": "hd", "567": "hd", "568": "sd", "569": "sd", "570": "sd", "571": "sd", "572": "sd", "573": "sd", "574": "sd", "575": "sd", "576": "sd", "577": "sd", "578": "sd", "579": "sd", "580": "sd", "581": "sd", "582": "sd", "583": "sd", "584": "sd", "585": "hd", "586": "sd", "587": "sd", "588": "sd", "589": "sd", "590": "sd", "591": "sd", "592": "hd", "593": "hd", "594": "hd", "595": "hd", "596": "hd", "597": "hd", "598": "hd", "599": "hd", "600": "hd", "601": "hd", "602": "hd", "603": "hd", "604": "hd", "605": "sd", "606": "hd", "607": "hd", "608": "hd", "609": "hd", "610": "hd", "611": "hd", "612": "hd", "613": "hd", "614": "hd", "615": "hd", "616": "hd", "617": "hd", "618": "hd", "619": "hd", "620": "hd", "621": "hd", "622": "hd", "623": "hd", "624": "hd", "625": "hd", "626": "hd", "627": "hd", "628": "hd", "629": "hd", "630": "hd", "631": "hd", "632": "hd", "633": "hd", "634": "hd", "635": "hd", "636": "hd", "637": "hd", "638": "hd", "639": "hd", "640": "sd", "641": "hd", "642": "hd", "643": "hd", "644": "hd", "645": "sd", "646": "sd", "647": "sd", "648": "sd", "649": "sd", "650": "sd", "651": "sd", "652": "sd", "653": "sd", "654": "sd", "655": "sd", "656": "sd", "657": "sd", "658": "sd", "659": "sd", "660": "sd", "661": "sd", "662": "sd", "663": "sd", "664": "sd", "665": "sd", "666": "sd", "667": "hd", "668": "hd", "669": "hd", "670": "hd", "671": "hd", "672": "hd", "673": "hd", "674": "hd", "675": "hd", "676": "sd", "677": "hd", "678": "hd", "679": "hd", "680": "hd", "681": "hd", "682": "hd", "683": "hd", "684": "hd", "685": "hd", "686": "hd", "687": "hd", "688": "hd", "689": "hd", "690": "hd", "691": "hd", "692": "hd", "693": "hd", "694": "hd", "695": "hd", "696": "hd", "697": "hd", "698": "hd", "699": "hd", "700": "hd", "701": "hd", "702": "hd", "703": "hd", "704": "hd", "705": "hd", "706": "hd", "707": "hd", "708": "hd", "709": "hd", "710": "hd", "711": "hd", "712": "hd", "713": "hd", "714": "hd", "715": "hd", "716": "hd", "717": "hd", "718": "hd", "719": "hd", "720": "hd", "721": "hd", "722": "hd", "723": "hd", "724": "hd", "725": "hd", "726": "hd", "727": "hd", "728": "hd", "729": "hd", "730": "hd", "731": "hd", "732": "hd", "733": "hd", "734": "hd", "735": "hd", "736": "hd", "737": "hd", "738": "hd", "739": "hd", "740": "hd", "741": "hd", "742": "hd", "743": "hd", "744": "hd", "745": "hd", "746": "hd", "747": "hd", "748": "hd", "749": "hd", "750": "hd", "751": "hd", "752": "hd", "753": "hd", "754": "hd", "755": "hd", "756": "hd", "757": "hd", "758": "hd", "759": "hd", "760": "hd", "761": "hd", "762": "hd", "763": "hd", "764": "hd", "765": "hd", "766": "hd", "767": "hd", "768": "hd", "769": "hd", "770": "hd", "771": "hd", "772": "hd", "773": "hd", "774": "hd", "775": "hd", "776": "hd", "777": "hd", "778": "hd", "779": "hd", "780": "hd", "781": "hd", "782": "hd", "783": "hd", "784": "hd", "785": "hd", "786": "hd"}, "caption": {"0": "false", "1": "false", "2": "false", "3": "false", "4": "false", "5": "true", "6": "false", "7": "false", "8": "false", "9": "true", "10": "false", "11": "false", "12": "false", "13": "false", "14": "false", "15": "false", "16": "false", "17": "false", "18": "false", "19": "false", "20": "false", "21": "false", "22": "false", "23": "false", "24": "false", "25": "false", "26": "false", "27": "false", "28": "false", "29": "false", "30": "false", "31": "false", "32": "false", "33": "false", "34": "false", "35": "false", "36": "false", "37": "false", "38": "false", "39": "false", "40": "false", "41": "false", "42": "false", "43": "false", "44": "false", "45": "false", "46": "false", "47": "false", "48": "false", "49": "false", "50": "false", "51": "false", "52": "false", "53": "false", "54": "false", "55": "false", "56": "false", "57": "false", "58": "false", "59": "false", "60": "false", "61": "false", "62": "false", "63": "false", "64": "false", "65": "false", "66": "false", "67": "false", "68": "false", "69": "false", "70": "false", "71": "false", "72": "false", "73": "false", "74": "false", "75": "false", "76": "false", "77": "false", "78": "false", "79": "false", "80": "false", "81": "false", "82": "false", "83": "false", "84": "false", "85": "false", "86": "false", "87": "false", "88": "false", "89": "false", "90": "false", "91": "false", "92": "false", "93": "false", "94": "false", "95": "false", "96": "false", "97": "false", "98": "false", "99": "false", "100": "false", "101": "false", "102": "false", "103": "false", "104": "false", "105": "false", "106": "false", "107": "false", "108": "false", "109": "false", "110": "false", "111": "false", "112": "false", "113": "false", "114": "false", "115": "false", "116": "false", "117": "false", "118": "false", "119": "false", "120": "false", "121": "false", "122": "false", "123": "false", "124": "false", "125": "false", "126": "false", "127": "false", "128": "false", "129": "false", "130": "false", "131": "false", "132": "false", "133": "false", "134": "false", "135": "false", "136": "false", "137": "false", "138": "false", "139": "false", "140": "false", "141": "false", "142": "false", "143": "false", "144": "false", "145": "false", "146": "false", "147": "false", "148": "false", "149": "false", "150": "false", "151": "false", "152": "false", "153": "false", "154": "false", "155": "false", "156": "false", "157": "false", "158": "false", "159": "false", "160": "false", "161": "false", "162": "false", "163": "false", "164": "false", "165": "false", "166": "false", "167": "false", "168": "false", "169": "false", "170": "false", "171": "false", "172": "false", "173": "false", "174": "false", "175": "false", "176": "false", "177": "false", "178": "false", "179": "false", "180": "false", "181": "false", "182": "false", "183": "false", "184": "false", "185": "false", "186": "false", "187": "false", "188": "false", "189": "false", "190": "false", "191": "false", "192": "false", "193": "false", "194": "false", "195": "false", "196": "false", "197": "false", "198": "false", "199": "false", "200": "false", "201": "false", "202": "false", "203": "false", "204": "false", "205": "false", "206": "false", "207": "false", "208": "false", "209": "false", "210": "false", "211": "false", "212": "false", "213": "false", "214": "false", "215": "false", "216": "false", "217": "false", "218": "false", "219": "false", "220": "false", "221": "false", "222": "false", "223": "false", "224": "false", "225": "false", "226": "false", "227": "false", "228": "false", "229": "false", "230": "false", "231": "false", "232": "false", "233": "false", "234": "false", "235": "false", "236": "false", "237": "false", "238": "false", "239": "false", "240": "false", "241": "false", "242": "false", "243": "false", "244": "false", "245": "false", "246": "false", "247": "false", "248": "false", "249": "false", "250": "false", "251": "false", "252": "false", "253": "false", "254": "false", "255": "false", "256": "true", "257": "true", "258": "true", "259": "true", "260": "false", "261": "false", "262": "false", "263": "false", "264": "false", "265": "false", "266": "false", "267": "false", "268": "false", "269": "false", "270": "false", "271": "false", "272": "false", "273": "false", "274": "false", "275": "false", "276": "false", "277": "false", "278": "false", "279": "false", "280": "false", "281": "false", "282": "false", "283": "false", "284": "false", "285": "false", "286": "false", "287": "false", "288": "false", "289": "false", "290": "false", "291": "false", "292": "false", "293": "false", "294": "false", "295": "false", "296": "false", "297": "false", "298": "false", "299": "false", "300": "false", "301": "false", "302": "false", "303": "false", "304": "false", "305": "false", "306": "false", "307": "false", "308": "false", "309": "false", "310": "false", "311": "false", "312": "false", "313": "false", "314": "false", "315": "false", "316": "false", "317": "false", "318": "false", "319": "false", "320": "false", "321": "false", "322": "false", "323": "false", "324": "false", "325": "false", "326": "false", "327": "false", "328": "false", "329": "false", "330": "false", "331": "false", "332": "false", "333": "false", "334": "false", "335": "false", "336": "false", "337": "false", "338": "false", "339": "false", "340": "false", "341": "false", "342": "false", "343": "false", "344": "true", "345": "false", "346": "false", "347": "false", "348": "false", "349": "false", "350": "false", "351": "false", "352": "false", "353": "false", "354": "false", "355": "false", "356": "false", "357": "false", "358": "false", "359": "false", "360": "false", "361": "false", "362": "false", "363": "false", "364": "false", "365": "false", "366": "false", "367": "false", "368": "false", "369": "false", "370": "false", "371": "false", "372": "false", "373": "false", "374": "false", "375": "false", "376": "false", "377": "false", "378": "false", "379": "false", "380": "false", "381": "false", "382": "false", "383": "false", "384": "false", "385": "false", "386": "false", "387": "false", "388": "false", "389": "false", "390": "false", "391": "false", "392": "false", "393": "false", "394": "false", "395": "false", "396": "false", "397": "false", "398": "false", "399": "false", "400": "false", "401": "false", "402": "false", "403": "false", "404": "false", "405": "false", "406": "false", "407": "false", "408": "false", "409": "false", "410": "false", "411": "false", "412": "false", "413": "false", "414": "false", "415": "false", "416": "false", "417": "false", "418": "false", "419": "false", "420": "false", "421": "false", "422": "false", "423": "false", "424": "false", "425": "false", "426": "false", "427": "false", "428": "false", "429": "false", "430": "false", "431": "false", "432": "false", "433": "false", "434": "false", "435": "false", "436": "false", "437": "false", "438": "false", "439": "false", "440": "false", "441": "false", "442": "false", "443": "false", "444": "false", "445": "false", "446": "false", "447": "false", "448": "false", "449": "false", "450": "false", "451": "false", "452": "false", "453": "false", "454": "false", "455": "false", "456": "false", "457": "false", "458": "false", "459": "false", "460": "false", "461": "false", "462": "false", "463": "false", "464": "false", "465": "false", "466": "false", "467": "false", "468": "false", "469": "false", "470": "false", "471": "false", "472": "false", "473": "false", "474": "false", "475": "false", "476": "false", "477": "false", "478": "false", "479": "false", "480": "false", "481": "false", "482": "false", "483": "false", "484": "false", "485": "false", "486": "false", "487": "false", "488": "false", "489": "false", "490": "false", "491": "false", "492": "false", "493": "false", "494": "false", "495": "false", "496": "false", "497": "false", "498": "true", "499": "false", "500": "false", "501": "false", "502": "false", "503": "false", "504": "false", "505": "false", "506": "false", "507": "false", "508": "false", "509": "false", "510": "false", "511": "false", "512": "false", "513": "false", "514": "false", "515": "false", "516": "false", "517": "false", "518": "false", "519": "false", "520": "false", "521": "false", "522": "false", "523": "false", "524": "false", "525": "false", "526": "false", "527": "false", "528": "false", "529": "false", "530": "false", "531": "false", "532": "false", "533": "false", "534": "true", "535": "true", "536": "true", "537": "true", "538": "true", "539": "true", "540": "true", "541": "true", "542": "true", "543": "true", "544": "true", "545": "true", "546": "true", "547": "true", "548": "true", "549": "true", "550": "true", "551": "true", "552": "true", "553": "true", "554": "true", "555": "true", "556": "true", "557": "true", "558": "true", "559": "true", "560": "true", "561": "true", "562": "true", "563": "true", "564": "true", "565": "true", "566": "true", "567": "true", "568": "true", "569": "true", "570": "true", "571": "true", "572": "false", "573": "true", "574": "true", "575": "true", "576": "true", "577": "true", "578": "true", "579": "true", "580": "true", "581": "true", "582": "true", "583": "true", "584": "true", "585": "true", "586": "true", "587": "true", "588": "true", "589": "true", "590": "true", "591": "true", "592": "true", "593": "true", "594": "true", "595": "true", "596": "true", "597": "true", "598": "true", "599": "true", "600": "true", "601": "true", "602": "true", "603": "true", "604": "true", "605": "true", "606": "true", "607": "true", "608": "true", "609": "true", "610": "true", "611": "true", "612": "true", "613": "true", "614": "true", "615": "true", "616": "true", "617": "true", "618": "true", "619": "true", "620": "true", "621": "true", "622": "true", "623": "true", "624": "true", "625": "true", "626": "true", "627": "true", "628": "true", "629": "true", "630": "true", "631": "true", "632": "true", "633": "true", "634": "true", "635": "true", "636": "true", "637": "true", "638": "true", "639": "true", "640": "true", "641": "true", "642": "true", "643": "true", "644": "true", "645": "true", "646": "true", "647": "true", "648": "true", "649": "true", "650": "true", "651": "true", "652": "true", "653": "true", "654": "true", "655": "true", "656": "true", "657": "true", "658": "true", "659": "true", "660": "true", "661": "true", "662": "true", "663": "true", "664": "true", "665": "true", "666": "true", "667": "true", "668": "false", "669": "false", "670": "false", "671": "false", "672": "false", "673": "false", "674": "false", "675": "false", "676": "false", "677": "false", "678": "false", "679": "false", "680": "false", "681": "false", "682": "false", "683": "false", "684": "false", "685": "false", "686": "false", "687": "false", "688": "false", "689": "false", "690": "false", "691": "false", "692": "false", "693": "false", "694": "false", "695": "false", "696": "false", "697": "false", "698": "false", "699": "false", "700": "false", "701": "true", "702": "true", "703": "true", "704": "true", "705": "true", "706": "true", "707": "true", "708": "true", "709": "true", "710": "true", "711": "true", "712": "true", "713": "true", "714": "true", "715": "true", "716": "true", "717": "true", "718": "true", "719": "true", "720": "true", "721": "true", "722": "true", "723": "true", "724": "true", "725": "true", "726": "true", "727": "true", "728": "true", "729": "true", "730": "true", "731": "true", "732": "true", "733": "true", "734": "true", "735": "true", "736": "true", "737": "true", "738": "true", "739": "true", "740": "true", "741": "true", "742": "true", "743": "true", "744": "true", "745": "true", "746": "true", "747": "false", "748": "false", "749": "false", "750": "false", "751": "false", "752": "false", "753": "false", "754": "false", "755": "false", "756": "false", "757": "false", "758": "false", "759": "false", "760": "false", "761": "false", "762": "false", "763": "false", "764": "false", "765": "false", "766": "false", "767": "false", "768": "false", "769": "false", "770": "false", "771": "false", "772": "false", "773": "false", "774": "false", "775": "false", "776": "false", "777": "false", "778": "false", "779": "false", "780": "false", "781": "false", "782": "false", "783": "false", "784": "false", "785": "false", "786": "false"}}}